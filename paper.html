<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<!--
-->
<link rel="stylesheet" href="css/cs.css"                 type="text/css" />
<link rel="stylesheet" href="css/content.css"            type="text/css" />
<!-- font family -->
<link href="https://fonts.cdnfonts.com/css/zilla-slab" rel="stylesheet">
<link href="//netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet" />
<link href="https://fonts.cdnfonts.com/css/comfortaa" rel="stylesheet">
<link href="https://fonts.cdnfonts.com/css/eb-garamond-2?styles=20043,20040" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<link rel="stylesheet" href="css/full_publication.css" type="text/css" />
<title>Chunhua Shen</title>
</head>
<body>
<div id="layout-content">
<div id="menu">
    <div id="menucontainer">
<ul id="nav">
   <li><a href="index.html" target="_self">Home</a></li>
   <li><a href="paper.html" target="_self">Publications</a></li>
   <li><a href="teaching.html" target="_self">Teaching</a></li>
</ul>
</div>
</div>
<div id="toptitle">
<h1>Selected Publications</h1>
<div id="subtitle">Categorised <a href="paper.html">by year <i class='fa fa-clock-o' aria-hidden='true'></i></a>, <a href="paper2.html">by venue <i class='fa fa-location-arrow' aria-hidden='true'></i></a>.  
<a href="fullpaper.html">Full publication list <i class='fa fa-list-ul' aria-hidden='true'></i></a>. <a href="cshen_papers.pdf">Full list in PDF <i class='fa fa-file-pdf-o' aria-hidden='true'></i></a>.</div>
</div>
<p><a href="https://scholar.google.com/citations?hl=en&amp;user=Ljk2BvIAAAAJ&amp;view_op=list_works&amp;pagesize=100">Google scholar (30650 citations)  <i class='ai ai-google-scholar'   aria-hidden='true'></i></a>,
<a href="https://dblp.org/pid/56/1673.html">DBLP <i class='ai ai-dblp ai-1x'></i></a>,
<a href="https://tinyurl.com/ww4dlqm">arXiv <i class='ai ai-biorxiv ai-1x'></i></a>.</p>
<h2>Journal: 47</h2>
<h3>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI): 31</h3>
<h3>International Journal of Computer Vision (IJCV): 14</h3>
<h3>Journal of Machine Learning Research (JMLR): 1</h3>
<h3>ACM Transactions on Graphics (TOG): 1</h3>
<p><br /></p>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2105.11610.pdf"><img class="imgP  right"   src="data/thumbnail/Bian2021IJCV_arXiv.jpg"></a><b>Unsupervised scale-consistent depth learning from video</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>J. Bian, H. Zhan, N. Wang, Z. Li, L. Zhang, C. Shen, M. Cheng, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2105.11610">arXiv</a><a href="data/bibtex/Bian2021IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Scale-consistent+Depth+Learning+from+Video+Bian,+Jia-Wang+and+Zhan,+Huangying+and+Wang,+Naiyan+and+Li,+Zhichao+and+Zhang,+Le+and+Shen,+Chunhua+and+Cheng,+Ming-Ming+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Scale-consistent+Depth+Learning+from+Video">semantic scholar</a><a href="https://github.com/JiawangBian/SC-SfMLearner-Release">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/IJCV2021Liuyl_arXiv.jpg"><b>Exploring the capacity of an orderless box discretization network for multi-orientation scene text detection</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Liu, T. He, H. Chen, X. Wang, C. Luo, S. Zhang, C. Shen, L. Jin</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1912.09629">arXiv</a><a href="data/bibtex/IJCV2021Liuyl.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploring+the+Capacity+of+an+Orderless+Box+Discretization+Network+for+Multi-orientation+Scene+Text+Detection+Liu,+Yuliang+and+He,+Tong+and+Chen,+Hao+and+Wang,+Xinyu+and+Luo,+Canjie+and+Zhang,+Shuaitao+and+Shen,+Chunhua+and+Jin,+Lianwen">google scholar</a><a href="https://www.semanticscholar.org/search?q=Exploring+the+Capacity+of+an+Orderless+Box+Discretization+Network+for+Multi-orientation+Scene+Text+Detection">semantic scholar</a><a href="https://git.io/TextDet">project webpage</a></p>
</li>
<li><p><b>NAS-FCOS: efficient search for object detection architectures</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Wang2021IJCV_NAS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={NAS-FCOS}:+Efficient+Search+for+Object+Detection+Architectures+Wang,+Ning+and+Gao,+Yang+and+Chen,+Hao+and+Wang,+Peng+and+Tian,+Zhi+and+Shen,+Chunhua+and+Zhang,+Yanning">google scholar</a><a href="https://www.semanticscholar.org/search?q={NAS-FCOS}:+Efficient+Search+for+Object+Detection+Architectures">semantic scholar</a><a href="https://github.com/Lausannen/NAS-FCOS">project webpage</a></p>
</li>
<li><p><b>A dual-attention-guided network for ghost-free high dynamic range imaging</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Q. Yan, D. Gong, Q. Shi, A. van den Hengel, C. Shen, I. Reid, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Yan2021Ghostfree.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Dual-Attention-guided+network+for+ghost-free+high+dynamic+range+imaging+Yan,+Qingsen+and+Gong,+Dong+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Reid,+Ian+and+Zhang,+Yanning">google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Dual-Attention-guided+network+for+ghost-free+high+dynamic+range+imaging">semantic scholar</a></p>
</li>
<li><p><b>BiSeNet v2: bilateral network with guided aggregation for real-time semantic segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Yu, C. Gao, J. Wang, G. Yu, C. Shen, N. Sang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2004.02147">arXiv</a><a href="data/bibtex/Yu2021BiSegV2.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={BiSeNet}+v2:+Bilateral+Network+with+Guided+Aggregation+for+Real-time+Semantic+Segmentation+Yu,+Changqian+and+Gao,+Changxin+and+Wang,+Jingbo+and+Yu,+Gang+and+Shen,+Chunhua+and+Sang,+Nong">google scholar</a><a href="https://www.semanticscholar.org/search?q={BiSeNet}+v2:+Bilateral+Network+with+Guided+Aggregation+for+Real-time+Semantic+Segmentation">semantic scholar</a></p>
</li>
<li><p><b>ABCNet v2: adaptive bezier-curve network for real-time end-to-end text spotting</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Liu, C. Shen, L. Jin, T. He, P. Chen, C. Liu, H. Chen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2105.03620">arXiv</a><a href="data/bibtex/Liu2021ABCNetv2.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={ABCNet}+v2:+Adaptive+Bezier-Curve+Network+for+Real-time+End-to-end+Text+Spotting+Liu,+Yuliang+and+Shen,+Chunhua+and+Jin,+Lianwen+and+He,+Tong+and+Chen,+Peng+and+Liu,+Chongyu+and+Chen,+Hao">google scholar</a><a href="https://www.semanticscholar.org/search?q={ABCNet}+v2:+Adaptive+Bezier-Curve+Network+for+Real-time+End-to-end+Text+Spotting">semantic scholar</a><a href="https://git.io/AdelaiDet">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Li2021Text_arXiv.jpg"><b>Towards end-to-end text spotting in natural scenes</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>P. Wang, H. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1906.06013">arXiv</a><a href="data/bibtex/Li2021Text.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+End-to-End+Text+Spotting+in+Natural+Scenes+Wang,+Peng+and+Li,+Hui+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+End-to-End+Text+Spotting+in+Natural+Scenes">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2021PANplus_arXiv.jpg"><b>PAN++: towards efficient and accurate end-to-end spotting of arbitrarily-shaped text</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>W. Wang, E. Xie, X. Li, X. Liu, D. Liang, Z. Yang, T. Lu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2105.00405">arXiv</a><a href="data/bibtex/Wang2021PANplus.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={PAN++}:+Towards+Efficient+and+Accurate+End-to-End+Spotting+of+Arbitrarily-Shaped+Text+Wang,+Wenhai+and+Xie,+Enze+and+Li,+Xiang+and+Liu,+Xuebo+and+Liang,+Ding+and+Yang,+Zhibo+and+Lu,+Tong+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q={PAN++}:+Towards+Efficient+and+Accurate+End-to-End+Spotting+of+Arbitrarily-Shaped+Text">semantic scholar</a></p>
</li>
<li><p><b>SOLO: a simple framework for instance segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Wang, R. Zhang, C. Shen, T. Kong, L. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2106.15947">arXiv</a><a href="data/bibtex/WXL2021SOLO.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SOLO}:+A+Simple+Framework+for+Instance+Segmentation+Wang,+Xinlong+and+Zhang,+Rufeng+and+Shen,+Chunhua+and+Kong,+Tao+and+Li,+Lei">google scholar</a><a href="https://www.semanticscholar.org/search?q={SOLO}:+A+Simple+Framework+for+Instance+Segmentation">semantic scholar</a><a href="https://git.io/AdelaiDet">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Yin2021PAMIvn_arXiv.jpg"><b>Virtual normal: enforcing geometric constraints for accurate and robust depth prediction</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>W. Yin, Y. Liu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2103.04216">arXiv</a><a href="data/bibtex/Yin2021PAMIvn.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Virtual+Normal:+Enforcing+Geometric+Constraints+for+Accurate+and+Robust+Depth+Prediction+Yin,+Wei+and+Liu,+Yifan+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Virtual+Normal:+Enforcing+Geometric+Constraints+for+Accurate+and+Robust+Depth+Prediction">semantic scholar</a><a href="https://git.io/Depth">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhuang2021Quantization_arXiv.jpg"><b>Effective training of convolutional neural networks with low-bitwidth weights and activations</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>B. Zhuang, J. Liu, M. Tan, L. Liu, I. Reid, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.04680">arXiv</a><a href="data/bibtex/Zhuang2021Quantization.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Effective+Training+of+Convolutional+Neural+Networks+with+Low-bitwidth+Weights+and+Activations+Zhuang,+Bohan+and+Liu,+Jing+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Effective+Training+of+Convolutional+Neural+Networks+with+Low-bitwidth+Weights+and+Activations">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Luo2020IJCV_arXiv.jpg"><b>Separating content from style using adversarial learning for recognizing text in the wild</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Luo, Q. Lin, Y. Liu, L. Jin, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2001.04189">arXiv</a><a href="data/bibtex/Luo2020IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Separating+Content+from+Style+Using+Adversarial+Learning+for+Recognizing+Text+in+the+Wild+Luo,+Canjie+and+Lin,+Qingxiang+and+Liu,+Yuliang+and+Jin,+Lianwen+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Separating+Content+from+Style+Using+Adversarial+Learning+for+Recognizing+Text+in+the+Wild">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2020TOG_arXiv.jpg"><b>Real-time image smoothing via iterative least squares</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>W. Liu, P. Zhang, X. Huang, J. Yang, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>ACM Transactions on Graphics (TOG), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.07504">arXiv</a><a href="https://doi.org/10.1145/3388887">link</a><a href="data/bibtex/Liu2020TOG.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-time+image+smoothing+via+iterative+least+squares+Liu,+Wei+and+Zhang,+Pingping+and+Huang,+Xiaolin+and+Yang,+Jie+and+Shen,+Chunhua+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Real-time+image+smoothing+via+iterative+least+squares">semantic scholar</a><a href="https://github.com/wliusjtu/Real-time-Image-Smoothing-via-Iterative-Least-Squares">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2008.00942.pdf"><img class="imgP  right"   src="data/thumbnail/Cao2020GAN_arXiv.jpg"></a><b>Improving generative adversarial networks with local coordinate coding</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>J. Cao, Y. Guo, Q. Wu, C. Shen, J. Huang, M. Tan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2008.00942">arXiv</a><a href="data/bibtex/Cao2020GAN.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Improving+Generative+Adversarial+Networks+with+Local+Coordinate+Coding+Cao,+Jiezhang+and+Guo,+Yong+and+Wu,+Qingyao+and+Shen,+Chunhua+and+Huang,+Junzhou+and+Tan,+Mingkui">google scholar</a><a href="https://www.semanticscholar.org/search?q=Improving+Generative+Adversarial+Networks+with+Local+Coordinate+Coding">semantic scholar</a><a href="https://github.com/SCUTjinchengli/LCCGAN-v2">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1711.00253.pdf"><img class="imgP  right"   src="data/thumbnail/Chen2019PAMI_arXiv.jpg"></a><b>Adversarial learning of structure-aware fully convolutional networks for landmark localization</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Chen, C. Shen, H. Chen, X. Wei, L. Liu, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.00253">arXiv</a><a href="https://doi.org/10.1109/TPAMI.2019.2901875">link</a><a href="data/bibtex/Chen2019PAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adversarial+Learning+of+Structure-Aware+Fully+Convolutional+Networks+for+Landmark+Localization+Chen,+Yu+and+Shen,+Chunhua+and+Chen,+Hao+and+Wei,+Xiu-Shen+and+Liu,+Lingqiao+and+Yang,+Jian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Adversarial+Learning+of+Structure-Aware+Fully+Convolutional+Networks+for+Landmark+Localization">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2020PAMI_arXiv.jpg"><b>Structured knowledge distillation for dense prediction</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Liu, C. Shun, J. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.04197">arXiv</a><a href="https://ieeexplore.ieee.org/document/9115859">link</a><a href="data/bibtex/Liu2020PAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Knowledge+Distillation+for+Dense+Prediction+Liu,+Yifan+and+Shun,+Changyong+and+Wang,+Jingdong+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+Knowledge+Distillation+for+Dense+Prediction">semantic scholar</a><a href="https://github.com/irfanICMLL/structure_knowledge_distillation">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Lu2020PAMIIndexNet_arXiv.jpg"><b>Index networks</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Lu, Y. Dai, C. Shen, S. Xu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.09895">arXiv</a><a href="https://doi.org/10.1109/TPAMI.2020.3004474">link</a><a href="data/bibtex/Lu2020PAMIIndexNet.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Index+Networks+Lu,+Hao+and+Dai,+Yutong+and+Shen,+Chunhua+and+Xu,+Songcen">google scholar</a><a href="https://www.semanticscholar.org/search?q=Index+Networks">semantic scholar</a><a href="https://git.io/IndexNet">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhang2020OrderlessReID_arXiv.jpg"><b>Ordered or orderless: a revisit for video based person re-identification</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Zhang, Z. Shi, J. Zhou, M. Cheng, Y. Liu, J. Bian, Z. Zeng, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1912.11236">arXiv</a><a href="https://doi.org/10.1109/TPAMI.2020.2976969">link</a><a href="data/bibtex/Zhang2020OrderlessReID.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Ordered+or+Orderless:+A+Revisit+for+Video+based+Person+Re-Identification+Zhang,+Le+and+Shi,+Zenglin+and+Zhou,+Joey+Tianyi+and+Cheng,+Ming-Ming+and+Liu,+Yun+and+Bian,+Jia-Wang+and+Zeng,+Zeng+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Ordered+or+Orderless:+A+Revisit+for+Video+based+Person+Re-Identification">semantic scholar</a><a href="https://github.com/ZhangLeUestc/VideoReid-TPAMI2020">project webpage</a></p>
</li>
<li><p><b>Plenty is plague: fine-grained learning for visual question answering</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Zhou, R. Ji, J. Su, X. Sun, D. Meng, Y. Gao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="https://doi.org/10.1109/TPAMI.2019.2956699">link</a><a href="data/bibtex/Zhou2020TPAMIZhou.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Plenty+Is+Plague:+Fine-Grained+Learning+for+Visual+Question+Answering+Zhou,+Yiyi+and+Ji,+Rongrong+and+Su,+Jinsong+and+Sun,+Xiaoshuai+and+Meng,+Deyu+and+Gao,+Yue+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Plenty+Is+Plague:+Fine-Grained+Learning+for+Visual+Question+Answering">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1806.01576.pdf"><img class="imgP  right"   src="data/thumbnail/Adaptive2019Zhang_arXiv.jpg"></a><b>Adaptive importance learning for improving lightweight image super-resolution network</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Zhang, P. Wang, C. Shen, L. Liu, W. Wei, Y. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1806.01576">arXiv</a><a href="data/bibtex/Adaptive2019Zhang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adaptive+Importance+Learning+for+Improving+Lightweight+Image+Super-resolution+Network+Zhang,+Lei+and+Wang,+Peng+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Wei,+Wei+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Adaptive+Importance+Learning+for+Improving+Lightweight+Image+Super-resolution+Network">semantic scholar</a><a href="https://tinyurl.com/Super-resolution-Network">project webpage</a></p>
</li>
<li><p><b>RefineNet: multi-path refinement networks for dense prediction</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>G. Lin, F. Liu, A. Milan, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="https://doi.org/10.1109/TPAMI.2019.2893630">link</a><a href="data/bibtex/Fayao2019PAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={RefineNet}:+Multi-Path+Refinement+Networks+for+Dense+Prediction+Lin,+Guosheng+and+Liu,+Fayao+and+Milan,+Anton+and+Shen,+Chunhua+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q={RefineNet}:+Multi-Path+Refinement+Networks+for+Dense+Prediction">semantic scholar</a><a href="https://github.com/guosheng/refinenet">project webpage</a></p>
<ol reversed>
<li><p>Pytorch code is <a href="https://github.com/DrSleep/refinenet-pytorch">here</a>.</p>
</li></ol>
</li>
<li><p><b>Cluster sparsity field: an internal hyperspectral imagery prior for reconstruction</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Y. Zhang, C. Shen, A. van den Hengel, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="https://www.researchgate.net/publication/323914969_Cluster_Sparsity_Field_An_Internal_Hyperspectral_Imagery_Prior_for_Reconstruction">pdf</a><a href="data/bibtex/Zhang2018IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Cluster+Sparsity+Field:+An+Internal+Hyperspectral+Imagery+Prior+for+Reconstruction+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng">google scholar</a><a href="https://www.semanticscholar.org/search?q=Cluster+Sparsity+Field:+An+Internal+Hyperspectral+Imagery+Prior+for+Reconstruction">semantic scholar</a></p>
</li>
<li><p><b>Ordinal constraint binary coding for approximate nearest neighbor search</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Liu, R. Ji, J. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="https://www.researchgate.net/publication/324053386_Ordinal_Constraint_Binary_Coding_for_Approximate_Nearest_Neighbor_Search">pdf</a><a href="data/bibtex/HLiu2018TPAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Ordinal+Constraint+Binary+Coding+for+Approximate+Nearest+Neighbor+Search+Liu,+Hong+and+Ji,+Rongrong+and+Wang,+Jingdong+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Ordinal+Constraint+Binary+Coding+for+Approximate+Nearest+Neighbor+Search">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2017FVQA_arXiv.jpg"><b>FVQA: fact-based visual question answering</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>P. Wang, Q. Wu, C. Shen, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1606.05433">arXiv</a><a href="data/bibtex/Wang2017FVQA.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FVQA}:+Fact-based+Visual+Question+Answering+Wang,+Peng+and+Wu,+Qi+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q={FVQA}:+Fact-based+Visual+Question+Answering">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/IJCV2017Lin_arXiv.jpg"><b>Structured learning of binary codes with column generation for optimizing ranking measures</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>G. Lin, F. Liu, C. Shen, J. Wu, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1602.06654">arXiv</a><a href="data/bibtex/IJCV2017Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Learning+of+Binary+Codes+with+Column+Generation+for+Optimizing+Ranking+Measures+Lin,+Guosheng+and+Liu,+Fayao+and+Shen,+Chunhua+and+Wu,+Jianxin+and+Shen,+Heng+Tao">google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+Learning+of+Binary+Codes+with+Column+Generation+for+Optimizing+Ranking+Measures">semantic scholar</a><a href="https://bitbucket.org/guosheng/structhash">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Lin2017Semantic_arXiv.jpg"><b>Exploring context with deep structured models for semantic segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>G. Lin, C. Shen, A. van den Hengel, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.03183">arXiv</a><a href="data/bibtex/Lin2017Semantic.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploring+Context+with+Deep+Structured+models+for+Semantic+Segmentation+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Exploring+Context+with+Deep+Structured+models+for+Semantic+Segmentation">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1510.00921.pdf"><img class="imgP  right"   src="data/thumbnail/Cross2017Liu_arXiv.jpg"></a><b>Cross-convolutional-layer pooling for image recognition</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1510.00921">arXiv</a><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7779086">link</a><a href="data/bibtex/Cross2017Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Cross-convolutional-layer+Pooling+for+Image+Recognition+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Cross-convolutional-layer+Pooling+for+Image+Recognition">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TPAMI2017Liu_arXiv.jpg"><b>Compositional model based Fisher vector coding for image classification</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Liu, P. Wang, C. Shen, L. Wang, A. van den Hengel, C. Wang, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1601.04143">arXiv</a><a href="data/bibtex/TPAMI2017Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Compositional+Model+based+{F}isher+Vector+Coding+for+Image+Classification+Liu,+Lingqiao+and+Wang,+Peng+and+Shen,+Chunhua+and+Wang,+Lei+and+{van+den+Hengel},+Anton+and+Wang,+Chao+and+Shen,+Heng+Tao">google scholar</a><a href="https://www.semanticscholar.org/search?q=Compositional+Model+based+{F}isher+Vector+Coding+for+Image+Classification">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wu2017External_arXiv.jpg"><b>Image captioning and visual question answering based on attributes and external knowledge</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Q. Wu, C. Shen, P. Wang, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.02814">arXiv</a><a href="data/bibtex/Wu2017External.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Image+Captioning+and+Visual+Question+Answering+Based+on+Attributes+and+External+Knowledge+Wu,+Qi+and+Shen,+Chunhua+and+Wang,+Peng+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Image+Captioning+and+Visual+Question+Answering+Based+on+Attributes+and+External+Knowledge">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Yao2016IJCV_arXiv.jpg"><b>Mining mid-level visual patterns with deep CNN activations</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1506.06343">arXiv</a><a href="http://rdcu.be/j1mA">link</a><a href="data/bibtex/Yao2016IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mining+Mid-level+Visual+Patterns+with+Deep+{CNN}+Activations+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Mining+Mid-level+Visual+Patterns+with+Deep+{CNN}+Activations">semantic scholar</a><a href="https://github.com/yaoliUoA/MDPM">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1404.5009.pdf"><img class="imgP  right"   src="data/thumbnail/BnB2015Wang_arXiv.jpg"></a><b>Efficient semidefinite branch-and-cut for MAP-MRF inference</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>P. Wang, C. Shen, A. van den Hengel, P. Torr</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1404.5009">arXiv</a><a href="http://doi.org/10.1007/s11263-015-0865-2">link</a><a href="data/bibtex/BnB2015Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Semidefinite+Branch-and-Cut+for+{MAP-MRF}+Inference+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Torr,+Philip">google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+Semidefinite+Branch-and-Cut+for+{MAP-MRF}+Inference">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhang2015IJCV_arXiv.jpg"><b>Unsupervised feature learning for dense correspondences across scenes</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Zhang, C. Shen, T. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1501.00642">arXiv</a><a href="data/bibtex/Zhang2015IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Feature+Learning+for+Dense+Correspondences+across+Scenes+Zhang,+Chao+and+Shen,+Chunhua+and+Shen,+Tingzhi">google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Feature+Learning+for+Dense+Correspondences+across+Scenes">semantic scholar</a><a href="https://bitbucket.org/chhshen/ufl">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Xi2015TPAMI_arXiv.jpg"><b>Online metric-weighted linear representations for robust visual tracking</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Li, C. Shen, A. Dick, Z. Zhang, Y. Zhuang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1507.05737">arXiv</a><a href="data/bibtex/Xi2015TPAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Online+Metric-Weighted+Linear+Representations+for+Robust+Visual+Tracking+Li,+Xi+and+Shen,+Chunhua+and+Dick,+Anthony+and+Zhang,+Zhongfei+and+Zhuang,+Yueting">google scholar</a><a href="https://www.semanticscholar.org/search?q=Online+Metric-Weighted+Linear+Representations+for+Robust+Visual+Tracking">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Depth2015Liu_arXiv.jpg"><b>Learning depth from single monocular images using deep convolutional neural fields</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>F. Liu, C. Shen, G. Lin, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1502.07411">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2015.2505283">link</a><a href="data/bibtex/Depth2015Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Depth+from+Single+Monocular+Images+Using+Deep+Convolutional+Neural+Fields+Liu,+Fayao+and+Shen,+Chunhua+and+Lin,+Guosheng+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Depth+from+Single+Monocular+Images+Using+Deep+Convolutional+Neural+Fields">semantic scholar</a><a href="http://goo.gl/rAKWrS">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2015TPAMI_arXiv.jpg"><b>A generalized probabilistic framework for compact codebook creation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Liu, L. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1401.7713">arXiv</a><a href="http://doi.org/10.1109/TPAMI.2015.2441069">link</a><a href="data/bibtex/Liu2015TPAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Generalized+Probabilistic+Framework+for+Compact+Codebook+Creation+Liu,+Lingqiao+and+Wang,+Lei+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Generalized+Probabilistic+Framework+for+Compact+Codebook+Creation">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Paisitkriangkrai2015TPAMI_arXiv.jpg"><b>Pedestrian detection with spatially pooled features and structured ensemble learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1409.5209">arXiv</a><a href="http://doi.org/10.1109/TPAMI.2015.2474388">link</a><a href="data/bibtex/Paisitkriangkrai2015TPAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Pedestrian+Detection+with+Spatially+Pooled+Features+and+Structured+Ensemble+Learning+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Pedestrian+Detection+with+Spatially+Pooled+Features+and+Structured+Ensemble+Learning">semantic scholar</a><a href="https://github.com/chhshen/pedestrian-detection">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.7564.pdf"><img class="imgP  right"   src="data/thumbnail/BQP2015Wang_arXiv.jpg"></a><b>Large-scale binary quadratic optimization using semidefinite relaxation and applications</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>P. Wang, C. Shen, A. van den Hengel, P. Torr</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.7564">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2016.2541146">link</a><a href="data/bibtex/BQP2015Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Large-scale+Binary+Quadratic+Optimization+Using+Semidefinite+Relaxation+and+Applications+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Torr,+Philip+H.+S.">google scholar</a><a href="https://www.semanticscholar.org/search?q=Large-scale+Binary+Quadratic+Optimization+Using+Semidefinite+Relaxation+and+Applications">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1401.8126.pdf"><img class="imgP  right"   src="data/thumbnail/Harandi2015IJCV_arXiv.jpg"></a><b>Extrinsic methods for coding and dictionary learning on Grassmann manifolds</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>M. Harandi, R. Hartley, C. Shen, B. Lovell, C. Sanderson</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1401.8126">arXiv</a><a href="data/bibtex/Harandi2015IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Extrinsic+Methods+for+Coding+and+Dictionary+Learning+on+{G}rassmann+Manifolds+Harandi,+Mehrtash+and+Hartley,+Richard+and+Shen,+Chunhua+and+Lovell,+Brian+and+Sanderson,+Conrad">google scholar</a><a href="https://www.semanticscholar.org/search?q=Extrinsic+Methods+for+Coding+and+Dictionary+Learning+on+{G}rassmann+Manifolds">semantic scholar</a><a href="https://github.com/chhshen/Grassmann/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1408.5574.pdf"><img class="imgP  right"   src="data/thumbnail/FastHash2015Lin_arXiv.jpg"></a><b>Supervised hashing using graph cuts and boosted decision trees</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>G. Lin, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1408.5574">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2015.2404776">link</a><a href="data/bibtex/FastHash2015Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Supervised+Hashing+Using+Graph+Cuts+and+Boosted+Decision+Trees+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Supervised+Hashing+Using+Graph+Cuts+and+Boosted+Decision+Trees">semantic scholar</a><a href="https://bitbucket.org/chhshen/fasthash/">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2014SBoosting_PDF.jpg"><b>StructBoost: Boosting methods for predicting structured output variables</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Shen, G. Lin, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1302.3283">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2014.2315792">link</a><a href="http://goo.gl/goCVLK">pdf</a><a href="data/bibtex/Shen2014SBoosting.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={StructBoost}:+{B}oosting+Methods+for+Predicting+Structured+Output+Variables+Shen,+Chunhua+and+Lin,+Guosheng+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q={StructBoost}:+{B}oosting+Methods+for+Predicting+Structured+Output+Variables">semantic scholar</a></p>
</li>
<li><p><b>A hierarchical word-merging algorithm with class separability measure</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Wang, L. Zhou, C. Shen, L. Liu, H. Liu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="https://bitbucket.org/chhshen/chhshen.bitbucket.org/src/be12d4ef8deb6207ec97f0fdac6efbe2df151b59/_download/TPAMI14Wang.pdf">pdf</a><a href="data/bibtex/Wang2014PAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Hierarchical+Word-merging+Algorithm+with+Class+Separability+Measure+Wang,+Lei+and+Zhou,+Luping+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Liu,+Huan">google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Hierarchical+Word-merging+Algorithm+with+Class+Separability+Measure">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1301.2032.pdf"><img class="imgP  right"   src="data/thumbnail/FisherBoost2013IJCV_arXiv.jpg"></a><b>Training effective node classifiers for cascade classification</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Shen, P. Wang, S. Paisitkriangkrai, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>International Journal of Computer Vision (IJCV), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1301.2032">arXiv</a><a href="http://link.springer.com/article/10.1007%2Fs11263-013-0608-1">link</a><a href="data/bibtex/FisherBoost2013IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Training+Effective+Node+Classifiers+for+Cascade+Classification+Shen,+Chunhua+and+Wang,+Peng+and+Paisitkriangkrai,+Sakrapee+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Training+Effective+Node+Classifiers+for+Cascade+Classification">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TPAMI2013Xi_PDF.jpg"><b>Incremental learning of 3D-DCT compact representations for robust visual tracking</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Li, A. Dick, C. Shen, A. van den Hengel, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1207.3389">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2012.166">link</a><a href="https://sites.google.com/site/chhshen/publication/tpami12xi.pdf?attredirects=1">pdf</a><a href="data/bibtex/TPAMI2013Xi.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Incremental+Learning+of+{3D-DCT}+Compact+Representations+for+Robust+Visual+Tracking+Li,+Xi+and+Dick,+Anthony+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Wang,+Hanzi">google scholar</a><a href="https://www.semanticscholar.org/search?q=Incremental+Learning+of+{3D-DCT}+Compact+Representations+for+Robust+Visual+Tracking">semantic scholar</a><a href="https://github.com/chhshen/DCT-Tracking/">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/JMLR2012Shen_arXiv.jpg"><b>Positive semidefinite metric learning using boosting-like algorithms</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Shen, J. Kim, L. Wang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Journal of Machine Learning Research (JMLR), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1104.4704">arXiv</a><a href="http://jmlr.csail.mit.edu/papers/v13/shen12a.html">link</a><a href="data/bibtex/JMLR2012Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Positive+Semidefinite+Metric+Learning+Using+Boosting-like+Algorithms+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Positive+Semidefinite+Metric+Learning+Using+Boosting-like+Algorithms">semantic scholar</a><a href="https://bitbucket.org/chhshen/data/raw/45d101372013763d18f0a7ed191c86569532ed96/code/BoostMetric-NeurIPS09-codes-V0.1.tar.bz2">project webpage</a></p>
</li>
<li><p><b>UBoost: Boosting with the Universum</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Shen, P. Wang, F. Shen, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/67027">pdf</a><a href="data/bibtex/UBoost2011Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={UBoost}:+{B}oosting+with+the+{U}niversum+Shen,+Chunhua+and+Wang,+Peng+and+Shen,+Fumin+and+Wang,+Hanzi">google scholar</a><a href="https://www.semanticscholar.org/search?q={UBoost}:+{B}oosting+with+the+{U}niversum">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Dual2010Shen_arXiv.jpg"><b>On the dual formulation of boosting algorithms</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Shen, H. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0901.3590">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2010.47">link</a><a href="data/bibtex/Dual2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=On+the+Dual+Formulation+of+Boosting+Algorithms+Shen,+Chunhua+and+Li,+Hanxi">google scholar</a><a href="https://www.semanticscholar.org/search?q=On+the+Dual+Formulation+of+Boosting+Algorithms">semantic scholar</a></p>
</li>
<li><p><b>Adaptive object tracking based on an effective appearance filter</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Wang, D. Suter, K. Schindler, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2007</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TPAMI.2007.1112">link</a><a href="http://goo.gl/6rQTA1">pdf</a><a href="data/bibtex/Adaptive2007Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adaptive+object+tracking+based+on+an+effective+appearance+filter+Wang,+Hanzi+and+Suter,+David+and+Schindler,+Konrad+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Adaptive+object+tracking+based+on+an+effective+appearance+filter">semantic scholar</a></p>
<ol reversed>
<li><p>Featured article of September issue 2007.</p>
</li>
</ol>

</li>
</ol>
<h2>Conference: 138</h2>
<h3>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR): 82</h3>
<h3>Proc. IEEE International Conference on Computer Vision (ICCV): 25</h3>
<h3>Proc. European Conference on Computer Vision (ECCV): 20</h3>
<h3>Proc. International Conference on Machine Learning (ICML): 3</h3>
<h3>Proc. Advances in Neural Information Processing Systems (NeurIPS): 8</h3>
<p><br /></p>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2007.06919.pdf"><img class="imgP  right"   src="data/thumbnail/Chen2021CVPR3_arXiv.jpg"></a><b>AQD: towards accurate quantized object detection</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>P. Chen, J. Liu, B. Zhuang, M. Tan, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2007.06919">arXiv</a><a href="data/bibtex/Chen2021CVPR3.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={AQD}:+Towards+Accurate+Quantized+Object+Detection+Chen,+Peng+and+Liu,+Jing+and+Zhuang,+Bohan+and+Tan,+Mingkui+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q={AQD}:+Towards+Accurate+Quantized+Object+Detection">semantic scholar</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2011.14288.pdf"><img class="imgP  right"   src="data/thumbnail/Dai2021CVPR8_arXiv.jpg"></a><b>Learning affinity-aware upsampling for deep image matting</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Dai, H. Lu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2011.14288">arXiv</a><a href="data/bibtex/Dai2021CVPR8.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Affinity-Aware+Upsampling+for+Deep+Image+Matting+Dai,+Yutong+and+Lu,+Hao+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Affinity-Aware+Upsampling+for+Deep+Image+Matting">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2011.11204.pdf"><img class="imgP  right"   src="data/thumbnail/Guo2021CVPR14_arXiv.jpg"></a><b>Graph attention tracking</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>D. Guo, Y. Shao, Y. Cui, Z. Wang, L. Zhang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2011.11204">arXiv</a><a href="data/bibtex/Guo2021CVPR14.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Graph+Attention+Tracking+Guo,+Dongyan+and+Shao,+Yanyan+and+Cui,+Ying+and+Wang,+Zhenhua+and+Zhang,+Liyan+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Graph+Attention+Tracking">semantic scholar</a><a href="https://git.io/SiamGAT">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/He2021CVPR7_arXiv.jpg"><b>DyCo3D: robust instance segmentation of 3d point clouds through dynamic convolution</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>T. He, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2011.13328">arXiv</a><a href="data/bibtex/He2021CVPR7.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={DyCo3D}:+Robust+Instance+Segmentation+of+3D+Point+Clouds+through+Dynamic+Convolution+He,+Tong+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q={DyCo3D}:+Robust+Instance+Segmentation+of+3D+Point+Clouds+through+Dynamic+Convolution">semantic scholar</a><a href="https://git.io/DyCo3D">project webpage</a></p>
</li>
<li><p><b>HCRF-Flow: scene flow from point clouds with continuous high-order CRFs and position-aware flow embedding</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>R. Li, G. Lin, T. He, F. Liu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Li2021CVPR12.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={HCRF-Flow}:+Scene+Flow+from+Point+Clouds+with+Continuous+High-order+{CRFs}+and+Position-aware+Flow+Embedding+Li,+Ruibo+and+Lin,+Guosheng+and+He,+Tong+and+Liu,+Fayao+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q={HCRF-Flow}:+Scene+Flow+from+Point+Clouds+with+Continuous+High-order+{CRFs}+and+Position-aware+Flow+Embedding">semantic scholar</a></p>
</li>
<li><p><b>Generic perceptual loss for modelling structured output dependencies</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Liu, W. Yin, Y. Chen, H. Chen, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Liu2021CVPR5.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Generic+Perceptual+Loss+for+Modelling+Structured+Output+Dependencies+Liu,+Yifan+and+Yin,+Wei+and+Chen,+Yu+and+Chen,+Hao+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Generic+Perceptual+Loss+for+Modelling+Structured+Output+Dependencies">semantic scholar</a></p>
</li>
<li><p><b>FCPose: fully convolutional multi-person pose estimation with dynamic instance-aware convolutions</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>W. Mao, Z. Tian, X. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Mao2021CVPR4.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FCPose}:+Fully+Convolutional+Multi-Person+Pose+Estimation+with+Dynamic+Instance-Aware+Convolutions+Mao,+Weian+and+Tian,+Zhi+and+Wang,+Xinlong+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q={FCPose}:+Fully+Convolutional+Multi-Person+Pose+Estimation+with+Dynamic+Instance-Aware+Convolutions">semantic scholar</a><a href="https://git.io/AdelaiDet">project webpage</a></p>
</li>
<li><p><b>Feature decomposition and reconstruction learning for effective facial expression recognition</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>D. Ruan, Y. Yan, S. Lai, Z. Chai, C. Shen, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Ruan2021CVPR9.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Feature+Decomposition+and+Reconstruction+Learning+for+Effective+Facial+Expression+Recognition+Ruan,+Delian+and+Yan,+Yan+and+Lai,+Shenqi+and+Chai,+Zhenhua+and+Shen,+Chunhua+and+Wang,+Hanzi">google scholar</a><a href="https://www.semanticscholar.org/search?q=Feature+Decomposition+and+Reconstruction+Learning+for+Effective+Facial+Expression+Recognition">semantic scholar</a></p>
</li>
<li><p><b>Learning spatial-semantic relationship for facial attribute recognition with limited labeled data</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Shu, Y. Yan, S. Chen, J. Xue, C. Shen, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Shu2021CVPR10.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Spatial-Semantic+Relationship+for+Facial+Attribute+Recognition+with+Limited+Labeled+Data+Shu,+Ying+and+Yan,+Yan+and+Chen,+Si+and+Xue,+Jing-Hao+and+Shen,+Chunhua+and+Wang,+Hanzi">google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Spatial-Semantic+Relationship+for+Facial+Attribute+Recognition+with+Limited+Labeled+Data">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Tian2021CVPR2_arXiv.jpg"><b>BoxInst: high-performance instance segmentation with box annotations</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Z. Tian, C. Shen, X. Wang, H. Chen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2012.02310">arXiv</a><a href="data/bibtex/Tian2021CVPR2.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={BoxInst}:+High-Performance+Instance+Segmentation+with+Box+Annotations+Tian,+Zhi+and+Shen,+Chunhua+and+Wang,+Xinlong+and+Chen,+Hao">google scholar</a><a href="https://www.semanticscholar.org/search?q={BoxInst}:+High-Performance+Instance+Segmentation+with+Box+Annotations">semantic scholar</a><a href="https://git.io/AdelaiDet">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2021CVPR13_arXiv.jpg"><b>Dense contrastive learning for self-supervised visual pre-training</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Wang, R. Zhang, C. Shen, T. Kong, L. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2011.09157">arXiv</a><a href="data/bibtex/Wang2021CVPR13.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Dense+Contrastive+Learning+for+Self-Supervised+Visual+Pre-Training+Wang,+Xinlong+and+Zhang,+Rufeng+and+Shen,+Chunhua+and+Kong,+Tao+and+Li,+Lei">google scholar</a><a href="https://www.semanticscholar.org/search?q=Dense+Contrastive+Learning+for+Self-Supervised+Visual+Pre-Training">semantic scholar</a><a href="https://git.io/AdelaiDet">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2021CVPR11_arXiv.jpg"><b>End-to-end video instance segmentation with Transformers</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, H. Xia</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2011.14503">arXiv</a><a href="data/bibtex/Wang2021CVPR11.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=End-to-End+Video+Instance+Segmentation+with+{T}ransformers+Wang,+Yuqing+and+Xu,+Zhaoliang+and+Wang,+Xinlong+and+Shen,+Chunhua+and+Cheng,+Baoshan+and+Shen,+Hao+and+Xia,+Huaxia">google scholar</a><a href="https://www.semanticscholar.org/search?q=End-to-End+Video+Instance+Segmentation+with+{T}ransformers">semantic scholar</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Yin2021CVPR6_arXiv.jpg"><b>Learning to recover 3D scene shape from a single image</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>W. Yin, J. Zhang, O. Wang, S. Niklaus, L. Mai, S. Chen, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2012.09365">arXiv</a><a href="data/bibtex/Yin2021CVPR6.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+to+Recover+{3D}+Scene+Shape+from+a+Single+Image+Yin,+Wei+and+Zhang,+Jianming+and+Wang,+Oliver+and+Niklaus,+Simon+and+Mai,+Long+and+Chen,+Simon+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+to+Recover+{3D}+Scene+Shape+from+a+Single+Image">semantic scholar</a><a href="https://git.io/Depth">project webpage</a></p>
<ol reversed>
<li><p>Listed as one of the Best Paper Candidates, 32 out of about 6000 submissions.</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhang2021CVPR1_arXiv.jpg"><b>DoDNet: learning to segment multi-organ and tumors from multiple partially labeled datasets</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>J. Zhang, Y. Xie, Y. Xia, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2011.10217">arXiv</a><a href="data/bibtex/Zhang2021CVPR1.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={DoDNet}:+Learning+to+segment+multi-organ+and+tumors+from+multiple+partially+labeled+datasets+Zhang,+Jianpeng+and+Xie,+Yutong+and+Xia,+Yong+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q={DoDNet}:+Learning+to+segment+multi-organ+and+tumors+from+multiple+partially+labeled+datasets">semantic scholar</a><a href="https://github.com/aim-uofa/partially-labelled">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2008.05101.pdf"><img class="imgP  right"   src="data/thumbnail/Chen2021ICCV_arXiv.jpg"></a><b>FATNN: fast and accurate ternary neural networks</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>P. Chen, B. Zhuang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2008.05101">arXiv</a><a href="data/bibtex/Chen2021ICCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FATNN}:+Fast+and+Accurate+Ternary+Neural+Networks+Chen,+Peng+and+Zhuang,+Bohan+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q={FATNN}:+Fast+and+Accurate+Ternary+Neural+Networks">semantic scholar</a></p>
</li>
<li><p><b>Channel-wise knowledge distillation for dense prediction</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Shu, Y. Liu, J. Gao, L. Xu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2011.13256">arXiv</a><a href="data/bibtex/Shu2021ICCVKD.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Channel-wise+Knowledge+Distillation+for+Dense+Prediction+Shu,+Changyong+and+Liu,+Yifan+and+Gao,+Jianfei+and+Xu,+Lin+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Channel-wise+Knowledge+Distillation+for+Dense+Prediction">semantic scholar</a></p>
</li>
<li><p><b>Occluded person re-identification with single-scale global representations</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Yan, G. Pang, J. Jiao, X. Bai, X. Feng, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Yan2021ICCVOccl.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Occluded+Person+Re-Identification+with+Single-scale+Global+Representations+Yan,+Cheng+and+Pang,+Guansong+and+Jiao,+Jile+and+Bai,+Xiao+and+Feng,+Xuetao+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Occluded+Person+Re-Identification+with+Single-scale+Global+Representations">semantic scholar</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><b>BV-Person: a large-scale dataset for bird-view person re-identification</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Yan, G. Pang, L. Wang, J. Jiao, X. Feng, C. Shen, J. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Yan2021ICCVBVPerson.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={BV-Person}:+A+Large-scale+Dataset+for+Bird-view+Person+Re-identification+Yan,+Cheng+and+Pang,+Guansong+and+Wang,+Lei+and+Jiao,+Jile+and+Feng,+Xuetao+and+Shen,+Chunhua+and+Li,+Jingjing">google scholar</a><a href="https://www.semanticscholar.org/search?q={BV-Person}:+A+Large-scale+Dataset+for+Bird-view+Person+Re-identification">semantic scholar</a></p>
</li>
<li><p><b>A simple baseline for semi-supervised semantic segmentation with strong data augmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>J. Yuan, Y. Liu, C. Shen, Z. Wang, H. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2104.07256">arXiv</a><a href="data/bibtex/Yuan2021ICCVSimple.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Simple+Baseline+for+Semi-supervised+Semantic+Segmentation+with+Strong+Data+Augmentation+Yuan,+Jianlong+and+Liu,+Yifan+and+Shen,+Chunhua+and+Wang,+Zhibin+and+Li,+Hao">google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Simple+Baseline+for+Semi-supervised+Semantic+Segmentation+with+Strong+Data+Augmentation">semantic scholar</a></p>
</li>
<li><p><b>Meta navigator: search for a good adaptation policy for few-shot learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Zhang, H. Ding, G. Lin, R. Li, C. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Chizhang2021ICCVMeta.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Meta+Navigator:+Search+for+a+Good+Adaptation+Policy+for+Few-shot+Learning+Zhang,+Chi+and+Ding,+Henghui+and+Lin,+Guosheng+and+Li,+Ruibo+and+Wang,+Changhu+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Meta+Navigator:+Search+for+a+Good+Adaptation+Policy+for+Few-shot+Learning">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2001.00309.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020BlendMask7_arXiv.jpg"></a><b>BlendMask: top-down meets bottom-up for instance segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Chen, K. Sun, Z. Tian, C. Shen, Y. Huang, Y. Yan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2001.00309">arXiv</a><a href="data/bibtex/CVPR2020BlendMask7.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={BlendMask}:+Top-Down+Meets+Bottom-Up+for+Instance+Segmentation+Chen,+Hao+and+Sun,+Kunyang+and+Tian,+Zhi+and+Shen,+Chunhua+and+Huang,+Yongming+and+Yan,+Youliang">google scholar</a><a href="https://www.semanticscholar.org/search?q={BlendMask}:+Top-Down+Meets+Bottom-Up+for+Instance+Segmentation">semantic scholar</a><a href="https://github.com/aim-uofa/AdelaiDet/">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2002.10200.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Liu6_arXiv.jpg"></a><b>ABCNet: arbitrarily-shaped scene text spotting with adaptive Bezier-curve network in real time</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Liu, H. Chen, C. Shen, T. He, L. Jin, L. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2002.10200">arXiv</a><a href="data/bibtex/CVPR2020Liu6.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={ABCNet}:+Arbitrarily-Shaped+Scene+Text+Spotting+with+Adaptive+{B}ezier-Curve+Network+in+Real+Time+Liu,+Yuliang+and+Chen,+Hao+and+Shen,+Chunhua+and+He,+Tong+and+Jin,+Lianwen+and+Wang,+Liangwei">google scholar</a><a href="https://www.semanticscholar.org/search?q={ABCNet}:+Arbitrarily-Shaped+Scene+Text+Spotting+with+Adaptive+{B}ezier-Curve+Network+in+Real+Time">semantic scholar</a><a href="https://github.com/aim-uofa/AdelaiDet/">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.06780.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Self12_arXiv.jpg"></a><b>Self-trained deep ordinal regression for end-to-end video anomaly detection</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>G. Pang, C. Yan, C. Shen, A. van den Hengel, X. Bai</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.06780">arXiv</a><a href="data/bibtex/CVPR2020Self12.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Self-trained+Deep+Ordinal+Regression+for+End-to-End+Video+Anomaly+Detection+Pang,+Guansong+and+Yan,+Cheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Bai,+Xiao">google scholar</a><a href="https://www.semanticscholar.org/search?q=Self-trained+Deep+Ordinal+Regression+for+End-to-End+Video+Anomaly+Detection">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1904.10151.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020REVERIE1_arXiv.jpg"></a><b>REVERIE: remote embodied visual referring expression in real indoor environments</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Wang, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1904.10151">arXiv</a><a href="data/bibtex/CVPR2020REVERIE1.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={REVERIE}:+Remote+Embodied+Visual+Referring+Expression+in+Real+Indoor+Environments+Qi,+Yuankai+and+Wu,+Qi+and+Anderson,+Peter+and+Wang,+Xin+and+Wang,+William+Yang+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q={REVERIE}:+Remote+Embodied+Visual+Referring+Expression+in+Real+Indoor+Environments">semantic scholar</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1906.04423.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020NASFCOS10_arXiv.jpg"></a><b>NAS-FCOS: fast neural architecture search for object detection</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1906.04423">arXiv</a><a href="data/bibtex/CVPR2020NASFCOS10.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={NAS-FCOS}:+Fast+Neural+Architecture+Search+for+Object+Detection+Wang,+Ning+and+Gao,+Yang+and+Chen,+Hao+and+Wang,+Peng+and+Tian,+Zhi+and+Shen,+Chunhua+and+Zhang,+Yanning">google scholar</a><a href="https://www.semanticscholar.org/search?q={NAS-FCOS}:+Fast+Neural+Architecture+Search+for+Object+Detection">semantic scholar</a><a href="https://github.com/Lausannen/NAS-FCOS">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2002.10215.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Wang4_arXiv.jpg"></a><b>On the general value of evidence, and bilingual scene-text visual question answering</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Wang, Y. Liu, C. Shen, C. Ng, C. Luo, L. Jin, C. Chan, A. van den Hengel, L. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2002.10215">arXiv</a><a href="data/bibtex/CVPR2020Wang4.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=On+the+General+Value+of+Evidence,+and+Bilingual+Scene-Text+Visual+Question+Answering+Wang,+Xinyu+and+Liu,+Yuliang+and+Shen,+Chunhua+and+Ng,+Chun+Chet+and+Luo,+Canjie+and+Jin,+Lianwen+and+Chan,+Chee+Seng+and+{van+den+Hengel},+Anton+and+Wang,+Liangwei">google scholar</a><a href="https://www.semanticscholar.org/search?q=On+the+General+Value+of+Evidence,+and+Bilingual+Scene-Text+Visual+Question+Answering">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1909.13226.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Xie8_arXiv.jpg"></a><b>PolarMask: single shot instance segmentation with polar representation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>E. Xie, P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen, P. Luo</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1909.13226">arXiv</a><a href="data/bibtex/CVPR2020Xie8.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={PolarMask}:+Single+Shot+Instance+Segmentation+with+Polar+Representation+Xie,+Enze+and+Sun,+Peize+and+Song,+Xiaoge+and+Wang,+Wenhai+and+Liu,+Xuebo+and+Liang,+Ding+and+Shen,+Chunhua+and+Luo,+Ping">google scholar</a><a href="https://www.semanticscholar.org/search?q={PolarMask}:+Single+Shot+Instance+Segmentation+with+Polar+Representation">semantic scholar</a><a href="https://github.com/xieenze/PolarMask">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2004.01547.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Yu2_arXiv.jpg"></a><b>Context prior for scene segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Yu, J. Wang, C. Gao, G. Yu, C. Shen, N. Sang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2004.01547">arXiv</a><a href="data/bibtex/CVPR2020Yu2.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Context+Prior+for+Scene+Segmentation+Yu,+Changqian+and+Wang,+Jingbo+and+Gao,+Changxin+and+Yu,+Gang+and+Shen,+Chunhua+and+Sang,+Nong">google scholar</a><a href="https://www.semanticscholar.org/search?q=Context+Prior+for+Scene+Segmentation">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.06777.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020EMD9_arXiv.jpg"></a><b>DeepEMD: few-shot image classification with differentiable earth mover's distance and structured classifiers</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Zhang, Y. Cai, G. Lin, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.06777">arXiv</a><a href="data/bibtex/CVPR2020EMD9.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={DeepEMD}:+Few-Shot+Image+Classification+with+Differentiable+Earth+Mover's+Distance+and+Structured+Classifiers+Zhang,+Chi+and+Cai,+Yujun+and+Lin,+Guosheng+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q={DeepEMD}:+Few-Shot+Image+Classification+with+Differentiable+Earth+Mover's+Distance+and+Structured+Classifiers">semantic scholar</a><a href="https://github.com/icoz69/DeepEMD">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1909.08228.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020NAS11_arXiv.jpg"></a><b>Memory-efficient hierarchical neural architecture search for image denoising</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Zhang, Y. Li, H. Chen, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1909.08228">arXiv</a><a href="data/bibtex/CVPR2020NAS11.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Memory-Efficient+Hierarchical+Neural+Architecture+Search+for+Image+Denoising+Zhang,+Haokui+and+Li,+Ying+and+Chen,+Hao+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Memory-Efficient+Hierarchical+Neural+Architecture+Search+for+Image+Denoising">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.11712.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Zhang3_arXiv.jpg"></a><b>Mask encoding for single shot instance segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>R. Zhang, Z. Tian, C. Shen, M. You, Y. Yan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.11712">arXiv</a><a href="data/bibtex/CVPR2020Zhang3.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mask+Encoding+for+Single+Shot+Instance+Segmentation+Zhang,+Rufeng+and+Tian,+Zhi+and+Shen,+Chunhua+and+You,+Mingyu+and+Yan,+Youliang">google scholar</a><a href="https://www.semanticscholar.org/search?q=Mask+Encoding+for+Single+Shot+Instance+Segmentation">semantic scholar</a><a href="https://github.com/aim-uofa/AdelaiDet/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.11236.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Zhuang5_arXiv.jpg"></a><b>Training quantized neural networks with a full-precision auxiliary module</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>B. Zhuang, L. Liu, M. Tan, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.11236">arXiv</a><a href="data/bibtex/CVPR2020Zhuang5.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Training+Quantized+Neural+Networks+with+a+Full-precision+Auxiliary+Module+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Tan,+Mingkui+and+Shen,+Chunhua+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Training+Quantized+Neural+Networks+with+a+Full-precision+Auxiliary+Module">semantic scholar</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><b>Learning and memorizing representative prototypes for 3D point cloud semantic and instance segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>T. He, D. Gong, Z. Tian, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/He2020PC1.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+and+Memorizing+Representative+Prototypes+for+{3D}+Point+Cloud+Semantic+and+Instance+Segmentation+He,+Tong+and+Gong,+Dong+and+Tian,+Zhi+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+and+Memorizing+Representative+Prototypes+for+{3D}+Point+Cloud+Semantic+and+Instance+Segmentation">semantic scholar</a></p>
</li>
<li><p><b>Instance-aware embedding for point cloud instance segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>T. He, Y. Liu, C. Shen, X. Wang, C. Sun</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/He2020InstanceAware.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Instance-Aware+Embedding+for+Point+Cloud+Instance+Segmentation+He,+Tong+and+Liu,+Yifan+and+Shen,+Chunhua+and+Wang,+Xinlong+and+Sun,+Changming">google scholar</a><a href="https://www.semanticscholar.org/search?q=Instance-Aware+Embedding+for+Point+Cloud+Instance+Segmentation">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2020WeightingRL_arXiv.jpg"><b>Weighing counts: sequential crowd counting by reinforcement learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Liu, H. Lu, H. Zou, H. Xiong, Z. Cao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2007.08260">arXiv</a><a href="data/bibtex/Liu2020WeightingRL.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Weighing+Counts:+Sequential+Crowd+Counting+by+Reinforcement+Learning+Liu,+Liang+and+Lu,+Hao+and+Zou,+Hongwei+and+Xiong,+Haipeng+and+Cao,+Zhiguo+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Weighing+Counts:+Sequential+Crowd+Counting+by+Reinforcement+Learning">semantic scholar</a><a href="https://github.com/poppinace/libranet">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2020EfficientSemantic_arXiv.jpg"><b>Efficient semantic video segmentation with per-frame inference</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Liu, C. Shen, C. Yu, J. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2002.11433">arXiv</a><a href="data/bibtex/Liu2020EfficientSemantic.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Semantic+Video+Segmentation+with+Per-frame+Inference+Liu,+Yifan+and+Shen,+Chunhua+and+Yu,+Changqian+and+Wang,+Jingdong">google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+Semantic+Video+Segmentation+with+Per-frame+Inference">semantic scholar</a><a href="https://tinyurl.com/segment-video">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Tian2020CondInst_arXiv.jpg"><b>Conditional convolutions for instance segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Z. Tian, C. Shen, H. Chen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.05664">arXiv</a><a href="data/bibtex/Tian2020CondInst.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Conditional+Convolutions+for+Instance+Segmentation+Tian,+Zhi+and+Shen,+Chunhua+and+Chen,+Hao">google scholar</a><a href="https://www.semanticscholar.org/search?q=Conditional+Convolutions+for+Instance+Segmentation">semantic scholar</a><a href="https://github.com/aim-uofa/adet">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><b>Soft expert reward learning for vision-and-language navigation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Wang, Q. Wu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Wang2020Soft.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Soft+Expert+Reward+Learning+for+Vision-and-Language+Navigation+Wang,+Hu+and+Wu,+Qi+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Soft+Expert+Reward+Learning+for+Vision-and-Language+Navigation">semantic scholar</a></p>
</li>
<li><p><b>AE TextSpotter: learning visual and linguistic representation for ambiguous text spotting</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>W. Wang, X. Liu, X. Ji, E. Xie, D. Liang, Z. Yang, T. Lu, C. Shen, P. Luo</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Wang2020AET.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={AE+TextSpotter}:+Learning+Visual+and+Linguistic+Representation+for+Ambiguous+Text+Spotting+Wang,+Wenhai+and+Liu,+Xuebo+and+Ji,+Xiaozhong+and+Xie,+Enze+and+Liang,+Ding+and+Yang,+ZhiBo+and+Lu,+Tong+and+Shen,+Chunhua+and+Luo,+Ping">google scholar</a><a href="https://www.semanticscholar.org/search?q={AE+TextSpotter}:+Learning+Visual+and+Linguistic+Representation+for+Ambiguous+Text+Spotting">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2020SuperRes_arXiv.jpg"><b>Scene text image super-resolution in the wild</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>W. Wang, E. Xie, X. Liu, W. Wang, D. Liang, C. Shen, X. Bai</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2005.03341">arXiv</a><a href="data/bibtex/Wang2020SuperRes.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Scene+Text+Image+Super-Resolution+in+the+Wild+Wang,+Wenjia+and+Xie,+Enze+and+Liu,+Xuebo+and+Wang,+Wenhai+and+Liang,+Ding+and+Shen,+Chunhua+and+Bai,+Xiang">google scholar</a><a href="https://www.semanticscholar.org/search?q=Scene+Text+Image+Super-Resolution+in+the+Wild">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2020SOLO_arXiv.jpg"><b>SOLO: segmenting objects by locations</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Wang, T. Kong, C. Shen, Y. Jiang, L. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1912.04488">arXiv</a><a href="data/bibtex/Wang2020SOLO.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SOLO}:+Segmenting+Objects+by+Locations+Wang,+Xinlong+and+Kong,+Tao+and+Shen,+Chunhua+and+Jiang,+Yuning+and+Li,+Lei">google scholar</a><a href="https://www.semanticscholar.org/search?q={SOLO}:+Segmenting+Objects+by+Locations">semantic scholar</a><a href="https://github.com/aim-uofa/adet">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Xie2020Segtransp_arXiv.jpg"><b>Segmenting transparent objects in the wild</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>E. Xie, W. Wang, W. Wang, M. Ding, C. Shen, P. Luo</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.13948">arXiv</a><a href="data/bibtex/Xie2020Segtransp.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Segmenting+Transparent+Objects+in+the+Wild+Xie,+Enze+and+Wang,+Wenjia+and+Wang,+Wenhai+and+Ding,+Mingyu+and+Shen,+Chunhua+and+Luo,+Ping">google scholar</a><a href="https://www.semanticscholar.org/search?q=Segmenting+Transparent+Objects+in+the+Wild">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Yu2020RepGraphNet_arXiv.jpg"><b>Representative graph neural network</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Yu, Y. Liu, C. Gao, C. Shen, N. Sang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2008.05202">arXiv</a><a href="data/bibtex/Yu2020RepGraphNet.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Representative+Graph+Neural+Network+Yu,+Changqian+and+Liu,+Yifan+and+Gao,+Changxin+and+Shen,+Chunhua+and+Sang,+Nong">google scholar</a><a href="https://www.semanticscholar.org/search?q=Representative+Graph+Neural+Network">semantic scholar</a><a href="https://github.com/ycszen/RepGraph">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/SoloV22020_arXiv.jpg"><b>SOLOv2: dynamic and fast instance segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Wang, R. Zhang, T. Kong, L. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.10152">arXiv</a><a href="data/bibtex/SoloV22020.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SOLOv2}:+Dynamic+and+Fast+Instance+Segmentation+Wang,+Xinlong+and+Zhang,+Rufeng+and+Kong,+Tao+and+Li,+Lei+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q={SOLOv2}:+Dynamic+and+Fast+Instance+Segmentation">semantic scholar</a><a href="https://git.io/AdelaiDet">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.04688.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19He8_arXiv.jpg"></a><b>Knowledge adaptation for efficient semantic segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>T. He, C. Shen, Z. Tian, D. Gong, C. Sun, Y. Yan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.04688">arXiv</a><a href="data/bibtex/CVPR19He8.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Knowledge+Adaptation+for+Efficient+Semantic+Segmentation+He,+Tong+and+Shen,+Chunhua+and+Tian,+Zhi+and+Gong,+Dong+and+Sun,+Changming+and+Yan,+Youliang">google scholar</a><a href="https://www.semanticscholar.org/search?q=Knowledge+Adaptation+for+Efficient+Semantic+Segmentation">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1811.11903.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19HuiLi2_arXiv.jpg"></a><b>Visual question answering as reading comprehension</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Li, P. Wang, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1811.11903">arXiv</a><a href="data/bibtex/CVPR19HuiLi2.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+Question+Answering+as+Reading+Comprehension+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Visual+Question+Answering+as+Reading+Comprehension">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1810.10804.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Nekrasov1_arXiv.jpg"></a><b>Fast neural architecture search of compact semantic segmentation models via auxiliary cells</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>V. Nekrasov, H. Chen, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1810.10804">arXiv</a><a href="data/bibtex/CVPR19Nekrasov1.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Neural+Architecture+Search+of+Compact+Semantic+Segmentation+Models+via+Auxiliary+Cells+Nekrasov,+Vladimir+and+Chen,+Hao+and+Shen,+Chunhua+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+Neural+Architecture+Search+of+Compact+Semantic+Segmentation+Models+via+Auxiliary+Cells">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.02120.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Tian7_arXiv.jpg"></a><b>Decoders matter for semantic segmentation: data-dependent decoding enables flexible feature aggregation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Z. Tian, T. He, C. Shen, Y. Yan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.02120">arXiv</a><a href="data/bibtex/CVPR19Tian7.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Decoders+Matter+for+Semantic+Segmentation:+Data-Dependent+Decoding+Enables+Flexible+Feature+Aggregation+Tian,+Zhi+and+He,+Tong+and+Shen,+Chunhua+and+Yan,+Youliang">google scholar</a><a href="https://www.semanticscholar.org/search?q=Decoders+Matter+for+Semantic+Segmentation:+Data-Dependent+Decoding+Enables+Flexible+Feature+Aggregation">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1812.04794.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19PengWang0_arXiv.jpg"></a><b>Neighbourhood watch: referring expression comprehension via language-guided graph attention networks</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>P. Wang, Q. Wu, J. Cao, C. Shen, L. Gao, A. vanden Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1812.04794">arXiv</a><a href="data/bibtex/CVPR19PengWang0.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Neighbourhood+Watch:+Referring+Expression+Comprehension+via+Language-guided+Graph+Attention+Networks+Wang,+Peng+and+Wu,+Qi+and+Cao,+Jiewei+and+Shen,+Chunhua+and+Gao,+Lianli+and+{vanden+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Neighbourhood+Watch:+Referring+Expression+Comprehension+via+Language-guided+Graph+Attention+Networks">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1902.09852.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Wang4_arXiv.jpg"></a><b>Associatively segmenting instances and semantics in point clouds</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Wang, S. Liu, X. Shen, C. Shen, J. Jia</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1902.09852">arXiv</a><a href="data/bibtex/CVPR19Wang4.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Associatively+Segmenting+Instances+and+Semantics+in+Point+Clouds+Wang,+Xinlong+and+Liu,+Shu+and+Shen,+Xiaoyong+and+Shen,+Chunhua+and+Jia,+Jiaya">google scholar</a><a href="https://www.semanticscholar.org/search?q=Associatively+Segmenting+Instances+and+Semantics+in+Point+Clouds">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1904.10293.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Yan9_arXiv.jpg"></a><b>Attention-guided network for ghost-free high dynamic range imaging</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Q. Yan, D. Gong, Q. Shi, A. van den Hengel, C. Shen, I. Reid, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1904.10293">arXiv</a><a href="data/bibtex/CVPR19Yan9.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Attention-guided+Network+for+Ghost-free+High+Dynamic+Range+Imaging+Yan,+Qingsen+and+Gong,+Dong+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Reid,+Ian+and+Zhang,+Yanning">google scholar</a><a href="https://www.semanticscholar.org/search?q=Attention-guided+Network+for+Ghost-free+High+Dynamic+Range+Imaging">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.02351.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Zhang5_arXiv.jpg"></a><b>CANet: class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Zhang, G. Lin, F. Liu, R. Yao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.02351">arXiv</a><a href="data/bibtex/CVPR19Zhang5.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={CANet}:+Class-Agnostic+Segmentation+Networks+with+Iterative+Refinement+and+Attentive+Few-Shot+Learning+Zhang,+Chi+and+Lin,+Guosheng+and+Liu,+Fayao+and+Yao,+Rui+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q={CANet}:+Class-Agnostic+Segmentation+Networks+with+Iterative+Refinement+and+Attentive+Few-Shot+Learning">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR19Zhang6_PDF.jpg"><b>Mind your neighbours: image annotation with metadata neighbourhood graph co-attention networks</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>J. Zhang, Q. Wu, J. Zhang, C. Shen, J. Lu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Mind_Your_Neighbours_Image_Annotation_With_Metadata_Neighbourhood_Graph_Co-Attention_CVPR_2019_paper.pdf">link</a><a href="data/bibtex/CVPR19Zhang6.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mind+Your+Neighbours:+Image+Annotation+with+Metadata+Neighbourhood+Graph+Co-Attention+Networks+Zhang,+Junjie+and+Wu,+Qi+and+Zhang,+Jian+and+Shen,+Chunhua+and+Lu,+Jianfeng">google scholar</a><a href="https://www.semanticscholar.org/search?q=Mind+Your+Neighbours:+Image+Annotation+with+Metadata+Neighbourhood+Graph+Co-Attention+Networks">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1811.10413.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Zhuang3_arXiv.jpg"></a><b>Structured binary neural networks for accurate image classification and semantic segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>B. Zhuang, C. Shen, M. Tan, L. Liu, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1811.10413">arXiv</a><a href="data/bibtex/CVPR19Zhuang3.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Binary+Neural+Networks+for+Accurate+Image+Classification+and+Semantic+Segmentation+Zhuang,+Bohan+and+Shen,+Chunhua+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+Binary+Neural+Networks+for+Accurate+Image+Classification+and+Semantic+Segmentation">semantic scholar</a><a href="https://bitbucket.org/jingruixiaozhuang/group-net-image-classification/">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Matting2019Lu_arXiv.jpg"><b>Indices matter: learning to index for deep image matting</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Lu, Y. Dai, C. Shen, S. Xu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.00672">arXiv</a><a href="data/bibtex/Matting2019Lu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Indices+Matter:+Learning+to+Index+for+Deep+Image+Matting+Lu,+Hao+and+Dai,+Yutong+and+Shen,+Chunhua+and+Xu,+Songcen">google scholar</a><a href="https://www.semanticscholar.org/search?q=Indices+Matter:+Learning+to+Index+for+Deep+Image+Matting">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1904.01355.pdf"><img class="imgP  right"   src="data/thumbnail/FCOS2019Tian_arXiv.jpg"></a><b>FCOS: fully convolutional one-stage object detection</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Z. Tian, C. Shen, H. Chen, T. He</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1904.01355">arXiv</a><a href="data/bibtex/FCOS2019Tian.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FCOS}:+Fully+Convolutional+One-Stage+Object+Detection+Tian,+Zhi+and+Shen,+Chunhua+and+Chen,+Hao+and+He,+Tong">google scholar</a><a href="https://www.semanticscholar.org/search?q={FCOS}:+Fully+Convolutional+One-Stage+Object+Detection">semantic scholar</a><a href="https://tinyurl.com/FCOSv1">project webpage</a></p>
</li>
<li><p><b>Efficient and accurate arbitrary-shaped text detection with pixel aggregation network</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>W. Wang, E. Xie, X. Song, Y. Zang, W. Wang, T. Lu, G. Yu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/TextDet2019Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+and+Accurate+Arbitrary-Shaped+Text+Detection+with+Pixel+Aggregation+Network+Wang,+Wenhai+and+Xie,+Enze+and+Song,+Xiaoge+and+Zang,+Yuhang+and+Wang,+Wenjia+and+Lu,+Tong+and+Yu,+Gang+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+and+Accurate+Arbitrary-Shaped+Text+Detection+with+Pixel+Aggregation+Network">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/OpenSet2019Xiong_arXiv.jpg"><b>From open set to closed set: counting objects by spatial divide-and-conquer</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Xiong, H. Lu, C. Liu, L. Liu, Z. Cao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.06473">arXiv</a><a href="data/bibtex/OpenSet2019Xiong.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=From+Open+Set+to+Closed+Set:+Counting+Objects+by+Spatial+Divide-and-Conquer+Xiong,+Haipeng+and+Lu,+Hao+and+Liu,+Chengxin+and+Liu,+Liang+and+Cao,+Zhiguo+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=From+Open+Set+to+Closed+Set:+Counting+Objects+by+Spatial+Divide-and-Conquer">semantic scholar</a><a href="https://github.com/xhp-hust-2018-2011/S-DCNet">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/VNL2019Yin_arXiv.jpg"><b>Enforcing geometric constraints of virtual normal for depth prediction</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>W. Yin, Y. Liu, C. Shen, Y. Yan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1907.12209">arXiv</a><a href="data/bibtex/VNL2019Yin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Enforcing+geometric+constraints+of+virtual+normal+for+depth+prediction+Yin,+Wei+and+Liu,+Yifan+and+Shen,+Chunhua+and+Yan,+Youliang">google scholar</a><a href="https://www.semanticscholar.org/search?q=Enforcing+geometric+constraints+of+virtual+normal+for+depth+prediction">semantic scholar</a><a href="https://github.com/YvanYin/VNL_Monocular_Depth_Prediction">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Temporal2019Zhang_arXiv.jpg"><b>Exploiting temporal consistency for real-time video depth estimation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Zhang, C. Shen, Y. Li, Y. Cao, Y. Liu, Y. Yan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.03706">arXiv</a><a href="data/bibtex/Temporal2019Zhang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploiting+temporal+consistency+for+real-time+video+depth+estimation+Zhang,+Haokui+and+Shen,+Chunhua+and+Li,+Ying+and+Cao,+Yuanzhouhan+and+Liu,+Yu+and+Yan,+Youliang">google scholar</a><a href="https://www.semanticscholar.org/search?q=Exploiting+temporal+consistency+for+real-time+video+depth+estimation">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/PersonReID2019Zhang_arXiv.jpg"><b>Self-training with progressive augmentation for unsupervised cross-domain person re-identification</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Zhang, J. Cao, C. Shen, M. You</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1907.13315">arXiv</a><a href="data/bibtex/PersonReID2019Zhang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Self-Training+with+Progressive+Augmentation+for+Unsupervised+Cross-Domain+Person+Re-Identification+Zhang,+Xinyu+and+Cao,+Jiewei+and+Shen,+Chunhua+and+You,+Mingyu">google scholar</a><a href="https://www.semanticscholar.org/search?q=Self-Training+with+Progressive+Augmentation+for+Unsupervised+Cross-Domain+Person+Re-Identification">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Scale2019Bian_arXiv.jpg"><b>Unsupervised scale-consistent depth and ego-motion learning from monocular video</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>J. Bian, Z. Li, N. Wang, H. Zhan, C. Shen, M. Cheng, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.10553">arXiv</a><a href="data/bibtex/Scale2019Bian.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Scale-consistent+Depth+and+Ego-motion+Learning+from+Monocular+Video+Bian,+Jia-Wang+and+Li,+Zhichao+and+Wang,+Naiyan+and+Zhan,+Huangying+and+Shen,+Chunhua+and+Cheng,+Ming-Ming+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Scale-consistent+Depth+and+Ego-motion+Learning+from+Monocular+Video">semantic scholar</a><a href="https://github.com/JiawangBian/SC-SfMLearner-Release">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1911.00888.pdf"><img class="imgP  right"   src="data/thumbnail/Cao2019GAN_arXiv.jpg"></a><b>Multi-marginal wasserstein GAN</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>J. Cao, L. Mo, Y. Zhang, K. Jia, C. Shen, M. Tan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1911.00888">arXiv</a><a href="data/bibtex/Cao2019GAN.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-marginal+Wasserstein+{GAN}+Cao,+Jiezhang+and+Mo,+Langyuan+and+Zhang,+Yifan+and+Jia,+Kui+and+Shen,+Chunhua+and+Tan,+Mingkui">google scholar</a><a href="https://www.semanticscholar.org/search?q=Multi-marginal+Wasserstein+{GAN}">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1711.10703.pdf"><img class="imgP  right"   src="data/thumbnail/Chen2018CVPR_arXiv.jpg"></a><b>FSRNet: end-to-end learning face super-resolution with facial priors</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Chen, Y. Tai, X. Liu, C. Shen, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.10703">arXiv</a><a href="data/bibtex/Chen2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FSRNet}:+End-to-End+Learning+Face+Super-Resolution+with+Facial+Priors+Chen,+Yu+and+Tai,+Ying+and+Liu,+Xiaoming+and+Shen,+Chunhua+and+Yang,+Jian">google scholar</a><a href="https://www.semanticscholar.org/search?q={FSRNet}:+End-to-End+Learning+Face+Super-Resolution+with+Facial+Priors">semantic scholar</a><a href="https://github.com/tyshiwo/FSRNet">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/He2018CVPR_arXiv.jpg"><b>An end-to-end textspotter with explicit alignment and attention</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>T. He, Z. Tian, W. Huang, C. Shen, Y. Qiao, C. Sun</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1803.03474">arXiv</a><a href="data/bibtex/He2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+end-to-end+TextSpotter+with+Explicit+Alignment+and+Attention+He,+Tong+and+Tian,+Zhi+and+Huang,+Weilin+and+Shen,+Chunhua+and+Qiao,+Yu+and+Sun,+Changming">google scholar</a><a href="https://www.semanticscholar.org/search?q=An+end-to-end+TextSpotter+with+Explicit+Alignment+and+Attention">semantic scholar</a><a href="https://github.com/tonghe90/textspotter">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Ma2018CVPR_a_arXiv.jpg"><b>Visual question answering with memory-augmented networks</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Ma, C. Shen, A. Dick, Q. Wu, P. Wang, A. van den Hengel, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1707.04968">arXiv</a><a href="data/bibtex/Ma2018CVPR_a.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+Question+Answering+with+Memory-Augmented+Networks+Ma,+Chao+and+Shen,+Chunhua+and+Dick,+Anthony+and+Wu,+Qi+and+Wang,+Peng+and+{van+den+Hengel},+Anton+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Visual+Question+Answering+with+Memory-Augmented+Networks">semantic scholar</a></p>
</li>
<li><p><b>Bootstrapping the performance of webly supervised semantic segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>T. Shen, G. Lin, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/TongShen2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Bootstrapping+the+Performance+of+Webly+Supervised+Semantic+Segmentation+Shen,+Tong+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Bootstrapping+the+Performance+of+Webly+Supervised+Semantic+Segmentation">semantic scholar</a><a href="https://github.com/ascust/BDWSS">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Song2018CVPR_arXiv.jpg"><b>VITAL: visual tracking via adversarial learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Song, C. Ma, X. Wu, L. Gong, L. Bao, W. Zuo, C. Shen, R. Lau, M. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1804.04273">arXiv</a><a href="data/bibtex/Song2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={VITAL}:+VIsual+Tracking+via+Adversarial+Learning+Song,+Yibing+and+Ma,+Chao+and+Wu,+Xiaohe+and+Gong,+Lijun+and+Bao,+Linchao+and+Zuo,+Wangmeng+and+Shen,+Chunhua+and+Lau,+Rynson+and+Yang,+Ming-Hsuan">google scholar</a><a href="https://www.semanticscholar.org/search?q={VITAL}:+VIsual+Tracking+via+Adversarial+Learning">semantic scholar</a><a href="https://ybsong00.github.io/cvpr18_tracking/index">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2018CVPR_arXiv.jpg"><b>Repulsion loss: detecting pedestrians in a crowd</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Wang, T. Xiao, Y. Jiang, S. Shao, J. Sun, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.07752">arXiv</a><a href="data/bibtex/Wang2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Repulsion+Loss:+Detecting+Pedestrians+in+a+Crowd+Wang,+Xinlong+and+Xiao,+Tete+and+Jiang,+Yuning+and+Shao,+Shuai+and+Sun,+Jian+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Repulsion+Loss:+Detecting+Pedestrians+in+a+Crowd">semantic scholar</a></p>
<ol reversed>
<li><p>Others have implemented our paper: <a href="https://github.com/bailvwangzi/repulsion_loss_ssd">Repulsion loss in SSD</a> and <a href="https://github.com/rainofmine/Repulsion_Loss">Repulsion loss in RetinaNet</a>.</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/QWu2018CVPR_arXiv.jpg"><b>Are you talking to me? reasoned visual dialog generation through adversarial learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Q. Wu, P. Wang, C. Shen, I. Reid, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.07613">arXiv</a><a href="data/bibtex/QWu2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Are+You+Talking+to+Me?+Reasoned+Visual+Dialog+Generation+through+Adversarial+Learning+Wu,+Qi+and+Wang,+Peng+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Are+You+Talking+to+Me?+Reasoned+Visual+Dialog+Generation+through+Adversarial+Learning">semantic scholar</a></p>
</li>
<li><p><b>Monocular relative depth perception with web stereo data supervision</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>K. Xian, C. Shen, Z. Cao, H. Lu, Y. Xiao, R. Li, Z. Luo</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Xian2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Monocular+Relative+Depth+Perception+with+Web+Stereo+Data+Supervision+Xian,+Ke+and+Shen,+Chunhua+and+Cao,+Zhiguo+and+Lu,+Hao+and+Xiao,+Yang+and+Li,+Ruibo+and+Luo,+Zhenbo">google scholar</a><a href="https://www.semanticscholar.org/search?q=Monocular+Relative+Depth+Perception+with+Web+Stereo+Data+Supervision">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhuang2018CVPR_b_arXiv.jpg"><b>Towards effective low-bitwidth convolutional neural networks</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>B. Zhuang, C. Shen, M. Tan, L. Liu, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.00205">arXiv</a><a href="data/bibtex/Zhuang2018CVPR_b.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Effective+Low-bitwidth+Convolutional+Neural+Networks+Zhuang,+Bohan+and+Shen,+Chunhua+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+Effective+Low-bitwidth+Convolutional+Neural+Networks">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhuang2018CVPR_a_arXiv.jpg"><b>Parallel attention: a unified framework for visual object discovery through dialogs and queries</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>B. Zhuang, Q. Wu, C. Shen, I. Reid, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.06370">arXiv</a><a href="data/bibtex/Zhuang2018CVPR_a.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Parallel+Attention:+A+Unified+Framework+for+Visual+Object+Discovery+through+Dialogs+and+Queries+Zhuang,+Bohan+and+Wu,+Qi+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Parallel+Attention:+A+Unified+Framework+for+Visual+Object+Discovery+through+Dialogs+and+Queries">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1807.10097.pdf"><img class="imgP  right"   src="data/thumbnail/Deng2018ECCV_arXiv.jpg"></a><b>Learning to predict crisp boundaries</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>R. Deng, C. Shen, S. Liu, H. Wang, X. Liu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1807.10097">arXiv</a><a href="data/bibtex/Deng2018ECCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+to+Predict+Crisp+Boundaries+Deng,+Ruoxi+and+Shen,+Chunhua+and+Liu,+Shengjun+and+Wang,+Huibing+and+Liu,+Xinru">google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+to+Predict+Crisp+Boundaries">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhang2018ECCV_arXiv.jpg"><b>Goal-oriented visual question generation via intermediate rewards</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>J. Zhang, Q. Wu, C. Shen, J. Zhang, J. Lu, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.07614">arXiv</a><a href="data/bibtex/Zhang2018ECCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Goal-Oriented+Visual+Question+Generation+via+Intermediate+Rewards+Zhang,+Junjie+and+Wu,+Qi+and+Shen,+Chunhua+and+Zhang,+Jian+and+Lu,+Jianfeng+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Goal-Oriented+Visual+Question+Generation+via+Intermediate+Rewards">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1806.04895.pdf"><img class="imgP  right"   src="data/thumbnail/Cao2018ICML_arXiv.jpg"></a><b>Adversarial learning with local coordinate coding</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>J. Cao, Y. Guo, Q. Wu, C. Shen, J. Huang, M. Tan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. International Conference on Machine Learning (ICML&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1806.04895">arXiv</a><a href="data/bibtex/Cao2018ICML.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adversarial+Learning+with+Local+Coordinate+Coding+Cao,+Jiezhang+and+Guo,+Yong+and+Wu,+Qingyao+and+Shen,+Chunhua+and+Huang,+Junzhou+and+Tan,+Mingkui">google scholar</a><a href="https://www.semanticscholar.org/search?q=Adversarial+Learning+with+Local+Coordinate+Coding">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1612.02583.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017Gong_arXiv.jpg"></a><b>From motion blur to motion flow: a deep learning solution for removing heterogeneous motion blur</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>D. Gong, J. Yang, L. Liu, Y. Zhang, I. Reid, C. Shen, A. van den Hengel, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1612.02583">arXiv</a><a href="data/bibtex/CVPR2017Gong.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=From+Motion+Blur+to+Motion+Flow:+a+Deep+Learning+Solution+for+Removing+Heterogeneous+Motion+Blur+Gong,+Dong+and+Yang,+Jie+and+Liu,+Lingqiao+and+Zhang,+Yanning+and+Reid,+Ian+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng">google scholar</a><a href="https://www.semanticscholar.org/search?q=From+Motion+Blur+to+Motion+Flow:+a+Deep+Learning+Solution+for+Removing+Heterogeneous+Motion+Blur">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1611.09967.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017YaoLi_arXiv.jpg"></a><b>Sequential person recognition in photo albums with a recurrent network</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Li, G. Lin, B. Zhuang, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1611.09967">arXiv</a><a href="data/bibtex/CVPR2017YaoLi.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Sequential+Person+Recognition+in+Photo+Albums+with+a+Recurrent+Network+Li,+Yao+and+Lin,+Guosheng+and+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Sequential+Person+Recognition+in+Photo+Albums+with+a+Recurrent+Network">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1611.06612.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017Lin_arXiv.jpg"></a><b>RefineNet: multi-path refinement networks for high-resolution semantic segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>G. Lin, A. Milan, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1611.06612">arXiv</a><a href="data/bibtex/CVPR2017Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={RefineNet}:+Multi-Path+Refinement+Networks+for+High-Resolution+Semantic+Segmentation+Lin,+Guosheng+and+Milan,+Anton+and+Shen,+Chunhua+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q={RefineNet}:+Multi-Path+Refinement+Networks+for+High-Resolution+Semantic+Segmentation">semantic scholar</a><a href="https://github.com/guosheng/refinenet">project webpage</a></p>
<ol reversed>
<li><p><a href="https://github.com/DrSleep/light-weight-refinenet">Light-weight RefineNet with Pytorch code</a>.</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR2017WangAttend_PDF.jpg"><b>Multi-attention network for one shot learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>P. Wang, L. Liu, C. Shen, Z. Huang, A. van den Hengel, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Multi-Attention_Network_for_CVPR_2017_paper.pdf">pdf</a><a href="data/bibtex/CVPR2017WangAttend.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-attention+Network+for+One+Shot+Learning+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Huang,+Zi+and+{van+den+Hengel},+Anton+and+Shen,+Heng+Tao">google scholar</a><a href="https://www.semanticscholar.org/search?q=Multi-attention+Network+for+One+Shot+Learning">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1612.05386.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017WangVQA_arXiv.jpg"></a><b>The VQA-machine: learning how to use existing vision algorithms to answer new questions</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>P. Wang, Q. Wu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1612.05386">arXiv</a><a href="data/bibtex/CVPR2017WangVQA.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=The+{VQA}-Machine:+Learning+How+to+Use+Existing+Vision+Algorithms+to+Answer+New+Questions+Wang,+Peng+and+Wu,+Qi+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=The+{VQA}-Machine:+Learning+How+to+Use+Existing+Vision+Algorithms+to+Answer+New+Questions">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1611.09960.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017Zhuang_arXiv.jpg"></a><b>Attend in groups: a weakly-supervised deep learning framework for learning from web data</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>B. Zhuang, L. Liu, Y. Li, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1611.09960">arXiv</a><a href="data/bibtex/CVPR2017Zhuang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Attend+in+groups:+a+weakly-supervised+deep+learning+framework+for+learning+from+web+data+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Li,+Yao+and+Shen,+Chunhua+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Attend+in+groups:+a+weakly-supervised+deep+learning+framework+for+learning+from+web+data">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV2017Chen_arXiv.jpg"><b>Adversarial PoseNet: a structure-aware convolutional network for human pose estimation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Chen, C. Shen, X. Wei, L. Liu, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1705.00389">arXiv</a><a href="data/bibtex/ICCV2017Chen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adversarial+{PoseNet}:+A+Structure-aware+Convolutional+Network+for+Human+Pose+Estimation+Chen,+Yu+and+Shen,+Chunhua+and+Wei,+Xiu-Shen+and+Liu,+Lingqiao+and+Yang,+Jian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Adversarial+{PoseNet}:+A+Structure-aware+Convolutional+Network+for+Human+Pose+Estimation">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV2017HuiLi_arXiv.jpg"><b>Towards end-to-end text spotting with convolutional recurrent neural networks</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Li, P. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1707.03985">arXiv</a><a href="data/bibtex/ICCV2017HuiLi.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+End-to-end+Text+Spotting+with+Convolutional+Recurrent+Neural+Networks+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+End-to-end+Text+Spotting+with+Convolutional+Recurrent+Neural+Networks">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV2017WeiLiu_arXiv.jpg"><b>Semi-global weighted least squares in image filtering</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>W. Liu, X. Chen, C. Shen, Z. Liu, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1705.01674">arXiv</a><a href="data/bibtex/ICCV2017WeiLiu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Semi-Global+Weighted+Least+Squares+in+Image+Filtering+Liu,+Wei+and+Chen,+Xiaogang+and+Shen,+Chuanhua+and+Liu,+Zhi+and+Yang,+Jie">google scholar</a><a href="https://www.semanticscholar.org/search?q=Semi-Global+Weighted+Least+Squares+in+Image+Filtering">semantic scholar</a></p>
</li>
<li><p><b>When unsupervised domain adaptation meets tensor representations</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Lu, L. Zhang, Z. Cao, W. Wei, K. Xian, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/ICCV2017Haolu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=When+Unsupervised+Domain+Adaptation+Meets+Tensor+Representations+Lu,+Hao+and+Zhang,+Lei+and+Cao,+Zhiguo+and+Wei,+Wei+and+Xian,+Ke+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=When+Unsupervised+Domain+Adaptation+Meets+Tensor+Representations">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV2017Zhuang_arXiv.jpg"><b>Towards context-aware interaction recognition</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>B. Zhuang, L. Liu, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1703.06246">arXiv</a><a href="data/bibtex/ICCV2017Zhuang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Context-aware+Interaction+Recognition+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+Context-aware+Interaction+Recognition">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1504.01013.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16labelling_arXiv.jpg"></a><b>Efficient piecewise training of deep structured models for semantic segmentation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>G. Lin, C. Shen, A. van dan Hengel, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1504.01013">arXiv</a><a href="data/bibtex/CVPR16labelling.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+piecewise+training+of+deep+structured+models+for+semantic+segmentation+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+dan+Hengel},+Anton+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+piecewise+training+of+deep+structured+models+for+semantic+segmentation">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1604.01146.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16Zeroshot_arXiv.jpg"></a><b>Less is more: zero-shot learning from online textual documents with noise suppression</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>R. Qiao, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1604.01146">arXiv</a><a href="data/bibtex/CVPR16Zeroshot.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Less+is+More:+Zero-shot+Learning+from+Online+Textual+Documents+with+Noise+Suppression+Qiao,+Ruizhi+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Less+is+More:+Zero-shot+Learning+from+Online+Textual+Documents+with+Noise+Suppression">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1602.04422.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16Irregular_arXiv.jpg"></a><b>What's wrong with that object? identifying irregular object from images by modelling the detection score distribution</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>P. Wang, L. Liu, C. Shen, Z. Huang, A. van den Hengel, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1602.04422">arXiv</a><a href="data/bibtex/CVPR16Irregular.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=What's+Wrong+with+that+Object?+Identifying+Irregular+Object+From+Images+by+Modelling+the+Detection+Score+Distribution+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Huang,+Zi+and+{van+den+Hengel},+Anton+and+Shen,+Heng+Tao">google scholar</a><a href="https://www.semanticscholar.org/search?q=What's+Wrong+with+that+Object?+Identifying+Irregular+Object+From+Images+by+Modelling+the+Detection+Score+Distribution">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1506.01144.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16What_arXiv.jpg"></a><b>What value do explicit high level concepts have in vision to language problems</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Q. Wu, C. Shen, L. Liu, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1506.01144">arXiv</a><a href="data/bibtex/CVPR16What.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=What+Value+Do+Explicit+High+Level+Concepts+Have+in+Vision+to+Language+Problems+Wu,+Qi+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=What+Value+Do+Explicit+High+Level+Concepts+Have+in+Vision+to+Language+Problems">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1511.06973.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16AMA_arXiv.jpg"></a><b>Ask me anything: free-form visual question answering based on knowledge from external sources</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Q. Wu, P. Wang, C. Shen, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1511.06973">arXiv</a><a href="data/bibtex/CVPR16AMA.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Ask+Me+Anything:+Free-form+Visual+Question+Answering+Based+on+Knowledge+from+External+Sources+Wu,+Qi+and+Wang,+Peng+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Ask+Me+Anything:+Free-form+Visual+Question+Answering+Based+on+Knowledge+from+External+Sources">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1603.02844.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16Binary_arXiv.jpg"></a><b>Fast training of triplet-based deep binary embedding networks</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>B. Zhuang, G. Lin, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.02844">arXiv</a><a href="data/bibtex/CVPR16Binary.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Training+of+Triplet-based+Deep+Binary+Embedding+Networks+Zhuang,+Bohan+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+Training+of+Triplet-based+Deep+Binary+Embedding+Networks">semantic scholar</a><a href="https://bitbucket.org/jingruixiaozhuang/fast-training-of-triplet-based-deep-binary-embedding-networks">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ECCV16Li_arXiv.jpg"><b>Image co-localization by mimicking a good detector's confidence score distribution</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.04619">arXiv</a><a href="data/bibtex/ECCV16Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Image+Co-localization+by+Mimicking+a+Good+Detector's+Confidence+Score+Distribution+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Image+Co-localization+by+Mimicking+a+Good+Detector's+Confidence+Score+Distribution">semantic scholar</a></p>
</li>
<li><p><b>Cluster sparsity field for hyperspectral imagery denoising</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Y. Zhang, C. Shen, A. van den Hengel, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="data/bibtex/ECCV16hyperspectral.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Cluster+Sparsity+Field+for+Hyperspectral+Imagery+Denoising+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng">google scholar</a><a href="https://www.semanticscholar.org/search?q=Cluster+Sparsity+Field+for+Hyperspectral+Imagery+Denoising">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/NeurIPS2016_PDF.jpg"><b>Image restoration using very deep fully convolutional encoder-decoder networks with symmetric skip connections</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Mao, C. Shen, Y. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.09056">arXiv</a><a href="http://papers.NeurIPS.cc/paper/6172-image-restoration-using-very-deep-convolutional-encoder-decoder-networks-with-symmetric-skip-connections.pdf">link</a><a href="data/bibtex/NeurIPS2016.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Image+Restoration+Using+Very+Deep+Fully+Convolutional+Encoder-Decoder+Networks+with+Symmetric+Skip+Connections+Mao,+Xiao-Jiao+and+Shen,+Chunhua+and+Yang,+Yu-Bin">google scholar</a><a href="https://www.semanticscholar.org/search?q=Image+Restoration+Using+Very+Deep+Fully+Convolutional+Encoder-Decoder+Networks+with+Symmetric+Skip+Connections">semantic scholar</a><a href="https://bitbucket.org/chhshen/image-denoising/">project webpage</a></p>
<ol reversed>
<li><p>Others have <a href="https://github.com/titu1994/Image-Super-Resolution">implemented our paper</a>.</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR15h_PDF.jpg"><b>Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>B. Li, C. Shen, Y. Dai, A. van den Hengel, M. He</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Li_Depth_and_Surface_2015_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR15h.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Depth+and+Surface+Normal+Estimation+from+Monocular+Images+Using+Regression+on+Deep+Features+and+Hierarchical+{CRFs}+Li,+Bo+and+Shen,+Chunhua+and+Dai,+Yuchao+and+{van+den+Hengel},+Anton+and+He,+Mingyi">google scholar</a><a href="https://www.semanticscholar.org/search?q=Depth+and+Surface+Normal+Estimation+from+Monocular+Images+Using+Regression+on+Deep+Features+and+Hierarchical+{CRFs}">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.6382.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15a_arXiv.jpg"></a><b>Mid-level deep pattern mining</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.6382">arXiv</a><a href="data/bibtex/CVPR15a.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mid-level+Deep+Pattern+Mining+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Mid-level+Deep+Pattern+Mining">semantic scholar</a><a href="https://github.com/yaoliUoA/MDPM">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.6387.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15b_arXiv.jpg"></a><b>Deep convolutional neural fields for depth estimation from a single image</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>F. Liu, C. Shen, G. Lin</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.6387">arXiv</a><a href="data/bibtex/CVPR15b.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Convolutional+Neural+Fields+for+Depth+Estimation+from+a+Single+Image+Liu,+Fayao+and+Shen,+Chunhua+and+Lin,+Guosheng">google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Convolutional+Neural+Fields+for+Depth+Estimation+from+a+Single+Image">semantic scholar</a><a href="http://goo.gl/rAKWrS">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.7466.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15d_arXiv.jpg"></a><b>The treasure beneath convolutional layers: cross convolutional layer pooling for image classification</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.7466">arXiv</a><a href="data/bibtex/CVPR15d.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=The+Treasure+beneath+Convolutional+Layers:+Cross+convolutional+layer+Pooling+for+Image+Classification+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=The+Treasure+beneath+Convolutional+Layers:+Cross+convolutional+layer+Pooling+for+Image+Classification">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1503.01543.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15f_arXiv.jpg"></a><b>Learning to rank in person re-identification with metric ensembles</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1503.01543">arXiv</a><a href="data/bibtex/CVPR15f.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+to+rank+in+person+re-identification+with+metric+ensembles+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+to+rank+in+person+re-identification+with+metric+ensembles">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR15c_PDF.jpg"><b>Supervised discrete hashing</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>F. Shen, C. Shen, W. Liu, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Shen_Supervised_Discrete_Hashing_2015_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR15c.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Supervised+Discrete+Hashing+Shen,+Fumin+and+Shen,+Chunhua+and+Liu,+Wei+and+Shen,+Heng+Tao">google scholar</a><a href="https://www.semanticscholar.org/search?q=Supervised+Discrete+Hashing">semantic scholar</a><a href="https://github.com/bd622/DiscretHashing/">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR15g_PDF.jpg"><b>Learning graph structure for multi-label image classification via clique generation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>M. Tan, Q. Shi, A. van den Hengel, C. Shen, J. Gao, F. Hu, Z. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tan_Learning_Graph_Structure_2015_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR15g.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Graph+Structure+for+Multi-label+Image+Classification+via+Clique+Generation+Tan,+Mingkui+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Gao,+Junbin+and+Hu,+Fuyuan+and+Zhang,+Zhen">google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Graph+Structure+for+Multi-label+Image+Classification+via+Clique+Generation">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1504.01492.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15e_arXiv.jpg"></a><b>Efficient SDP inference for fully-connected CRFs based on low-rank decomposition</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>P. Wang, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1504.01492">arXiv</a><a href="data/bibtex/CVPR15e.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+{SDP}+Inference+for+Fully-connected+{CRFs}+Based+on+Low-rank+Decomposition+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+{SDP}+Inference+for+Fully-connected+{CRFs}+Based+on+Low-rank+Decomposition">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV15Zhang_PDF.jpg"><b>Hyperspectral compressive sensing using manifold-structured sparsity prior</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Y. Zhang, F. Li, C. Shen, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zhang_Hyperspectral_Compressive_Sensing_ICCV_2015_paper.pdf">pdf</a><a href="data/bibtex/ICCV15Zhang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Hyperspectral+Compressive+Sensing+Using+Manifold-Structured+Sparsity+Prior+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Li,+Fei+and+Shen,+Chunhua+and+Shi,+Qinfeng">google scholar</a><a href="https://www.semanticscholar.org/search?q=Hyperspectral+Compressive+Sensing+Using+Manifold-Structured+Sparsity+Prior">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/NeurIPS15Lin_PDF.jpg"><b>Deeply learning the messages in message passing inference</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>G. Lin, C. Shen, I. Reid, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1506.02108">arXiv</a><a href="http://papers.NeurIPS.cc/paper/5791-deeply-learning-the-messages-in-message-passing-inference.pdf">pdf</a><a href="data/bibtex/NeurIPS15Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deeply+Learning+the+Messages+in+Message+Passing+Inference+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Deeply+Learning+the+Messages+in+Message+Passing+Inference">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1404.1561.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR14Lin_arXiv.jpg"></a><b>Fast supervised hashing with decision trees for high-dimensional data</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>G. Lin, C. Shen, Q. Shi, A. van den Hengel, D. Suter</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;14), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1404.1561">arXiv</a><a href="https://bitbucket.org/chhshen/fasthash/src">link</a><a href="data/bibtex/CVPR14Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Supervised+Hashing+with+Decision+Trees+for+High-Dimensional+Data+Lin,+Guosheng+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Suter,+David">google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+Supervised+Hashing+with+Decision+Trees+for+High-Dimensional+Data">semantic scholar</a><a href="https://bitbucket.org/chhshen/fasthash/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1407.1151.pdf"><img class="imgP  right"   src="data/thumbnail/ECCV14Lin_arXiv.jpg"></a><b>Optimizing ranking measures for compact binary code learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>G. Lin, C. Shen, J. Wu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;14), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1407.1151">arXiv</a><a href="data/bibtex/ECCV14Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Optimizing+Ranking+Measures+for+Compact+Binary+Code+Learning+Lin,+Guosheng+and+Shen,+Chunhua+and+Wu,+Jianxin">google scholar</a><a href="https://www.semanticscholar.org/search?q=Optimizing+Ranking+Measures+for+Compact+Binary+Code+Learning">semantic scholar</a><a href="https://bitbucket.org/guosheng/structhash">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1407.0786.pdf"><img class="imgP  right"   src="data/thumbnail/ECCV14Paul_arXiv.jpg"></a><b>Strengthening the effectiveness of pedestrian detection with spatially pooled features</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;14), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1407.0786">arXiv</a><a href="data/bibtex/ECCV14Paul.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Strengthening+the+Effectiveness+of+Pedestrian+Detection+with+Spatially+Pooled+Features+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Strengthening+the+Effectiveness+of+Pedestrian+Detection+with+Spatially+Pooled+Features">semantic scholar</a><a href="https://github.com/chhshen/pedestrian-detection">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2014Fisher_arXiv.jpg"><b>Encoding high dimensional local features by sparse coding based Fisher vectors</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Liu, C. Shen, L. Wang, A. van den Hengel, C. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;14), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.6406">arXiv</a><a href="data/bibtex/Liu2014Fisher.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Encoding+High+Dimensional+Local+Features+by+Sparse+Coding+Based+{F}isher+Vectors+Liu,+Lingqiao+and+Shen,+Chunhua+and+Wang,+Lei+and+{van+den+Hengel},+Anton+and+Wang,+Chao">google scholar</a><a href="https://www.semanticscholar.org/search?q=Encoding+High+Dimensional+Local+Features+by+Sparse+Coding+Based+{F}isher+Vectors">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR13bLi_PDF.jpg"><b>Learning compact binary codes for visual tracking</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Li, C. Shen, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/77412">link</a><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Learning_Compact_Binary_2013_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR13bLi.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Compact+Binary+Codes+for+Visual+Tracking+Li,+Xi+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Compact+Binary+Codes+for+Visual+Tracking">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1303.7043.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR13aShen_arXiv.jpg"></a><b>Inductive hashing on manifolds</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>F. Shen, C. Shen, Q. Shi, A. van den Hengel, Z. Tang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1303.7043">arXiv</a><a href="data/bibtex/CVPR13aShen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Inductive+Hashing+on+Manifolds+Shen,+Fumin+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin">google scholar</a><a href="https://www.semanticscholar.org/search?q=Inductive+Hashing+on+Manifolds">semantic scholar</a><a href="https://github.com/chhshen/Hashing-on-Nonlinear-Manifolds">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1304.0840.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR13dWang_arXiv.jpg"></a><b>A fast semidefinite approach to solving binary quadratic problems</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>P. Wang, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1304.0840">arXiv</a><a href="data/bibtex/CVPR13dWang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Fast+Semidefinite+Approach+to+Solving+Binary+Quadratic+Problems+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Fast+Semidefinite+Approach+to+Solving+Binary+Quadratic+Problems">semantic scholar</a><a href="./projects/BQP/">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation, 60 out of 1870 submissions.</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR13cWang_PDF.jpg"><b>Bilinear programming for human activity recognition with unknown MRF graphs</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Z. Wang, Q. Shi, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/77411">link</a><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_Bilinear_Programming_for_2013_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR13cWang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Bilinear+Programming+for+Human+Activity+Recognition+with+unknown+{MRF}+graphs+Wang,+Zhenhua+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Bilinear+Programming+for+Human+Activity+Recognition+with+unknown+{MRF}+graphs">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR13eYao_PDF.jpg"><b>Part-based visual tracking with online latent structural learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>R. Yao, Q. Shi, C. Shen, Y. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/77413">link</a><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yao_Part-Based_Visual_Tracking_2013_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR13eYao.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Part-based+Visual+Tracking+with+Online+Latent+Structural+Learning+Yao,+Rui+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Part-based+Visual+Tracking+with+Online+Latent+Structural+Learning">semantic scholar</a><a href="https://github.com/chhshen/PartTracking">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV13Li_arXiv.jpg"><b>Contextual hypergraph modeling for salient object detection</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Li, Y. Li, C. Shen, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1310.5767">arXiv</a><a href="data/bibtex/ICCV13Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Contextual+Hypergraph+Modeling+for+Salient+Object+Detection+Li,+Xi+and+Li,+Yao+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Contextual+Hypergraph+Modeling+for+Salient+Object+Detection">semantic scholar</a><a href="https://bitbucket.org/chhshen/saliency-detection">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV13Lin_arXiv.jpg"><b>A general two-step approach to learning-based hashing</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>G. Lin, C. Shen, D. Suter, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1309.1853">arXiv</a><a href="data/bibtex/ICCV13Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+General+Two-step+Approach+to+Learning-Based+Hashing+Lin,+Guosheng+and+Shen,+Chunhua+and+Suter,+David+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=A+General+Two-step+Approach+to+Learning-Based+Hashing">semantic scholar</a><a href="https://bitbucket.org/guosheng/two-step-hashing/">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV13Pai_arXiv.jpg"><b>Efficient pedestrian detection by directly optimizing the partial area under the ROC curve</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1310.0900">arXiv</a><a href="http://hdl.handle.net/2440/83158">pdf</a><a href="data/bibtex/ICCV13Pai.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+pedestrian+detection+by+directly+optimizing+the+partial+area+under+the+{ROC}+curve+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+pedestrian+detection+by+directly+optimizing+the+partial+area+under+the+{ROC}+curve">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV2013Harandi_arXiv.jpg"><b>Dictionary learning and sparse coding on Grassmann manifolds: an extrinsic solution</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>M. Harandi, C. Sanderson, C. Shen, B. Lovell</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1310.4891">arXiv</a><a href="data/bibtex/ICCV2013Harandi.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Dictionary+Learning+and+Sparse+Coding+on+{G}rassmann+Manifolds:+An+Extrinsic+Solution+{Harandi},+Mehrtash+and+{Sanderson},+Conrad+and+Shen,+Chunhua+and+Lovell,+Brian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Dictionary+Learning+and+Sparse+Coding+on+{G}rassmann+Manifolds:+An+Extrinsic+Solution">semantic scholar</a><a href="https://github.com/chhshen/Grassmann/">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICML13a_PDF.jpg"><b>Learning hash functions using column generation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Li, G. Lin, C. Shen, A. van den Hengel, A. Dick</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. International Conference on Machine Learning (ICML&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1303.0339">arXiv</a><a href="http://jmlr.csail.mit.edu/proceedings/papers/v28/li13a.pdf">pdf</a><a href="data/bibtex/ICML13a.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Hash+Functions+Using+Column+Generation+Li,+Xi+and+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Dick,+Anthony">google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Hash+Functions+Using+Column+Generation">semantic scholar</a><a href="https://bitbucket.org/guosheng/column-generation-hashing/">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1204.2912.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR12a_arXiv.jpg"></a><b>Non-sparse linear representations for visual tracking with online reservoir metric learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Li, C. Shen, Q. Shi, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;12), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1204.2912">arXiv</a><a href="http://hdl.handle.net/2440/70244">pdf</a><a href="data/bibtex/CVPR12a.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Non-sparse+Linear+Representations+for+Visual+Tracking+with+Online+Reservoir+Metric+Learning+Li,+Xi+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Non-sparse+Linear+Representations+for+Visual+Tracking+with+Online+Reservoir+Metric+Learning">semantic scholar</a></p>
</li>
<li><p><b>Sharing features in multi-class boosting via group sparsity</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;12), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/69851">pdf</a><a href="data/bibtex/CVPR12b.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Sharing+Features+in+Multi-class+Boosting+via+Group+Sparsity+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Sharing+Features+in+Multi-class+Boosting+via+Group+Sparsity">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ECCV12_PDF.jpg"><b>Robust tracking with weighted online structured learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>R. Yao, Q. Shi, C. Shen, Y. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;12), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="https://sites.google.com/site/chhshen/publication/weighted_tracking_eccv12.pdf?attredirects=1">pdf</a><a href="data/bibtex/ECCV12.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Robust+Tracking+with+Weighted+Online+Structured+Learning+Yao,+Rui+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Robust+Tracking+with+Weighted+Online+Structured+Learning">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICML12_arXiv.jpg"><b>Is margin preserved after random projection?</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Q. Shi, C. Shen, R. Hill, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. International Conference on Machine Learning (ICML&rsquo;12), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1206.4651">arXiv</a><a href="http://hdl.handle.net/2440/71063">link</a><a href="data/bibtex/ICML12.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Is+margin+preserved+after+random+projection?+Shi,+Qinfeng+and+Shen,+Chunhua+and+Hill,+Rhys+and+van+den+Hengel,+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Is+margin+preserved+after+random+projection?">semantic scholar</a></p>
<ol reversed>
<li><p>This work provides an analysis of margin distortion under random projections, the conditions under which margins are preserved, and presents bounds on the margin distortion.</p>
</li></ol>
</li>
<li><p><b>Real-time visual tracking using compressive sensing</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>H. Li, C. Shen, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://goo.gl/dsjsoM">pdf</a><a href="data/bibtex/Li2011CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-time+visual+tracking+Using+compressive+sensing+Li,+Hanxi+and+Shen,+Chunhua+and+Shi,+Qinfeng">google scholar</a><a href="https://www.semanticscholar.org/search?q=Real-time+visual+tracking+Using+compressive+sensing">semantic scholar</a></p>
</li>
<li><p><b>A generalized probabilistic framework for compact codebook creation</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Liu, L. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/63014">pdf</a><a href="data/bibtex/Liu2011CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+generalized+probabilistic+framework+for+compact+codebook+creation+Liu,+Lingqiao+and+Wang,+Lei+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=A+generalized+probabilistic+framework+for+compact+codebook+creation">semantic scholar</a></p>
</li>
<li><p><b>A direct formulation for totally-corrective multi-class boosting</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Shen, Z. Hao</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/62919">pdf</a><a href="data/bibtex/Shen2011CVPRa.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+direct+formulation+for+totally-corrective+multi-class+boosting+Shen,+Chunhua+and+Hao,+Zhihui">google scholar</a><a href="https://www.semanticscholar.org/search?q=A+direct+formulation+for+totally-corrective+multi-class+boosting">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2011CVPRb_PDF.jpg"><b>A scalable dual approach to semidefinite metric learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Shen, J. Kim, L. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://goo.gl/UyVdEc">pdf</a><a href="data/bibtex/Shen2011CVPRb.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Scalable+Dual+Approach+to+Semidefinite+Metric+Learning+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei">google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Scalable+Dual+Approach+to+Semidefinite+Metric+Learning">semantic scholar</a></p>
</li>
<li><p><b>Is face recognition really a compressive sensing problem?</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Q. Shi, A. Eriksson, A. van den Hengel, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/67036">pdf</a><a href="data/bibtex/Shi2011CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Is+face+recognition+really+a+Compressive+Sensing+problem?+Shi,+Qinfeng+and+Eriksson,+Anders+and+van+den+Hengel,+Anton+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Is+face+recognition+really+a+Compressive+Sensing+problem?">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV2011_PDF.jpg"><b>Graph mode-based contextual kernels for robust SVM tracking</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>X. Li, A. Dick, H. Wang, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://goo.gl/GzpBVb">pdf</a><a href="data/bibtex/ICCV2011.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Graph+mode-based+contextual+kernels+for+robust+{SVM}+tracking+Li,+Xi+and+Dick,+Anthony+and+Wang,+Hanzi+and+Shen,+Chunhua+and+van+den+Hengel,+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Graph+mode-based+contextual+kernels+for+robust+{SVM}+tracking">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shi2010CVPR_PDF.jpg"><b>Rapid face recognition using hashing</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Q. Shi, H. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;10), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://sites.google.com/site/chhshen/publication/cvpr10.pdf?attredirects=1">pdf</a><a href="data/bibtex/Shi2010CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Rapid+face+recognition+using+hashing+Shi,+Qinfeng+and+Li,+Hanxi+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Rapid+face+recognition+using+hashing">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2010ECCV_arXiv.jpg"><b>LACBoost and FisherBoost: optimally building cascade classifiers</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Shen, P. Wang, H. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;10), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1005.4103">arXiv</a><a href="http://dx.doi.org/10.1007/978-3-642-15552-9_44">link</a><a href="data/bibtex/Shen2010ECCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={LACBoost}+and+{FisherBoost}:+Optimally+Building+Cascade+Classifiers+Shen,+Chunhua+and+Wang,+Peng+and+Li,+Hanxi">google scholar</a><a href="https://www.semanticscholar.org/search?q={LACBoost}+and+{FisherBoost}:+Optimally+Building+Cascade+Classifiers">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Paisitkriangkrai2009CVPR_PDF.jpg"><b>Efficiently training a better visual detector with sparse eigenvectors</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;09), 2009</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0903.3103">arXiv</a><a href="http://sites.google.com/site/chhshen/publication/CVPR2009GSLDA.pdf?attredirects=1">link</a><a href="data/bibtex/Paisitkriangkrai2009CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficiently+Training+a+Better+Visual+Detector+with+Sparse+Eigenvectors+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficiently+Training+a+Better+Visual+Detector+with+Sparse+Eigenvectors">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2009PSD_PDF.jpg"><b>Positive semidefinite metric learning with boosting</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Shen, J. Kim, L. Wang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;09), 2009</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0910.2279">arXiv</a><a href="http://papers.NeurIPS.cc/paper/3658-positive-semidefinite-metric-learning-with-boosting.pdf">pdf</a><a href="data/bibtex/Shen2009PSD.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Positive+semidefinite+metric+learning+with+Boosting+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Positive+semidefinite+metric+learning+with+Boosting">semantic scholar</a><a href="https://bitbucket.org/chhshen/data/raw/45d101372013763d18f0a7ed191c86569532ed96/code/BoostMetric-NIPS09-codes-V0.1.tar.bz2">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Fast2008Wang_PDF.jpg"><b>A fast algorithm for creating a compact and discriminative visual codebook</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>L. Wang, L. Zhou, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. European Conference on Computer Vision (ECCV&rsquo;08), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1007/978-3-540-88693-8_53">link</a><a href="http://sites.google.com/site/chhshen/publication/ECCV2008Wang.pdf?attredirects=1">pdf</a><a href="data/bibtex/Fast2008Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Fast+Algorithm+for+Creating+a+Compact+and+Discriminative+Visual+Codebook+Wang,+Lei+and+Zhou,+Luping+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Fast+Algorithm+for+Creating+a+Compact+and+Discriminative+Visual+Codebook">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2008PSD_PDF.jpg"><b>PSDBoost: matrix-generation linear programming for positive semidefinite matrices learning</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Shen, A. Welsh, L. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;08), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://papers.NeurIPS.cc/paper/3611-psdboost-matrix-generation-linear-programming-for-positive-semidefinite-matrices-learning.pdf">pdf</a><a href="data/bibtex/Shen2008PSD.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={PSDB}oost:+Matrix-generation+linear+programming+for+positive+semidefinite+matrices+learning+Shen,+Chunhua+and+Welsh,+Alan+and+Wang,+Lei">google scholar</a><a href="https://www.semanticscholar.org/search?q={PSDB}oost:+Matrix-generation+linear+programming+for+positive+semidefinite+matrices+learning">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Kernel2007Quang_PDF.jpg"><b>Kernel-based tracking from a probabilistic viewpoint</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>Q. Nguyen, A. Robles-Kelly, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;07), 2007</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/CVPR.2007.383240">link</a><a href="http://goo.gl/1QNmaq">pdf</a><a href="data/bibtex/Kernel2007Quang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Kernel-based+tracking+from+a+probabilistic+viewpoint+Nguyen,+Quang+and+Robles-Kelly,+Antonio+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Kernel-based+tracking+from+a+probabilistic+viewpoint">semantic scholar</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2005Fast_PDF.jpg"><b>Fast global kernel density mode seeking with application to localisation and tracking</b>  
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <i>C. Shen, M. Brooks, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" />  <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;05), 2005</i>.
<br /><img class="eq" src="eqs/3225992146121917387-140.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/ICCV.2005.94">link</a><a href="http://goo.gl/UHzjWW">pdf</a><a href="data/bibtex/Shen2005Fast.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+global+kernel+density+mode+seeking+with+application+to+localisation+and+tracking+Shen,+Chunhua+and+Brooks,+Michael+J.+and+{van+den+Hengel},+Anton">google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+global+kernel+density+mode+seeking+with+application+to+localisation+and+tracking">semantic scholar</a></p>
<ol reversed>
<li><p>Oral presentation, 45 out of 1200 submissions.</p>
</li>
</ol>

</li>
</ol>
<div id="footer">
<div id="footer-text">
&copy; <b>Chunhua Shen</b>
&bull;
last update: 2021-09-12 11:04:56 ACST
&bull;
<a href="#" onClick="changewidth(1);return false" title="Expand page width"><b>&larr;&rarr;</b></a>
&bull;
<a href="#" onClick="changewidth(-1);return false" title="Reduce page width"><b>&rarr;&larr;</b></a>
<!-- Javascript -->
<script
    src="./script/jquery-1.6.2.min.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery.flot.min.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery-scroll.js"
    type="text/javascript">
</script>
<script
    src="./script/width_change.js"
    type="text/javascript">
</script>
<script
    src="./script/reverse_ol.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery.highlight.js"
    type="text/javascript">
</script>
<!-- Required for the jQuery.LocalScroll Plug-in -->
<script type="text/javascript">
    $(document).ready(function(){
    //
    $.localScroll();
    //
    // Round images
    //
	$(".rounded-img, .rounded-img2").load(function() {
	$(this).wrap(function(){
	return '<span class="' + $(this).attr('class')
                + '" style="background:url(' + $(this).attr('src')
                + ') no-repeat center center; width: '
                + $(this).width() + 'px; height: '
                + $(this).height() + 'px;" />';
		});
		$(this).css("opacity","0");
	});
      //
      //
      //  nav tab animation
        var navDuration = 150; //time in miliseconds
        $('#nav li a').hover(function() {
          $(this).animate({ paddingTop:"50px"  }, navDuration);
        }, function() {
             $(this).animate({ paddingTop:"31px"}, navDuration);
        });
        //
        // plot citation figure using jquery flot, 2012 July, CS
        //
        var flot_options = {
        legend: {
            show: false,
            margin: 10,
            backgroundOpacity: 0.5
                },
        bars:  {
            show: true,
            barWidth: 0.6,
            align: "center"
        },
        yaxis: {
            min: -20,
            tickFormatter: function(val, axis) {
                if (val < 50)
                    return " &nbsp; ";  // some string
                else
                    return val < axis.max ? val.toFixed(0) :   "  &nbsp;  ";
            }
        },
        grid: {
            borderWidth: 0
        }
    };  // end of flot_options
    $.getJSON("./data/cs_cite.json", function(json) {
       //succes - data loaded, now use plot:
       var plotarea = $("#citation_plot_holder");
       var data=[json.data];
       $.plot(plotarea , data, flot_options);
    });
//
// end of jquery flot
//
    changewidth( 0.9 );
//
//
//  highlight ``Shen''
    $("body p").highlight(['C. Shen', 'Chunhua Shen']);
//  highlight selected publication venues
    $("body p").highlight(['CVPR', 'ICCV', 'ECCV', 'ICML', 'NeurIPS',
    'TPAMI', 'IJCV', 'JMLR'],  { element: 'span', className: 'selected_venue' } );
//
    });
</script>
<!-- News ticker -->
<script type="text/javascript">
    function tick(){
        $('#ticker li:first').slideUp( function () { $(this).appendTo($('#ticker')).slideDown(); });
    }
    setInterval(function(){ tick () }, 5000);
</script>
</div>
</div>
</div>
</body>
</html>
