<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<!--
-->
<link rel="stylesheet" href="css/cs.css"                 type="text/css" />
<link rel="stylesheet" href="css/content.css"            type="text/css" />
<!-- font family -->
<link href="https://fonts.cdnfonts.com/css/zilla-slab" rel="stylesheet">
<link href="//netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet" />
<link href="https://fonts.cdnfonts.com/css/comfortaa" rel="stylesheet">
<link href="https://fonts.cdnfonts.com/css/eb-garamond-2?styles=20043,20040" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<link rel="stylesheet" href="css/full_publication.css" type="text/css" />
<title>Chunhua Shen</title>
</head>
<body>
<div id="layout-content">
<div id="menu">
    <div id="menucontainer">
<ul id="nav">
   <li><a href="index.html" target="_self">Home</a></li>
   <li><a href="paper.html" target="_self">Publications</a></li>
   <li><a href="teaching.html" target="_self">Teaching</a></li>
</ul>
</div>
</div>
<div id="toptitle">
<h1>Publications (Full List)</h1>
<div id="subtitle">Categorised <a href="fullpaper2.html">by venue <i class='fa fa-location-arrow' aria-hidden='true'></i></a>,  <a href="fullpaper.html">by year <i class='fa fa-clock-o' aria-hidden='true'></i></a>. <b>348</b>  papers.</div>
</div>
<p><a href="http://scholar.google.com/citations?hl=en&amp;user=Ljk2BvIAAAAJ&amp;view_op=list_works&amp;pagesize=100">Google scholar (28924 citations)  <i class='ai ai-google-scholar'   aria-hidden='true'></i></a>,
<a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Chunhua">DBLP <i class='ai ai-dblp ai-1x'></i></a>,
<a href="https://tinyurl.com/ww4dlqm">arXiv <i class='ai ai-biorxiv ai-1x'></i></a>.</p>
<p><div id="citation_plot_holder"></div></p>
<h2>Journal</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1912.09629.pdf"><img class="imgP  right"   src="data/thumbnail/IJCV2021Liuyl_arXiv.jpg"></a><b>Exploring the capacity of an orderless box discretization network for multi-orientation scene text detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Liu, T. He, H. Chen, X. Wang, C. Luo, S. Zhang, C. Shen, L. Jin</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1912.09629">arXiv</a><a href="data/bibtex/IJCV2021Liuyl.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploring+the+Capacity+of+an+Orderless+Box+Discretization+Network+for+Multi-orientation+Scene+Text+Detection+Liu,+Yuliang+and+He,+Tong+and+Chen,+Hao+and+Wang,+Xinyu+and+Luo,+Canjie+and+Zhang,+Shuaitao+and+Shen,+Chunhua+and+Jin,+Lianwen">search</a><a href="https://git.io/TextDet">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2105.11610.pdf"><img class="imgP  right"   src="data/thumbnail/Bian2021IJCV_arXiv.jpg"></a><b>Unsupervised scale-consistent depth learning from video</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>J. Bian, H. Zhan, N. Wang, Z. Li, L. Zhang, C. Shen, M. Cheng, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2105.11610">arXiv</a><a href="data/bibtex/Bian2021IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Scale-consistent+Depth+Learning+from+Video+Bian,+Jia-Wang+and+Zhan,+Huangying+and+Wang,+Naiyan+and+Li,+Zhichao+and+Zhang,+Le+and+Shen,+Chunhua+and+Cheng,+Ming-Ming+and+Reid,+Ian">search</a><a href="https://github.com/JiawangBian/SC-SfMLearner-Release">project webpage</a></p>
</li>
<li><p><b>Learning deep part-aware embedding for person retrieval</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Zhao, C. Shen, X. Yu, H. Chen, Y. Gao, S. Xiong</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Zhao2021PR1.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Deep+Part-Aware+Embedding+for+Person+Retrieval+Zhao,+Yang+and+Shen,+Chunhua+and+Yu,+Xiaohan+and+Chen,+Hao+and+Gao,+Yongsheng+and+Xiong,+Shengwu">search</a></p>
</li>
<li><p><b>An adversarial human pose estimation network injected with graph structure</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Tian, P. Wang, G. Liang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Tian2021Adversarial.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+Adversarial+Human+Pose+Estimation+Network+Injected+with+Graph+Structure+Tian,+Lei+and+Wang,+Peng+and+Liang,+Guoqiang+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1908.04680.pdf"><img class="imgP  right"   src="data/thumbnail/Zhuang2021Quantization_arXiv.jpg"></a><b>Effective training of convolutional neural networks with low-bitwidth weights and activations</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>B. Zhuang, J. Liu, M. Tan, L. Liu, I. Reid, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.04680">arXiv</a><a href="data/bibtex/Zhuang2021Quantization.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Effective+Training+of+Convolutional+Neural+Networks+with+Low-bitwidth+Weights+and+Activations+Zhuang,+Bohan+and+Liu,+Jing+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2103.04216.pdf"><img class="imgP  right"   src="data/thumbnail/Yin2021PAMIvn_arXiv.jpg"></a><b>Virtual normal: enforcing geometric constraints for accurate and robust depth prediction</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>W. Yin, Y. Liu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2103.04216">arXiv</a><a href="data/bibtex/Yin2021PAMIvn.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Virtual+Normal:+Enforcing+Geometric+Constraints+for+Accurate+and+Robust+Depth+Prediction+Yin,+Wei+and+Liu,+Yifan+and+Shen,+Chunhua">search</a><a href="https://git.io/Depth">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2105.00405.pdf"><img class="imgP  right"   src="data/thumbnail/Wang2021PANplus_arXiv.jpg"></a><b>PAN++: towards efficient and accurate end-to-end spotting of arbitrarily-shaped text</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>W. Wang, E. Xie, X. Li, X. Liu, D. Liang, Z. Yang, T. Lu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2105.00405">arXiv</a><a href="data/bibtex/Wang2021PANplus.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={PAN++}:+Towards+Efficient+and+Accurate+End-to-End+Spotting+of+Arbitrarily-Shaped+Text+Wang,+Wenhai+and+Xie,+Enze+and+Li,+Xiang+and+Liu,+Xuebo+and+Liang,+Ding+and+Yang,+Zhibo+and+Lu,+Tong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1906.06013.pdf"><img class="imgP  right"   src="data/thumbnail/Li2021Text_arXiv.jpg"></a><b>Towards end-to-end text spotting in natural scenes</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>P. Wang, H. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1906.06013">arXiv</a><a href="data/bibtex/Li2021Text.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+End-to-End+Text+Spotting+in+Natural+Scenes+Wang,+Peng+and+Li,+Hui+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2007.02500.pdf"><img class="imgP  right"   src="data/thumbnail/Pan2020ACMSurvey_arXiv.jpg"></a><b>Deep learning for anomaly detection: a review</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>G. Pang, C. Shen, L. Cao, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>ACM Computing Surveys (ACMSurvey), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2007.02500">arXiv</a><a href="data/bibtex/Pan2020ACMSurvey.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Learning+for+Anomaly+Detection:+A+Review+Pang,+Guansong+and+Shen,+Chunhua+and+Cao,+Longbing+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Towards light-weight portrait matting via parameter sharing</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Dai, H. Lu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Computer Graphics Forum (CGF), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Daiyt2020.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Light-Weight+Portrait+Matting+via+Parameter+Sharing+Dai,+Yutong+and+Lu,+Hao+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2001.04189.pdf"><img class="imgP  right"   src="data/thumbnail/Luo2020IJCV_arXiv.jpg"></a><b>Separating content from style using adversarial learning for recognizing text in the wild</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Luo, Q. Lin, Y. Liu, L. Jin, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2001.04189">arXiv</a><a href="data/bibtex/Luo2020IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Separating+Content+from+Style+Using+Adversarial+Learning+for+Recognizing+Text+in+the+Wild+Luo,+Canjie+and+Lin,+Qingxiang+and+Liu,+Yuliang+and+Jin,+Lianwen+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>TasselNetv2: in-field counting of wheat spikes with context-augmented local regression networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>H. Xiong, Z. Cao, H. Lu, S. Madec, L. Liu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Plant Methods (PLME), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/TasselNet2020.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={TasselNetv2}:+in-field+counting+of+wheat+spikes+with+context-augmented+local+regression+networks+Xiong,+Haipeng+and+Cao,+Zhiguo+and+Lu,+Hao+and+Madec,+Simon+and+Liu,+Liang+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1908.03839.pdf"><img class="imgP  right"   src="data/thumbnail/MobileFAN2020_arXiv.jpg"></a><b>MobileFAN: transferring deep hidden representation for face alignment</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Zhao, Y. Liu, C. Shen, Y. Gao, S. Xiong</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.03839">arXiv</a><a href="data/bibtex/MobileFAN2020.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={MobileFAN}:+Transferring+Deep+Hidden+Representation+for+Face+Alignment+Zhao,+Yang+and+Liu,+Yifan+and+Shen,+Chunhua+and+Gao,+Yongsheng+and+Xiong,+Shengwu">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1909.06023.pdf"><img class="imgP  right"   src="data/thumbnail/Zhangx2020T-ITS_arXiv.jpg"></a><b>Part-guided attention learning for vehicle instance retrieval</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Zhang, R. Zhang, J. Cao, D. Gong, M. You, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1909.06023">arXiv</a><a href="data/bibtex/Zhangx2020T-ITS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Part-Guided+Attention+Learning+for+Vehicle+Instance+Retrieval+Zhang,+Xinyu+and+Zhang,+Rufeng+and+Cao,+Jiewei+and+Gong,+Dong+and+You,+Mingyu+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>A robust attentional framework for license plate recognition in the wild</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Zhang, P. Wang, H. Li, Z. Li, C. Shen, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Li2020Carlicense.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+robust+attentional+framework+for+license+plate+recognition+in+the+wild+Zhang,+Linjiang+and+Wang,+Peng+and+Li,+Hui+and+Li,+Zhen+and+Shen,+Chunhua+and+Zhang,+Yanning">search</a></p>
</li>
<li><p><b>Real-time high-performance semantic image segmentation of urban street scenes</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>G. Dong, Y. Yan, C. Shen, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Dong2020segmentation.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-time+high-performance+semantic+image+segmentation+of+urban+street+scenes+Dong,+Genshun+and+Yan,+Yan+and+Shen,+Chunhua+and+Wang,+Hanzi">search</a></p>
</li>
<li><p><b>Towards effective deep embedding for zero-shot learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Zhang, P. Wang, L. Liu, C. Shen, W. Wei, Y. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2020Zeroshot.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Effective+Deep+Embedding+for+Zero-Shot+Learning+Zhang,+Lei+and+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Wei,+Wei+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>NSSNet: scale-aware object counting with non-scale suppression</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Liu, Z. Cao, H. Lu, H. Xiong, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/LiuL2020CSVT.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={NSSNet}:+Scale-aware+object+counting+with+non-scale+suppression+Liu,+Liang+and+Cao,+Zhiguo+and+Lu,+Hao+and+Xiong,+Haipeng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.12338.pdf"><img class="imgP  right"   src="data/thumbnail/Zhang2020Covid_arXiv.jpg"></a><b>Viral pneumonia screening on chest x-ray images using confidence-aware anomaly detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>J. Zhang, Y. Xie, Z. Liao, G. Pang, J. Verjans, W. Li, Z. Sun, J. He, Y. Li, C. Shen, Y. Xia</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Medical Imaging (TMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.12338">arXiv</a><a href="data/bibtex/Zhang2020Covid.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Viral+Pneumonia+Screening+on+Chest+X-ray+Images+Using+Confidence-Aware+Anomaly+Detection+Zhang,+Jianpeng+and+Xie,+Yutong+and+Liao,+Zhibin+and+Pang,+Guansong+and+Verjans,+Johan+and+Li,+Wenxin+and+Sun,+Zongji+and+He,+Jian+and+Li,+Yi+and+Shen,+Chunhua+and+Xia,+Yong">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.03313.pdf"><img class="imgP  right"   src="data/thumbnail/Xie2020TMIa_arXiv.jpg"></a><b>A mutual bootstrapping model for automated skin lesion segmentation and classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Xie, J. Zhang, Y. Xia, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Medical Imaging (TMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.03313">arXiv</a><a href="data/bibtex/Xie2020TMIa.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Mutual+Bootstrapping+Model+for+Automated+Skin+Lesion+Segmentation+and+Classification+Xie,+Yutong+and+Zhang,+Jianpeng+and+Xia,+Yong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>SESV: accurate medical image segmentation by predicting and correcting errors</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Xie, J. Zhang, H. Lu, C. Shen, Y. Xia</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Medical Imaging (TMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Xie2020TMIb.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SESV}:+Accurate+Medical+Image+Segmentation+by+Predicting+and+Correcting+Errors+Xie,+Yutong+and+Zhang,+Jianpeng+and+Lu,+Hao+and+Shen,+Chunhua+and+Xia,+Yong">search</a></p>
</li>
<li><p><b>OPMP: an omni-directional pyramid mask proposal network for arbitrary-shape scene text detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>S. Zhang, Y. Liu, L. Jin, Z. Wei, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/ShengZhang2020TMM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={OPMP}:+An+Omni-directional+Pyramid+Mask+Proposal+Network+for+Arbitrary-shape+Scene+Text+Detection+Zhang,+Sheng+and+Liu,+Yuliang+and+Jin,+Lianwen+and+Wei,+Zhongrong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Joint deep learning of facial expression synthesis and recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Yan, Y. Huang, S. Chen, C. Shen, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Yan2020TMM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Joint+deep+learning+of+facial+expression+synthesis+and+recognition+Yan,+Yan+and+Huang,+Ying+and+Chen,+Si+and+Shen,+Chunhua+and+Wang,+Hanzi">search</a></p>
</li>
<li><p><b>Accurate tensor completion via adaptive low-rank representation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Q. Shi, C. Shen, A. van den Hengel, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2020TNNLS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Accurate+Tensor+Completion+via+Adaptive+Low-Rank+Representation+Zhang,+Lei+and+Wei,+Wei+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Zhang,+Yanning">search</a></p>
</li>
<li><p><b>Deep clustering with sample-assignment invariance prior</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Peng, H. Zhu, J. Feng, C. Shen, H. Zhang, J. Zhou</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Peng2020TNNLS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Clustering+with+Sample-Assignment+Invariance+Prior+Peng,+Xi+and+Zhu,+Hongyuan+and+Feng,+Jiashi+and+Shen,+Chunhua+and+Zhang,+Haixian+and+Zhou,+Joey">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1804.03368.pdf"><img class="imgP  right"   src="data/thumbnail/Gong2020TNNLS_arXiv.jpg"></a><b>Learning deep gradient descent optimization for image deconvolution</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>D. Gong, Z. Zhang, Q. Shi, A. van den Hengel, C. Shen, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1804.03368">arXiv</a><a href="data/bibtex/Gong2020TNNLS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Deep+Gradient+Descent+Optimization+for+Image+Deconvolution+Gong,+Dong+and+Zhang,+Zhen+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Zhang,+Yanning">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.07504.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2020TOG_arXiv.jpg"></a><b>Real-time image smoothing via iterative least squares</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>W. Liu, P. Zhang, X. Huang, J. Yang, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>ACM Transactions on Graphics (TOG), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.07504">arXiv</a><a href="https://doi.org/10.1145/3388887">link</a><a href="data/bibtex/Liu2020TOG.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-time+image+smoothing+via+iterative+least+squares+Liu,+Wei+and+Zhang,+Pingping+and+Huang,+Xiaolin+and+Yang,+Jie+and+Shen,+Chunhua+and+Reid,+Ian">search</a><a href="https://github.com/wliusjtu/Real-time-Image-Smoothing-via-Iterative-Least-Squares">project webpage</a></p>
</li>
<li><p><b>Plenty is plague: fine-grained learning for visual question answering</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Zhou, R. Ji, J. Su, X. Sun, D. Meng, Y. Gao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="https://doi.org/10.1109/TPAMI.2019.2956699">link</a><a href="data/bibtex/Zhou2020TPAMIZhou.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Plenty+Is+Plague:+Fine-Grained+Learning+for+Visual+Question+Answering+Zhou,+Yiyi+and+Ji,+Rongrong+and+Su,+Jinsong+and+Sun,+Xiaoshuai+and+Meng,+Deyu+and+Gao,+Yue+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1912.11236.pdf"><img class="imgP  right"   src="data/thumbnail/Zhang2020OrderlessReID_arXiv.jpg"></a><b>Ordered or orderless: a revisit for video based person re-identification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Zhang, Z. Shi, J. Zhou, M. Cheng, Y. Liu, J. Bian, Z. Zeng, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1912.11236">arXiv</a><a href="https://doi.org/10.1109/TPAMI.2020.2976969">link</a><a href="data/bibtex/Zhang2020OrderlessReID.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Ordered+or+Orderless:+A+Revisit+for+Video+based+Person+Re-Identification+Zhang,+Le+and+Shi,+Zenglin+and+Zhou,+Joey+Tianyi+and+Cheng,+Ming-Ming+and+Liu,+Yun+and+Bian,+Jia-Wang+and+Zeng,+Zeng+and+Shen,+Chunhua">search</a><a href="https://github.com/ZhangLeUestc/VideoReid-TPAMI2020">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1908.09895.pdf"><img class="imgP  right"   src="data/thumbnail/Lu2020PAMIIndexNet_arXiv.jpg"></a><b>Index networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>H. Lu, Y. Dai, C. Shen, S. Xu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.09895">arXiv</a><a href="https://doi.org/10.1109/TPAMI.2020.3004474">link</a><a href="data/bibtex/Lu2020PAMIIndexNet.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Index+Networks+Lu,+Hao+and+Dai,+Yutong+and+Shen,+Chunhua+and+Xu,+Songcen">search</a><a href="https://git.io/IndexNet">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.04197.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2020PAMI_arXiv.jpg"></a><b>Structured knowledge distillation for dense prediction</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Liu, C. Shun, J. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.04197">arXiv</a><a href="https://ieeexplore.ieee.org/document/9115859">link</a><a href="data/bibtex/Liu2020PAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Knowledge+Distillation+for+Dense+Prediction+Liu,+Yifan+and+Shun,+Changyong+and+Wang,+Jingdong+and+Shen,+Chunhua">search</a><a href="https://github.com/irfanICMLL/structure_knowledge_distillation">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Chen2019PAMI_arXiv.jpg"><b>Adversarial learning of structure-aware fully convolutional networks for landmark localization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Chen, C. Shen, H. Chen, X. Wei, L. Liu, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.00253">arXiv</a><a href="https://doi.org/10.1109/TPAMI.2019.2901875">link</a><a href="data/bibtex/Chen2019PAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adversarial+Learning+of+Structure-Aware+Fully+Convolutional+Networks+for+Landmark+Localization+Chen,+Yu+and+Shen,+Chunhua+and+Chen,+Hao+and+Wei,+Xiu-Shen+and+Liu,+Lingqiao+and+Yang,+Jian">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Cao2020GAN_arXiv.jpg"><b>Improving generative adversarial networks with local coordinate coding</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>J. Cao, Y. Guo, Q. Wu, C. Shen, J. Huang, M. Tan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2008.00942">arXiv</a><a href="data/bibtex/Cao2020GAN.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Improving+Generative+Adversarial+Networks+with+Local+Coordinate+Coding+Cao,+Jiezhang+and+Guo,+Yong+and+Wu,+Qingyao+and+Shen,+Chunhua+and+Huang,+Junzhou+and+Tan,+Mingkui">search</a><a href="https://github.com/SCUTjinchengli/LCCGAN-v2">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1806.01576.pdf"><img class="imgP  right"   src="data/thumbnail/Adaptive2019Zhang_arXiv.jpg"></a><b>Adaptive importance learning for improving lightweight image super-resolution network</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Zhang, P. Wang, C. Shen, L. Liu, W. Wei, Y. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1806.01576">arXiv</a><a href="data/bibtex/Adaptive2019Zhang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adaptive+Importance+Learning+for+Improving+Lightweight+Image+Super-resolution+Network+Zhang,+Lei+and+Wang,+Peng+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Wei,+Wei+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton">search</a><a href="https://tinyurl.com/Super-resolution-Network">project webpage</a></p>
</li>
<li><p><b>Accurate imagery recovery using a multi-observation patch model</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Q. Shen, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Information Sciences (IS), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2019Accurate.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Accurate+Imagery+Recovery+Using+a+Multi-Observation+Patch+Model+Zhang,+Lei+and+Wei,+Wei+and+Shen,+Qiang+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Heritage image annotation via collective knowledge</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>J. Zhang, Q. Wu, J. Zhang, C. Shen, J. Lu, Q. Wu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2019PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Heritage+Image+Annotation+via+Collective+Knowledge+Zhang,+Junjie+and+Wu,+Qi+and+Zhang,+Jian+and+Shen,+Chunhua+and+Lu,+Jianfeng+and+Wu,+Qiang">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1611.10080.pdf"><img class="imgP  right"   src="data/thumbnail/Wu2019PR_arXiv.jpg"></a><b>Wider or deeper: revisiting the ResNet model for visual recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Z. Wu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1611.10080">arXiv</a><a href="data/bibtex/Wu2019PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Wider+or+Deeper:+Revisiting+the+{ResNet}+Model+for+Visual+Recognition+Wu,+Zifeng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Order-aware convolutional pooling for video based action recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>P. Wang, L. Liu, C. Shen, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Wang2019PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Order-aware+Convolutional+Pooling+for+Video+Based+Action+Recognition+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Shen,+Heng+Tao">search</a></p>
</li>
<li><p><b>Structural analysis of attributes for vehicle re-identification and retrieval</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Zhao, C. Shen, H. Wang, S. Chen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Zhao2019Structural.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structural+Analysis+of+Attributes+for+Vehicle+Re-identification+and+Retrieval+Zhao,+Yanzhu+and+Shen,+Chunhua+and+Wang,+Huibing+and+Chen,+Shengyong">search</a></p>
</li>
<li><p><b>Human detection aided by deeply learned semantic masks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Wang, C. Shen, H. Li, S. Xu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Wangxy2019CSVT.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Human+Detection+Aided+by+Deeply+Learned+Semantic+Masks+Wang,+Xinyu+and+Shen,+Chunhua+and+Li,+Hanxi+and+Xu,+Shugong">search</a></p>
</li>
<li><p><b>Embedding bilateral filter in least squares for efficient edge-preserving image smoothing</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>W. Liu, P. Zhang, X. Chen, C. Shen, X. Huang, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Liu2019CSVT.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Embedding+Bilateral+Filter+in+Least+Squares+for+Efficient+Edge-preserving+Image+Smoothing+Liu,+Wei+and+Zhang,+Pingping+and+Chen,+Xiaogang+and+Shen,+Chunhua+and+Huang,+Xiaolin+and+Yang,+Jie">search</a></p>
</li>
<li><p><b>Counting objects by blockwise classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Liu, H. Lu, H. Xiong, K. Xian, Z. Cao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Counting2019CSVT.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Counting+Objects+by+Blockwise+Classification+Liu,+Liang+and+Lu,+Hao+and+Xiong,+Haipeng+and+Xian,+Ke+and+Cao,+Zhiguo+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Hyperspectral classification based on lightweight 3D-CNN with transfer learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>H. Zhang, Y. Li, Y. Jiang, P. Wang, Q. Shen, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2019Lightweight.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Hyperspectral+Classification+Based+on+Lightweight+{3D-CNN}+With+Transfer+Learning+Zhang,+Haokui+and+Li,+Ying+and+Jiang,+Yenan+and+Wang,+Peng+and+Shen,+Qiang+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Salient object detection with lossless feature reflection and weighted structural loss</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>P. Zhang, W. Liu, H. Lu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2019Salient.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Salient+Object+Detection+with+Lossless+Feature+Reflection+and+Weighted+Structural+Loss+Zhang,+Pingping+and+Liu,+Wei+and+Lu,+Huchuan+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1805.04288.pdf"><img class="imgP  right"   src="data/thumbnail/Wei2019TIP_arXiv.jpg"></a><b>Piecewise classifier mappings: learning fine-grained learners for novel categories with few examples</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Wei, P. Wang, L. Liu, C. Shen, J. Wu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1805.04288">arXiv</a><a href="data/bibtex/Wei2019TIP.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Piecewise+classifier+mappings:+Learning+fine-grained+learners+for+novel+categories+with+few+examples+Wei,+Xiu-Shen+and+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Wu,+Jianxin">search</a></p>
</li>
<li><p><b>Multiple instance learning with emerging novel class</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Wei, H. Ye, X. Mu, J. Wu, C. Shen, Z. Zhou</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Knowledge and Data Engineering (TKDE), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Wei2019TKDE.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multiple+Instance+Learning+with+Emerging+Novel+Class+Wei,+Xiu-Shen+and+Ye,+Han-Jia+and+Mu,+Xin+and+Wu,+Jianxin+and+Shen,+Chunhua+and+Zhou,+Zhi-Hua">search</a></p>
</li>
<li><p><b>Attention residual learning for skin lesion classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>J. Zhang, Y. Xie, Y. Xia, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Medical Imaging (TMI), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2019Attn.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Attention+residual+learning+for+skin+lesion+classification+Zhang,+Jianpeng+and+Xie,+Yutong+and+Xia,+Yong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1803.02563.pdf"><img class="imgP  right"   src="data/thumbnail/TZhang2019TMM_arXiv.jpg"></a><b>Decoupled spatial neural attention for weakly supervised semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>T. Zhang, G. Lin, J. Cai, T. Shen, C. Shen, A. Kot</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1803.02563">arXiv</a><a href="data/bibtex/TZhang2019TMM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Decoupled+Spatial+Neural+Attention+for+Weakly+Supervised+Semantic+Segmentation+Zhang,+Tianyi+and+Lin,+Guosheng+and+Cai,+Jianfei+and+Shen,+Tong+and+Shen,+Chunhua+and+Kot,+Alex+C.">search</a></p>
</li>
<li><p><b>RefineNet: multi-path refinement networks for dense prediction</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>G. Lin, F. Liu, A. Milan, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="https://doi.org/10.1109/TPAMI.2019.2893630">link</a><a href="data/bibtex/Fayao2019PAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={RefineNet}:+Multi-Path+Refinement+Networks+for+Dense+Prediction+Lin,+Guosheng+and+Liu,+Fayao+and+Milan,+Anton+and+Shen,+Chunhua+and+Reid,+Ian">search</a><a href="https://github.com/guosheng/refinenet">project webpage</a></p>
<ol reversed>
<li><p>Pytorch code is <a href="https://github.com/DrSleep/refinenet-pytorch">here</a>.</p>
</li></ol>
</li>
<li><p><b>Cluster sparsity field: an internal hyperspectral imagery prior for reconstruction</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Y. Zhang, C. Shen, A. van den Hengel, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="https://www.researchgate.net/publication/323914969_Cluster_Sparsity_Field_An_Internal_Hyperspectral_Imagery_Prior_for_Reconstruction">pdf</a><a href="data/bibtex/Zhang2018IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Cluster+Sparsity+Field:+An+Internal+Hyperspectral+Imagery+Prior+for+Reconstruction+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1601.05610.pdf"><img class="imgP  right"   src="data/thumbnail/Li2018IVC_arXiv.jpg"></a><b>Reading car license plates using deep neural networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>H. Li, P. Wang, M. You, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Image and Vision Computing (IVC), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1601.05610">arXiv</a><a href="data/bibtex/Li2018IVC.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Reading+Car+License+Plates+Using+Deep+Neural+Networks+Li,+Hui+and+Wang,+Peng+and+You,+Mingyu+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1805.01282.pdf"><img class="imgP  right"   src="data/thumbnail/Zhuang2018PR_arXiv.jpg"></a><b>Multi-label learning based deep transfer neural network for facial attribute classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>N. Zhuang, Y. Yan, S. Chen, H. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1805.01282">arXiv</a><a href="data/bibtex/Zhuang2018PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-label+Learning+Based+Deep+Transfer+Neural+Network+for+Facial+Attribute+Classification+Zhuang,+Ni+and+Yan,+Yan+and+Chen,+Si+and+Wang,+Hanzi+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1707.06397.pdf"><img class="imgP  right"   src="data/thumbnail/Wei2018PR_arXiv.jpg"></a><b>Unsupervised object discovery and co-localization by deep descriptor transforming</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Wei, C. Zhang, J. Wu, C. Shen, Z. Zhou</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1707.06397">arXiv</a><a href="data/bibtex/Wei2018PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Object+Discovery+and+Co-Localization+by+Deep+Descriptor+Transforming+Wei,+Xiu-Shen+and+Zhang,+Chen-Lin+and+Wu,+Jianxin+and+Shen,+Chunhua+and+Zhou,+Zhi-Hua">search</a></p>
</li>
<li><p><b>An extended filtered channel framework for pedestrian detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>M. You, Y. Zhang, C. Shen, X. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/You2018T-ITS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+extended+filtered+channel+framework+for+pedestrian+detection+You,+Minyu+and+Zhang,+Yubin+and+Shen,+Chunhua+and+Zhang,+Xinyu">search</a></p>
</li>
<li><p><b>Towards end-to-end car license plates detection and recognition with deep neural networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>H. Li, P. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Li2018T-ITSa.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+End-to-End+Car+License+Plates+Detection+and+Recognition+with+Deep+Neural+Networks+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Unsupervised domain adaptation using robust class-wise matching</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Zhang, P. Wang, W. Wei, H. Lu, C. Shen, A. van den Hengel, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2018TCSVT.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Domain+Adaptation+Using+Robust+Class-Wise+Matching+Zhang,+Lei+and+Wang,+Peng+and+Wei,+Wei+and+Lu,+Hao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Zhang,+Yanning">search</a></p>
</li>
<li><p><b>Semantics-aware visual object tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>R. Yao, G. Lin, C. Shen, Y. Zhang, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Yao2018TCSVT.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Semantics-Aware+Visual+Object+Tracking+Yao,+Rui+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+Shi,+Qinfeng">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1603.04525.pdf"><img class="imgP  right"   src="data/thumbnail/TCSVT2017Hu_arXiv.jpg"></a><b>Pushing the limits of deep CNNs for pedestrian detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Q. Hu, P. Wang, C. Shen, A. van den Hengel, F. Porikli</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.04525">arXiv</a><a href="data/bibtex/TCSVT2017Hu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Pushing+the+Limits+of+Deep+{CNNs}+for+Pedestrian+Detection+Hu,+Qichang+and+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Porikli,+Fatih">search</a></p>
</li>
<li><p><b>An embarrassingly simple approach to visual domain adaptation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>H. Lu, C. Shen, Z. Cao, Y. Xiao, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Lu2018TIP.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+Embarrassingly+Simple+Approach+to+Visual+Domain+Adaptation+Lu,+Hao+and+Shen,+Chunhua+and+Cao,+Zhiguo+and+Xiao,+Yang+and+{van+den+Hengel},+Anton">search</a><a href="https://github.com/poppinace/ldada">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1612.01082.pdf"><img class="imgP  right"   src="data/thumbnail/Zhang2018TMM_arXiv.jpg"></a><b>Multi-label image classification with regional latent semantic dependencies</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>J. Zhang, Q. Wu, C. Shen, J. Zhang, J. Lu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1612.01082">arXiv</a><a href="data/bibtex/Zhang2018TMM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-Label+Image+Classification+with+Regional+Latent+Semantic+Dependencies+Zhang,+Junjie+and+Wu,+Qi+and+Shen,+Chunhua+and+Zhang,+Jian+and+Lu,+Jianfeng">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1712.09048.pdf"><img class="imgP  right"   src="data/thumbnail/Guo2018TMM_arXiv.jpg"></a><b>Automatic image cropping for visual aesthetic enhancement using deep neural networks and cascaded regression</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>G. Guo, H. Wang, C. Shen, Y. Yan, H. Liao</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1712.09048">arXiv</a><a href="data/bibtex/Guo2018TMM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Automatic+image+cropping+for+visual+aesthetic+enhancement+using+deep+neural+networks+and+cascaded+regression+Guo,+Guanjun+and+Wang,+Hanzi+and+Shen,+Chunhua+and+Yan,+Yan+and+Liao,+Hong-Yuan">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1606.05433.pdf"><img class="imgP  right"   src="data/thumbnail/Wang2017FVQA_arXiv.jpg"></a><b>FVQA: fact-based visual question answering</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>P. Wang, Q. Wu, C. Shen, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1606.05433">arXiv</a><a href="data/bibtex/Wang2017FVQA.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FVQA}:+Fact-based+Visual+Question+Answering+Wang,+Peng+and+Wu,+Qi+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Ordinal constraint binary coding for approximate nearest neighbor search</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>H. Liu, R. Ji, J. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="https://www.researchgate.net/publication/324053386_Ordinal_Constraint_Binary_Coding_for_Approximate_Nearest_Neighbor_Search">pdf</a><a href="data/bibtex/HLiu2018TPAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Ordinal+Constraint+Binary+Coding+for+Approximate+Nearest+Neighbor+Search+Liu,+Hong+and+Ji,+Rongrong+and+Wang,+Jingdong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Visual question answering: a survey of methods and datasets</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Q. Wu, D. Teney, P. Wang, C. Shen, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Computer Vision and Image Understanding (CVIU), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1607.05910">arXiv</a><a href="data/bibtex/CVIU2017VQA.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+question+answering:+A+survey+of+methods+and+datasets+Wu,+Qi+and+Teney,+Damien+and+Wang,+Peng+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1602.06654.pdf"><img class="imgP  right"   src="data/thumbnail/IJCV2017Lin_arXiv.jpg"></a><b>Structured learning of binary codes with column generation for optimizing ranking measures</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>G. Lin, F. Liu, C. Shen, J. Wu, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1602.06654">arXiv</a><a href="data/bibtex/IJCV2017Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Learning+of+Binary+Codes+with+Column+Generation+for+Optimizing+Ranking+Measures+Lin,+Guosheng+and+Liu,+Fayao+and+Shen,+Chunhua+and+Wu,+Jianxin+and+Shen,+Heng+Tao">search</a><a href="https://bitbucket.org/guosheng/structhash">project webpage</a></p>
</li>
<li><p><b>Removal of optically thick clouds from high-resolution satellite imagery using dictionary group learning and interdictionary nonlocal joint sparse coding</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Li, W. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (JSTAEORS), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Li2017Removal.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Removal+of+Optically+Thick+Clouds+From+High-resolution+Satellite+Imagery+Using+Dictionary+Group+Learning+and+Interdictionary+Nonlocal+Joint+Sparse+Coding+Li,+Ying+and+Li,+Wenbo+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1707.02290.pdf"><img class="imgP  right"   src="data/thumbnail/Lu2017Counting_arXiv.jpg"></a><b>TasselNet: counting maize tassels in the wild via local counts regression network</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>H. Lu, Z. Cao, Y. Xiao, B. Zhuang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Plant Methods (PLME), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1707.02290">arXiv</a><a href="data/bibtex/Lu2017Counting.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={TasselNet}:+Counting+maize+tassels+in+the+wild+via+local+counts+regression+network+Lu,+Hao+and+Cao,+Zhiguo+and+Xiao,+Yang+and+Zhuang,+Bohan+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1606.01595.pdf"><img class="imgP  right"   src="data/thumbnail/Wu2017PR_arXiv.jpg"></a><b>Deep linear discriminant analysis on Fisher networks: a hybrid architecture for person re-identification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Wu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1606.01595">arXiv</a><a href="data/bibtex/Wu2017PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Linear+Discriminant+Analysis+on+{F}isher+Networks:+A+Hybrid+Architecture+for+Person+Re-identification+Wu,+Lin+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Mask-CNN: localizing parts and selecting descriptors for bird species categorization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Wei, C. Xie, J. Wu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Wei2017PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mask-{CNN}:+Localizing+parts+and+selecting+descriptors+for+bird+species+categorization+Wei,+Xiu-Shen+and+Xie,+Chen-Wei+and+Wu,+Jianxin+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1504.04923.pdf"><img class="imgP  right"   src="data/thumbnail/PR2017Qiao_arXiv.jpg"></a><b>Learning discriminative trajectorylet detector sets for accurate skeleton-based action recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>R. Qiao, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1504.04923">arXiv</a><a href="data/bibtex/PR2017Qiao.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+discriminative+trajectorylet+detector+sets+for+accurate+skeleton-based+action+recognition+Qiao,+Ruizhi+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Deep CNNs with spatially weighted pooling for fine-grained car recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Q. Hu, H. Wang, T. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/SWP2017Hu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+{CNNs}+with+Spatially+Weighted+Pooling+for+Fine-grained+Car+Recognition+Hu,+Qichang+and+Wang,+Huibing+and+Li,+Teng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1604.08660.pdf"><img class="imgP  right"   src="data/thumbnail/TCSVT2017Sheng_arXiv.jpg"></a><b>Crowd counting via weighted VLAD on dense attribute feature maps</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>B. Sheng, C. Shen, G. Lin, J. Li, W. Yang, C. Sun</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1604.08660">arXiv</a><a href="data/bibtex/TCSVT2017Sheng.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Crowd+Counting+via+Weighted+{VLAD}+on+Dense+Attribute+Feature+Maps+Sheng,+Biyun+and+Shen,+Chunhua+and+Lin,+Guosheng+and+Li,+Jun+and+Yang,+Wankou+and+Sun,+Changyin">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1605.02305.pdf"><img class="imgP  right"   src="data/thumbnail/Cao2017_arXiv.jpg"></a><b>Estimating depth from monocular images as classification using deep fully convolutional residual networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Cao, Z. Wu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1605.02305">arXiv</a><a href="data/bibtex/Cao2017.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Estimating+Depth+from+Monocular+Images+as+Classification+Using+Deep+Fully+Convolutional+Residual+Networks+Cao,+Yuanzhouhan+and+Wu,+Zifeng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1601.07649.pdf"><img class="imgP  right"   src="data/thumbnail/TIP2017Liu_arXiv.jpg"></a><b>Discriminative training of deep fully-connected continuous CRF with task-specific loss</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>F. Liu, G. Lin, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1601.07649">arXiv</a><a href="data/bibtex/TIP2017Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Discriminative+Training+of+Deep+Fully-connected+Continuous+{CRF}+with+Task-specific+Loss+Liu,+Fayao+and+Lin,+Guosheng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1610.01706.pdf"><img class="imgP  right"   src="data/thumbnail/TIP2016Cao_arXiv.jpg"></a><b>Exploiting depth from single monocular images for object detection and semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Cao, C. Shen, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1610.01706">arXiv</a><a href="data/bibtex/TIP2016Cao.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploiting+Depth+from+Single+Monocular+Images+for+Object+Detection+and+Semantic+Segmentation+Cao,+Yuanzhouhan+and+Shen,+Chunhua+and+Shen,+Heng+Tao">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1703.08764.pdf"><img class="imgP  right"   src="data/thumbnail/TNNLS2017Liu_arXiv.jpg"></a><b>Structured learning of tree potentials in CRF for image segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>F. Liu, G. Lin, R. Qiao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1703.08764">arXiv</a><a href="data/bibtex/TNNLS2017Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Learning+of+Tree+Potentials+in+{CRF}+for+Image+Segmentation+Liu,+Fayao+and+Lin,+Guosheng+and+Qiao,+Ruizhi+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1603.02814.pdf"><img class="imgP  right"   src="data/thumbnail/Wu2017External_arXiv.jpg"></a><b>Image captioning and visual question answering based on attributes and external knowledge</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Q. Wu, C. Shen, P. Wang, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.02814">arXiv</a><a href="data/bibtex/Wu2017External.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Image+Captioning+and+Visual+Question+Answering+Based+on+Attributes+and+External+Knowledge+Wu,+Qi+and+Shen,+Chunhua+and+Wang,+Peng+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1601.04143.pdf"><img class="imgP  right"   src="data/thumbnail/TPAMI2017Liu_arXiv.jpg"></a><b>Compositional model based Fisher vector coding for image classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Liu, P. Wang, C. Shen, L. Wang, A. van den Hengel, C. Wang, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1601.04143">arXiv</a><a href="data/bibtex/TPAMI2017Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Compositional+Model+based+{F}isher+Vector+Coding+for+Image+Classification+Liu,+Lingqiao+and+Wang,+Peng+and+Shen,+Chunhua+and+Wang,+Lei+and+{van+den+Hengel},+Anton+and+Wang,+Chao+and+Shen,+Heng+Tao">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1510.00921.pdf"><img class="imgP  right"   src="data/thumbnail/Cross2017Liu_arXiv.jpg"></a><b>Cross-convolutional-layer pooling for image recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1510.00921">arXiv</a><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7779086">link</a><a href="data/bibtex/Cross2017Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Cross-convolutional-layer+Pooling+for+Image+Recognition+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1603.03183.pdf"><img class="imgP  right"   src="data/thumbnail/Lin2017Semantic_arXiv.jpg"></a><b>Exploring context with deep structured models for semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>G. Lin, C. Shen, A. van den Hengel, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.03183">arXiv</a><a href="data/bibtex/Lin2017Semantic.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploring+Context+with+Deep+Structured+models+for+Semantic+Segmentation+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Reid,+Ian">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1511.08531.pdf"><img class="imgP  right"   src="data/thumbnail/CVIU2016_arXiv.jpg"></a><b>Structured learning of metric ensembles with application to person re-identification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, L. Wu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Computer Vision and Image Understanding (CVIU), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1511.08531">arXiv</a><a href="data/bibtex/CVIU2016.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+learning+of+metric+ensembles+with+application+to+person+re-identification+Paisitkriangkrai,+Sakrapee+and+Wu,+Lin+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1501.00642.pdf"><img class="imgP  right"   src="data/thumbnail/Zhang2015IJCV_arXiv.jpg"></a><b>Unsupervised feature learning for dense correspondences across scenes</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Zhang, C. Shen, T. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1501.00642">arXiv</a><a href="data/bibtex/Zhang2015IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Feature+Learning+for+Dense+Correspondences+across+Scenes+Zhang,+Chao+and+Shen,+Chunhua+and+Shen,+Tingzhi">search</a><a href="https://bitbucket.org/chhshen/ufl">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1404.5009.pdf"><img class="imgP  right"   src="data/thumbnail/BnB2015Wang_arXiv.jpg"></a><b>Efficient semidefinite branch-and-cut for MAP-MRF inference</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>P. Wang, C. Shen, A. van den Hengel, P. Torr</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1404.5009">arXiv</a><a href="http://doi.org/10.1007/s11263-015-0865-2">link</a><a href="data/bibtex/BnB2015Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Semidefinite+Branch-and-Cut+for+{MAP-MRF}+Inference+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Torr,+Philip">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1506.06343.pdf"><img class="imgP  right"   src="data/thumbnail/Yao2016IJCV_arXiv.jpg"></a><b>Mining mid-level visual patterns with deep CNN activations</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1506.06343">arXiv</a><a href="http://rdcu.be/j1mA">link</a><a href="data/bibtex/Yao2016IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mining+Mid-level+Visual+Patterns+with+Deep+{CNN}+Activations+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a><a href="https://github.com/yaoliUoA/MDPM">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1310.1690.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2016Tracking_arXiv.jpg"></a><b>Online unsupervised feature learning for visual tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>F. Liu, C. Shen, I. Reid, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Image and Vision Computing (IVC), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1310.1690">arXiv</a><a href="data/bibtex/Liu2016Tracking.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Online+Unsupervised+Feature+Learning+for+Visual+Tracking+Liu,+Fayao+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Canonical principal angles correlation analysis for two-view data</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>S. Wang, J. Lu, X. Gu, C. Shen, R. Xia, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Journal of Visual Communication and Image Representation (JVCIR), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1016/j.jvcir.2015.12.001">link</a><a href="data/bibtex/Canonical2016Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Canonical+principal+angles+correlation+analysis+for+two-view+data+Wang,+Sheng+and+Lu,+Jianfeng+and+Gu,+Xingjian+and+Shen,+Chunhua+and+Xia,+Rui+and+Yang,+Jingyu">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1406.6811.pdf"><img class="imgP  right"   src="data/thumbnail/PRFace2016Shen_arXiv.jpg"></a><b>Face image classification by pooling raw features</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>F. Shen, C. Shen, X. Zhou, Y. Yang, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1406.6811">arXiv</a><a href="data/bibtex/PRFace2016Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Face+Image+Classification+by+Pooling+Raw+Features+Shen,+Fumin+and+Shen,+Chunhua+and+Zhou,+Xiang+and+Yang,+Yang+and+Shen,+Heng+Tao">search</a><a href="https://github.com/bd622/FacePooling">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1110.0264.pdf"><img class="imgP  right"   src="data/thumbnail/Face2016Li_arXiv.jpg"></a><b>Face recognition using linear representation ensembles</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>H. Li, F. Shen, C. Shen, Y. Yang, Y. Gao</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1110.0264">arXiv</a><a href="http://dx.doi.org/10.1016/j.patcog.2015.12.011">link</a><a href="data/bibtex/Face2016Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Face+Recognition+Using+Linear+Representation+Ensembles+Li,+Hanxi+and+Shen,+Fumin+and+Shen,+Chunhua+and+Yang,+Yang+and+Gao,+Yongsheng">search</a></p>
</li>
<li><p><b>Fast detection of multiple objects in traffic scenes with a common detection framework</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Q. Hu, S. Paisitkriangkrai, C. Shen, A. van den Hengel, F. Porikli</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Hu2015T-ITS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Detection+of+Multiple+Objects+in+Traffic+Scenes+with+a+Common+Detection+Framework+Hu,+Qichang+and+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Porikli,+Fatih">search</a></p>
</li>
<li><p><b>Part-based robust tracking using online latent structured learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>R. Yao, Q. Shi, C. Shen, Y. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TCSVT.2016.2527358">link</a><a href="data/bibtex/Part2016Yao.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Part-based+robust+tracking+using+online+latent+structured+learning+Yao,+Rui+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1503.01224.pdf"><img class="imgP  right"   src="data/thumbnail/Pooling2016Wang_arXiv.jpg"></a><b>Temporal pyramid pooling based convolutional neural network for action recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>P. Wang, Y. Cao, C. Shen, L. Liu, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1503.01224">arXiv</a><a href="data/bibtex/Pooling2016Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Temporal+Pyramid+Pooling+Based+Convolutional+Neural+Network+for+Action+Recognition+Wang,+Peng+and+Cao,+Yuanzhouhan+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Shen,+Heng+Tao">search</a></p>
</li>
<li><p><b>Dictionary learning for promoting structured sparsity in hyerpsectral compressive sensing</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Y. Zhang, C. Shen, A. van den Hengel, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2016TGSE.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Dictionary+Learning+for+Promoting+Structured+Sparsity+in+Hyerpsectral+Compressive+Sensing+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng">search</a></p>
</li>
<li><p><b>Scalable linear visual feature learning via online parallel nonnegative matrix factorization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Zhao, X. Li, Z. Zhang, C. Shen, L. Gao, X. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TNNLS.2015.2499273">link</a><a href="data/bibtex/Zhao2015TNN.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Scalable+Linear+Visual+Feature+Learning+via+Online+Parallel+Nonnegative+Matrix+Factorization+Zhao,+Xueyi+and+Li,+Xi+and+Zhang,+Zhongfei+and+Shen,+Chunhua+and+Gao,+Lixin+and+Li,+Xuelong">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.7564.pdf"><img class="imgP  right"   src="data/thumbnail/BQP2015Wang_arXiv.jpg"></a><b>Large-scale binary quadratic optimization using semidefinite relaxation and applications</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>P. Wang, C. Shen, A. van den Hengel, P. Torr</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.7564">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2016.2541146">link</a><a href="data/bibtex/BQP2015Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Large-scale+Binary+Quadratic+Optimization+Using+Semidefinite+Relaxation+and+Applications+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Torr,+Philip+H.+S.">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1409.5209.pdf"><img class="imgP  right"   src="data/thumbnail/Paisitkriangkrai2015TPAMI_arXiv.jpg"></a><b>Pedestrian detection with spatially pooled features and structured ensemble learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1409.5209">arXiv</a><a href="http://doi.org/10.1109/TPAMI.2015.2474388">link</a><a href="data/bibtex/Paisitkriangkrai2015TPAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Pedestrian+Detection+with+Spatially+Pooled+Features+and+Structured+Ensemble+Learning+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a><a href="https://github.com/chhshen/pedestrian-detection">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1401.7713.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2015TPAMI_arXiv.jpg"></a><b>A generalized probabilistic framework for compact codebook creation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Liu, L. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1401.7713">arXiv</a><a href="http://doi.org/10.1109/TPAMI.2015.2441069">link</a><a href="data/bibtex/Liu2015TPAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Generalized+Probabilistic+Framework+for+Compact+Codebook+Creation+Liu,+Lingqiao+and+Wang,+Lei+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1502.07411.pdf"><img class="imgP  right"   src="data/thumbnail/Depth2015Liu_arXiv.jpg"></a><b>Learning depth from single monocular images using deep convolutional neural fields</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>F. Liu, C. Shen, G. Lin, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1502.07411">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2015.2505283">link</a><a href="data/bibtex/Depth2015Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Depth+from+Single+Monocular+Images+Using+Deep+Convolutional+Neural+Fields+Liu,+Fayao+and+Shen,+Chunhua+and+Lin,+Guosheng+and+Reid,+Ian">search</a><a href="http://goo.gl/rAKWrS">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1507.05737.pdf"><img class="imgP  right"   src="data/thumbnail/Xi2015TPAMI_arXiv.jpg"></a><b>Online metric-weighted linear representations for robust visual tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Li, C. Shen, A. Dick, Z. Zhang, Y. Zhuang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1507.05737">arXiv</a><a href="data/bibtex/Xi2015TPAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Online+Metric-Weighted+Linear+Representations+for+Robust+Visual+Tracking+Li,+Xi+and+Shen,+Chunhua+and+Dick,+Anthony+and+Zhang,+Zhongfei+and+Zhuang,+Yueting">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1401.8126.pdf"><img class="imgP  right"   src="data/thumbnail/Harandi2015IJCV_arXiv.jpg"></a><b>Extrinsic methods for coding and dictionary learning on Grassmann manifolds</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>M. Harandi, R. Hartley, C. Shen, B. Lovell, C. Sanderson</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1401.8126">arXiv</a><a href="data/bibtex/Harandi2015IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Extrinsic+Methods+for+Coding+and+Dictionary+Learning+on+{G}rassmann+Manifolds+Harandi,+Mehrtash+and+Hartley,+Richard+and+Shen,+Chunhua+and+Lovell,+Brian+and+Sanderson,+Conrad">search</a><a href="https://github.com/chhshen/Grassmann/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1503.08263.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2015CRFPR_arXiv.jpg"></a><b>CRF learning with CNN features for image segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>F. Liu, G. Lin, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1503.08263">arXiv</a><a href="data/bibtex/Liu2015CRFPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={CRF}+Learning+with+{CNN}+Features+for+Image+Segmentation+Liu,+Fayao+and+Lin,+Guosheng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1412.0826.pdf"><img class="imgP  right"   src="data/thumbnail/Hashing2015Shen_arXiv.jpg"></a><b>Hashing on nonlinear manifolds</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>F. Shen, C. Shen, Q. Shi, A. van den Hengel, Z. Tang, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1412.0826">arXiv</a><a href="data/bibtex/Hashing2015Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Hashing+on+Nonlinear+Manifolds+Shen,+Fumin+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin+and+Shen,+Heng+Tao">search</a><a href="https://github.com/chhshen/Hashing-on-Nonlinear-Manifolds">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1409.2104.pdf"><img class="imgP  right"   src="data/thumbnail/TIP2014Shortcut_arXiv.jpg"></a><b>A computational model of the short-cut rule for 2D shape decomposition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Luo, C. Shen, X. Liu, C. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1409.2104">arXiv</a><a href="data/bibtex/TIP2014Shortcut.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Computational+Model+of+the+Short-Cut+Rule+for+{2D}+Shape+Decomposition+Luo,+Lei+and+Shen,+Chunhua+and+Liu,+Xinwang+and+Zhang,+Chunyuan">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.7450.pdf"><img class="imgP  right"   src="data/thumbnail/SDP2015Li_arXiv.jpg"></a><b>Worst-case linear discriminant analysis as scalable semidefinite feasibility problems</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>H. Li, C. Shen, A. van den Hengel, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.7450">arXiv</a><a href="data/bibtex/SDP2015Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Worst-Case+Linear+Discriminant+Analysis+as+Scalable+Semidefinite+Feasibility+Problems+Li,+Hui+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng">search</a><a href="https://github.com/chhshen/SDP-WLDA">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1408.5574.pdf"><img class="imgP  right"   src="data/thumbnail/FastHash2015Lin_arXiv.jpg"></a><b>Supervised hashing using graph cuts and boosted decision trees</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>G. Lin, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1408.5574">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2015.2404776">link</a><a href="data/bibtex/FastHash2015Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Supervised+Hashing+Using+Graph+Cuts+and+Boosted+Decision+Trees+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a><a href="https://bitbucket.org/chhshen/fasthash/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1304.1250.pdf"><img class="imgP  right"   src="data/thumbnail/Shen2014Outlier_arXiv.jpg"></a><b>Fast approximate <img class="eq" src="eqs/6240687686047976017-130.png"  style="vertical-align: -4px" /> minimization: Speeding up robust regression</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>F. Shen, C. Shen, R. Hill, A. van den Hengel, Z. Tang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Computational Statistics and Data Analysis (CSDA), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1304.1250">arXiv</a><a href="data/bibtex/Shen2014Outlier.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+approximate+L_\infty+minimization:+{S}peeding+up+robust+regression+Shen,+Fumin+and+Shen,+Chunhua+and+Hill,+Rhys+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1310.0890.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2014MKL_arXiv.jpg"></a><b>Multiple kernel learning in the primal for multi-modal Alzheimer's disease classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>F. Liu, L. Zhou, C. Shen, J. Yin</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Journal of Biomedical and Health Informatics (JBHI), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1310.0890">arXiv</a><a href="http://dx.doi.org/10.1109/JBHI.2013.2285378">link</a><a href="data/bibtex/Liu2014MKL.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multiple+Kernel+Learning+in+the+Primal+for+Multi-modal+{A}lzheimer's+Disease+Classification+Liu,+Fayao+and+Zhou,+Luping+and+Shen,+Chunhua+and+Yin,+Jianping">search</a></p>
<ol reversed>
<li><p>Online published at IEEE: 10 October 2013.</p>
</li></ol>
</li>
<li><p><b>Multiple kernel clustering based on centered kernel alignment</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Lu, L. Wang, J. Lu, J. Yang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/MKL2014.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multiple+kernel+clustering+based+on+centered+kernel+alignment+Lu,+Yanting+and+Wang,+Liantao+and+Lu,+Jianfeng+and+Yang,+Jingyu+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1402.5497.pdf"><img class="imgP  right"   src="data/thumbnail/Yan2014TIPa_arXiv.jpg"></a><b>Efficient semidefinite spectral clustering via Lagrange duality</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Yan, C. Shen, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1402.5497">arXiv</a><a href="data/bibtex/Yan2014TIPa.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Semidefinite+Spectral+Clustering+via+{L}agrange+Duality+Yan,+Yan+and+Shen,+Chunhua+and+Wang,+Hanzi">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1402.6383.pdf"><img class="imgP  right"   src="data/thumbnail/Paul2014TIPb_arXiv.jpg"></a><b>Large-margin learning of compact binary image encodings</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1402.6383">arXiv</a><a href="data/bibtex/Paul2014TIPb.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Large-margin+Learning+of+Compact+Binary+Image+Encodings+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1309.6691.pdf"><img class="imgP  right"   src="data/thumbnail/Li2014TIP_arXiv.jpg"></a><b>Characterness: An indicator of text in the wild</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Li, W. Jia, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1309.6691">arXiv</a><a href="http://dx.doi.org/10.1109/TIP.2014.2302896">link</a><a href="data/bibtex/Li2014TIP.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Characterness:+{A}n+Indicator+of+Text+in+the+Wild+Li,+Yao+and+Jia,+Wenjing+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a><a href="https://github.com/yaoliUoA/characterness">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1401.0764.pdf"><img class="imgP  right"   src="data/thumbnail/Li2013Hyper_arXiv.jpg"></a><b>Context-aware hypergraph construction for robust spectral clustering</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Li, W. Hu, C. Shen, A. Dick, Z. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Knowledge and Data Engineering (TKDE), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1401.0764">arXiv</a><a href="http://doi.ieeecomputersociety.org/10.1109/TKDE.2013.126">link</a><a href="data/bibtex/Li2013Hyper.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Context-aware+hypergraph+construction+for+robust+spectral+clustering+Li,+Xi+and+Hu,+Weiming+and+Shen,+Chunhua+and+Dick,+Anthony+and+Zhang,+Zhongfei">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1303.6066.pdf"><img class="imgP  right"   src="data/thumbnail/Paul2013TMM_arXiv.jpg"></a><b>Asymmetric pruning for learning cascade detectors</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1303.6066">arXiv</a><a href="http://dx.doi.org/10.1109/TMM.2014.2308723">link</a><a href="data/bibtex/Paul2013TMM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Asymmetric+pruning+for+learning+cascade+detectors+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2014Metric_arXiv.jpg"><b>Efficient dual approach to distance metric learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, J. Kim, F. Liu, L. Wang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1302.3219">arXiv</a><a href="data/bibtex/Shen2014Metric.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Dual+Approach+to+Distance+Metric+Learning+Shen,+Chunhua+and+Kim,+Junae+and+Liu,+Fayao+and+Wang,+Lei+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Paul2013Fastboosting_PDF.jpg"><b>A scalable stage-wise approach to large-margin multi-class loss based boosting</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1307.5497">arXiv</a><a href="http://dx.doi.org/10.1109/TNNLS.2013.2282369">link</a><a href="https://bytebucket.org/chhshen/data/raw/7e2f958b104603e54e9d8376a8e1672363f742a3/papers/Paisitkriangkrai2014TNNLS.pdf">pdf</a><a href="data/bibtex/Paul2013Fastboosting.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+scalable+stage-wise+approach+to+large-margin+multi-class+loss+based+boosting+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1302.0963.pdf"><img class="imgP  right"   src="data/thumbnail/Paisitkriangkrai2013RandomBoost_arXiv.jpg"></a><b>RandomBoost: Simplified multi-class boosting through randomization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, Q. Shi, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1302.0963">arXiv</a><a href="http://dx.doi.org/10.1109/TNNLS.2013.2281214">link</a><a href="data/bibtex/Paisitkriangkrai2013RandomBoost.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={RandomBoost}:+{S}implified+Multi-class+Boosting+through+Randomization+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>A hierarchical word-merging algorithm with class separability measure</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Wang, L. Zhou, C. Shen, L. Liu, H. Liu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="https://bitbucket.org/chhshen/chhshen.bitbucket.org/src/be12d4ef8deb6207ec97f0fdac6efbe2df151b59/_download/TPAMI14Wang.pdf">pdf</a><a href="data/bibtex/Wang2014PAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Hierarchical+Word-merging+Algorithm+with+Class+Separability+Measure+Wang,+Lei+and+Zhou,+Luping+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Liu,+Huan">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2014SBoosting_PDF.jpg"><b>StructBoost: Boosting methods for predicting structured output variables</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, G. Lin, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1302.3283">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2014.2315792">link</a><a href="http://goo.gl/goCVLK">pdf</a><a href="data/bibtex/Shen2014SBoosting.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={StructBoost}:+{B}oosting+Methods+for+Predicting+Structured+Output+Variables+Shen,+Chunhua+and+Lin,+Guosheng+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1301.2032.pdf"><img class="imgP  right"   src="data/thumbnail/FisherBoost2013IJCV_arXiv.jpg"></a><b>Training effective node classifiers for cascade classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, P. Wang, S. Paisitkriangkrai, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1301.2032">arXiv</a><a href="http://link.springer.com/article/10.1007%2Fs11263-013-0608-1">link</a><a href="data/bibtex/FisherBoost2013IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Training+Effective+Node+Classifiers+for+Cascade+Classification+Shen,+Chunhua+and+Wang,+Peng+and+Paisitkriangkrai,+Sakrapee+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Fully corrective boosting with arbitrary loss and regularization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, H. Li, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Neural Networks (NN), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/78929">pdf</a><a href="data/bibtex/Shen2013NN.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fully+Corrective+Boosting+with+Arbitrary+Loss+and+Regularization+Shen,+Chunhua+and+Li,+Hanxi+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Approximate least trimmed sum of squares fitting and applications in image analysis</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>F. Shen, C. Shen, A. van den Hengel, Z. Tang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6408142">link</a><a href="http://hdl.handle.net/2440/79428">pdf</a><a href="data/bibtex/LMS2013TIP.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Approximate+Least+Trimmed+Sum+of+Squares+Fitting+and+Applications+in+Image+Analysis+Shen,+Fumin+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin">search</a></p>
</li>
<li><p><b>Visual tracking with spatio-temporal Dempster-Shafer information fusion</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Li, A. Dick, C. Shen, Z. Zhang, A. van den Hengel, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/77448">pdf</a><a href="data/bibtex/Xi2013TIP.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+Tracking+with+Spatio-Temporal+{Dempster-Shafer}+Information+Fusion+Li,+Xi+and+Dick,+Anthony+and+Shen,+Chunhua+and+Zhang,+Zhongfei+and+{van+den+Hengel},+Anton+and+Wang,+Hanzi">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1303.4803.pdf"><img class="imgP  right"   src="data/thumbnail/Xi2013Survey_arXiv.jpg"></a><b>A survey of appearance models in visual object tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Li, W. Hu, C. Shen, Z. Zhang, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>ACM Transactions on Intelligent Systems and Technology (TIST), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1303.4803">arXiv</a><a href="data/bibtex/Xi2013Survey.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Survey+of+Appearance+Models+in+Visual+Object+Tracking+Li,+Xi+and+Hu,+Weiming+and+Shen,+Chunhua+and+Zhang,+Zhongfei+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Shape similarity analysis by self-tuning locally constrained mixed-diffusion</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Luo, C. Shen, C. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/73304">pdf</a><a href="data/bibtex/TMM2013Shape.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Shape+Similarity+Analysis+by+Self-Tuning+Locally+Constrained+Mixed-Diffusion+Luo,+Lei+and+Shen,+Chunhua+and+Zhang,+Chunyuan+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TPAMI2013Xi_PDF.jpg"><b>Incremental learning of 3D-DCT compact representations for robust visual tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Li, A. Dick, C. Shen, A. van den Hengel, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1207.3389">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2012.166">link</a><a href="https://sites.google.com/site/chhshen/publication/tpami12xi.pdf?attredirects=1">pdf</a><a href="data/bibtex/TPAMI2013Xi.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Incremental+Learning+of+{3D-DCT}+Compact+Representations+for+Robust+Visual+Tracking+Li,+Xi+and+Dick,+Anthony+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Wang,+Hanzi">search</a><a href="https://github.com/chhshen/DCT-Tracking/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1104.4704.pdf"><img class="imgP  right"   src="data/thumbnail/JMLR2012Shen_arXiv.jpg"></a><b>Positive semidefinite metric learning using boosting-like algorithms</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, J. Kim, L. Wang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Journal of Machine Learning Research (JMLR), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1104.4704">arXiv</a><a href="http://jmlr.csail.mit.edu/papers/v13/shen12a.html">link</a><a href="data/bibtex/JMLR2012Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Positive+Semidefinite+Metric+Learning+Using+Boosting-like+Algorithms+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei+and+{van+den+Hengel},+Anton">search</a><a href="https://bitbucket.org/chhshen/data/raw/45d101372013763d18f0a7ed191c86569532ed96/code/BoostMetric-NeurIPS09-codes-V0.1.tar.bz2">project webpage</a></p>
</li>
<li><p><b>Fast and robust object detection using asymmetric totally-corrective boosting</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>P. Wang, C. Shen, N. Barnes, H. Zheng</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/66763">pdf</a><a href="data/bibtex/AsymBoost2011Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+and+Robust+Object+Detection+Using+Asymmetric+Totally-corrective+Boosting+Wang,+Peng+and+Shen,+Chunhua+and+Barnes,+Nick+and+Zheng,+Hong">search</a></p>
</li>
<li><p><b>UBoost: Boosting with the Universum</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, P. Wang, F. Shen, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/67027">pdf</a><a href="data/bibtex/UBoost2011Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={UBoost}:+{B}oosting+with+the+{U}niversum+Shen,+Chunhua+and+Wang,+Peng+and+Shen,+Fumin+and+Wang,+Hanzi">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/0903.3103.pdf"><img class="imgP  right"   src="data/thumbnail/GSLDA2010Shen_arXiv.jpg"></a><b>Efficiently learning a detection cascade with sparse eigenvectors</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, S. Paisitkriangkrai, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0903.3103">arXiv</a><a href="http://dx.doi.org/10.1109/TIP.2010.2055880">link</a><a href="data/bibtex/GSLDA2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficiently+Learning+a+Detection+Cascade+with+Sparse+Eigenvectors+Shen,+Chunhua+and+Paisitkriangkrai,+Sakrapee+and+Zhang,+Jian">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1005.4118.pdf"><img class="imgP  right"   src="data/thumbnail/Incremental2010Shen_arXiv.jpg"></a><b>Incremental training of a detector using online sparse eigen-decomposition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1005.4118">arXiv</a><a href="http://dx.doi.org/10.1109/TIP.2010.2053548">link</a><a href="data/bibtex/Incremental2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Incremental+Training+of+a+Detector+Using+Online+Sparse+Eigen-decomposition+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Li2010Interactive_PDF.jpg"><b>Interactive color image segmentation with linear programming</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>H. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Machine Vision and Applications (MVA), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://www.springerlink.com/content/b254775776114226">link</a><a href="http://sites.google.com/site/chhshen/publication/MVA2010LP.pdf">pdf</a><a href="data/bibtex/Li2010Interactive.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Interactive+Color+Image+Segmentation+with+Linear+Programming+Li,+Hongdong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Generalized2010Shen_PDF.jpg"><b>Generalized kernel-based visual tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, J. Kim, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0905.2463">arXiv</a><a href="http://dx.doi.org/10.1109/TCSVT.2009.2031393">link</a><a href="http://sites.google.com/site/chhshen/publication/TCSVT2010.pdf">pdf</a><a href="data/bibtex/Generalized2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Generalized+Kernel-based+Visual+Tracking+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Hanzi">search</a><a href="https://github.com/chhshen/KernelTracking">project webpage</a></p>
</li>
<li><p><b>Feature selection with redundancy-constrained class separability</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Zhou, L. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks (TNN), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TNN.2010.2044189">link</a><a href="data/bibtex/Zhou2010FS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Feature+Selection+With+Redundancy-Constrained+Class+Separability+Zhou,+Luping+and+Wang,+Lei+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/0904.2037.pdf"><img class="imgP  right"   src="data/thumbnail/MDBoost2010Shen_arXiv.jpg"></a><b>Boosting through optimization of margin distributions</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, H. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks (TNN), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0904.2037">arXiv</a><a href="http://dx.doi.org/10.1109/TNN.2010.2040484">link</a><a href="data/bibtex/MDBoost2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Boosting+through+optimization+of+margin+distributions+Shen,+Chunhua+and+Li,+Hanxi">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1003.0487.pdf"><img class="imgP  right"   src="data/thumbnail/Scalable2010Shen_arXiv.jpg"></a><b>Scalable large-margin Mahalanobis distance metric learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, J. Kim, L. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks (TNN), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1003.0487">arXiv</a><a href="http://dx.doi.org/10.1109/TNN.2010.2052630">link</a><a href="data/bibtex/Scalable2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Scalable+Large-Margin+{M}ahalanobis+Distance+Metric+Learning+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/0901.3590.pdf"><img class="imgP  right"   src="data/thumbnail/Dual2010Shen_arXiv.jpg"></a><b>On the dual formulation of boosting algorithms</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, H. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0901.3590">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2010.47">link</a><a href="data/bibtex/Dual2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=On+the+Dual+Formulation+of+Boosting+Algorithms+Shen,+Chunhua+and+Li,+Hanxi">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Performance2008Paul_PDF.jpg"><b>Performance evaluation of local features in human classification and detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IET Computer Vision (IETCV), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1049/iet-cvi:20080026">link</a><a href="http://sites.google.com/site/chhshen/publication/Huam2009IET.pdf">pdf</a><a href="data/bibtex/Performance2008Paul.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Performance+Evaluation+of+Local+Features+in+Human+Classification+and+Detection+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian">search</a></p>
<ol reversed>
<li><p>Invited submission, special issue of DICTA2007.</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/SDP2008Shen_PDF.jpg"><b>Supervised dimensionality reduction via sequential semidefinite programming</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, H. Li, M. Brooks</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1016/j.patcog.2008.06.015">link</a><a href="http://sites.google.com/site/chhshen/publication/PR1.pdf">pdf</a><a href="data/bibtex/SDP2008Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Supervised+Dimensionality+Reduction+via+Sequential+Semidefinite+Programming+Shen,+Chunhua+and+Li,+Hongdong+and+Brooks,+Michael+J.">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Human2008Paul_PDF.jpg"><b>Fast pedestrian detection using a cascade of boosted covariance features</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TCSVT.2008.928213">link</a><a href="http://goo.gl/lgpDJB">pdf</a><a href="data/bibtex/Human2008Paul.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Pedestrian+Detection+Using+a+Cascade+of+Boosted+Covariance+Features+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Fast2007Shen_PDF.jpg"><b>Fast global kernel density mode seeking: applications to localization and tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Shen, M. Brooks, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2007</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TIP.2007.894233">link</a><a href="http://sites.google.com/site/chhshen/publication/TIP2007Shen.pdf">pdf</a><a href="data/bibtex/Fast2007Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+global+kernel+density+mode+seeking:+applications+to+localization+and+tracking+Shen,+Chunhua+and+Brooks,+Michael+J.+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Adaptive object tracking based on an effective appearance filter</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>H. Wang, D. Suter, K. Schindler, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2007</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TPAMI.2007.1112">link</a><a href="http://goo.gl/6rQTA1">pdf</a><a href="data/bibtex/Adaptive2007Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adaptive+object+tracking+based+on+an+effective+appearance+filter+Wang,+Hanzi+and+Suter,+David+and+Schindler,+Konrad+and+Shen,+Chunhua">search</a></p>
<ol reversed>
<li><p>Featured article of September issue 2007.</p>
</li></ol>
</li>
<li><p><b>Active control of radiation from a piston set in a rigid sphere</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Z. Lin, J. Lu, C. Shen, X. Qiu, B. Xu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Journal of Acoustical Society of America (JASA), 2004</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1121/1.1736654">link</a><a href="http://goo.gl/nc4SjU">pdf</a><a href="data/bibtex/Lin2004Active.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Active+control+of+radiation+from+a+piston+set+in+a+rigid+sphere+Lin,+Zhibin+and+Lu,+Jing+and+Shen,+Chunhua+and+Qiu,+Xiaojun+and+Xu,+Boling">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Lu2003Lattice_PDF.jpg"><b>Lattice form adaptive infinite impulse response filtering algorithm for active noise control</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>J. Lu, C. Shen, X. Qiu, B. Xu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Journal of Acoustical Society of America (JASA), 2003</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1121/1.1529665">link</a><a href="http://sites.google.com/site/chhshen/publication/Lattice2003JASA.pdf?attredirects=1">pdf</a><a href="data/bibtex/Lu2003Lattice.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Lattice+form+adaptive+infinite+impulse+response+filtering+algorithm+for+active+noise+control+Lu,+Jing+and+Shen,+Chunhua+and+Qiu,+Xiaojun+and+Xu,+Boling">search</a></p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>Diverse knowledge distillation for end-to-end person search</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Zhang, X. Wang, J. Bian, C. Shen, M. You</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/ZhangPerson2021AAAI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Diverse+Knowledge+Distillation+for+End-to-end+Person+Search+Zhang,+Xinyu+and+Wang,+Xinlong+and+Bian,+Jia-Wang+and+Shen,+Chunhua+and+You,+Minyu">search</a></p>
</li>
<li><p><b>SA-BNN: state-aware binary neural network</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Liu, P. Chen, B. Zhuang, C. Shen, B. Zhang, W. Ding</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/LiuBNN2021AAAI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SA-BNN}:+State-Aware+Binary+Neural+Network+Liu,+Chunlei+and+Chen,+Peng+and+Zhuang,+Bohan+and+Shen,+Chunhua+and+Zhang,+Baochang+and+Ding,+Wenrui">search</a></p>
</li>
<li><p><b>Deep reasoning network for few-shot semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Zhuge, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Proc. ACM International Conference on Multimedia (ACMMM&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/MM2021B.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Reasoning+Network+for+Few-shot+Semantic+Segmentation+Zhuge,+Yuzhi+and+Shen,+Chunhua">search</a></p>
</li>
</ol>
<div id="footer">
<div id="footer-text">
&copy; <b>Chunhua Shen</b>
&bull;
last update: 2021-07-09 16:24:17 ACST
&bull;
<a href="#" onClick="changewidth(1);return false" title="Expand page width"><b>&larr;&rarr;</b></a>
&bull;
<a href="#" onClick="changewidth(-1);return false" title="Reduce page width"><b>&rarr;&larr;</b></a>
<!-- Javascript -->
<script
    src="./script/jquery-1.6.2.min.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery.flot.min.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery-scroll.js"
    type="text/javascript">
</script>
<script
    src="./script/width_change.js"
    type="text/javascript">
</script>
<script
    src="./script/reverse_ol.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery.highlight.js"
    type="text/javascript">
</script>
<!-- Required for the jQuery.LocalScroll Plug-in -->
<script type="text/javascript">
    $(document).ready(function(){
    //
    $.localScroll();
    //
    // Round images
    //
	$(".rounded-img, .rounded-img2").load(function() {
	$(this).wrap(function(){
	return '<span class="' + $(this).attr('class')
                + '" style="background:url(' + $(this).attr('src')
                + ') no-repeat center center; width: '
                + $(this).width() + 'px; height: '
                + $(this).height() + 'px;" />';
		});
		$(this).css("opacity","0");
	});
      //
      //
      //  nav tab animation
        var navDuration = 150; //time in miliseconds
        $('#nav li a').hover(function() {
          $(this).animate({ paddingTop:"50px"  }, navDuration);
        }, function() {
             $(this).animate({ paddingTop:"31px"}, navDuration);
        });
        //
        // plot citation figure using jquery flot, 2012 July, CS
        //
        var flot_options = {
        legend: {
            show: false,
            margin: 10,
            backgroundOpacity: 0.5
                },
        bars:  {
            show: true,
            barWidth: 0.6,
            align: "center"
        },
        yaxis: {
            min: -20,
            tickFormatter: function(val, axis) {
                if (val < 50)
                    return " &nbsp; ";  // some string
                else
                    return val < axis.max ? val.toFixed(0) :   "  &nbsp;  ";
            }
        },
        grid: {
            borderWidth: 0
        }
    };  // end of flot_options
    $.getJSON("./data/cs_cite.json", function(json) {
       //succes - data loaded, now use plot:
       var plotarea = $("#citation_plot_holder");
       var data=[json.data];
       $.plot(plotarea , data, flot_options);
    });
//
// end of jquery flot
//
    changewidth( 0.9 );
//
//
//  highlight ``Shen''
    $("body p").highlight(['C. Shen', 'Chunhua Shen']);
//  highlight selected publication venues
    $("body p").highlight(['CVPR', 'ICCV', 'ECCV', 'ICML', 'NeurIPS',
    'TPAMI', 'IJCV', 'JMLR'],  { element: 'span', className: 'selected_venue' } );
//
    });
</script>
<!-- News ticker -->
<script type="text/javascript">
    function tick(){
        $('#ticker li:first').slideUp( function () { $(this).appendTo($('#ticker')).slideDown(); });
    }
    setInterval(function(){ tick () }, 5000);
</script>
</div>
</div>
</div>
</body>
</html>
