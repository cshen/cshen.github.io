<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="css/fonts_import.css"       type="text/css" />
<link rel="stylesheet" href="css/cs.css"                 type="text/css" />
<link rel="stylesheet" href="css/content.css"            type="text/css" />
<!-- font family -->
<link href="//netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<link rel="stylesheet" href="css/full_publication.css" type="text/css" />
<title>Chunhua Shen</title>
</head>
<body>
<div id="layout-content">
<div id="menu">
    <div id="menucontainer">
<ul id="nav">
   <li><a href="index.html" target="_self">Home</a></li>
   <li><a href="paper.html" target="_self">Publications</a></li>
   <li><a href="teaching.html" target="_self">Teaching</a></li>
</ul>
</div>
</div>
<div id="toptitle">
<h1>Publications (Full List)</h1>
<div id="subtitle">Categorised <a href="fullpaper2.html" target=&ldquo;blank&rdquo;>by venue <i class='fa fa-location-arrow' aria-hidden='true'></i></a>,  <a href="fullpaper.html" target=&ldquo;blank&rdquo;>by year <i class='fa fa-clock-o' aria-hidden='true'></i></a>. <b>421</b>  papers.  
</div>
</div>
<p><a href="https://scholar.google.com/citations?hl=en&amp;user=Ljk2BvIAAAAJ&amp;view_op=list_works&amp;pagesize=100" target=&ldquo;blank&rdquo;>Google scholar (61280 citations)  <i class='ai ai-google-scholar'   aria-hidden='true'></i></a>,
<a href="https://dblp.org/pid/56/1673.html" target=&ldquo;blank&rdquo;>DBLP <i class='ai ai-dblp ai-1x'></i></a>,
<a href="https://arxiv.org/a/shen_c_1.html" target=&ldquo;blank&rdquo;>arXiv <i class='ai ai-arxiv ai-1x'></i></a>.
</p>
<p><div id="citation_plot_holder"></div>
</p>
<h1>2023</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhang2023SegVITv2xxxarXiv.jpg"><b>SegViT v2: exploring efficient and continual semantic segmentation with plain vision transformers</b>   
<br />\(\cdot\) <i>B. Zhang, L. Liu, M. Phan, Z. Tian, C. Shen, Y. Liu</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2306.06289" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhang2023SegVITv2.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SegViT}+v2:+Exploring+Efficient+and+Continual+Semantic+Segmentation+with+Plain+Vision+Transformers+Zhang,+Bowen+and+Liu,+Liyang+and+Phan,+Minh+Hieu+and+Tian,+Zhi+and+Shen,+Chunhua+and+Liu,+Yifan" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SegViT}+v2:+Exploring+Efficient+and+Continual+Semantic+Segmentation+with+Plain+Vision+Transformers" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/zbwxp/SegVit" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>SPL-Net: spatial-semantic patch learning network for facial attribute recognition with limited labeled data</b>   
<br />\(\cdot\) <i>Y. Yan, Y. Shu, S. Chen, J. Xue, C. Shen, H. Wang</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2023</i>.
<br />\(\cdot\) <a href="data/bibtex/YAN2023IJCVSPL.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SPL-Net}:+Spatial-Semantic+Patch+Learning+Network+for+Facial+Attribute+Recognition+with+Limited+Labeled+Data+Yan,+Yan+and+Shu,+Ying+and+Chen,+Si+and+Xue,+Jing-Hao+and+Shen,+Chunhua+and+Wang,+Hanzi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SPL-Net}:+Spatial-Semantic+Patch+Learning+Network+for+Facial+Attribute+Recognition+with+Limited+Labeled+Data" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Lu2023IJCVCountingxxxarXiv.jpg"><b>From open set to closed set: supervised spatial divide-and-conquer for object counting</b>   
<br />\(\cdot\) <i>H. Xiong, H. Lu, C. Liu, L. Liu, C. Shen, Z. Cao</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2001.01886" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Lu2023IJCVCounting.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=From+Open+Set+to+Closed+Set:+Supervised+Spatial+Divide-and-Conquer+for+Object+Counting+Xiong,+Haipeng+and+Lu,+Hao+and+Liu,+Chengxin+and+Liu,+Liang+and+Shen,+Chunhua+and+Cao,+Zhiguo" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=From+Open+Set+to+Closed+Set:+Supervised+Spatial+Divide-and-Conquer+for+Object+Counting" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>A dynamic feature interaction framework for multi-task visual perception</b>   
<br />\(\cdot\) <i>Y. Xi, H. Chen, N. Wang, P. Wang, Y. Zhang, C. Shen, Y. Liu</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2023</i>.
<br />\(\cdot\) <a href="data/bibtex/XiY2023IJCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Dynamic+Feature+Interaction+Framework+for+Multi-task+Visual+Perception+Xi,+Yuling+and+Chen,+Hao+and+Wang,+Ning+and+Wang,+Peng+and+Zhang,+Yanning+and+Shen,+Chunhua+and+Liu,+Yifan" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Dynamic+Feature+Interaction+Framework+for+Multi-task+Visual+Perception" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Lin2023IJCVSuperxxxarXiv.jpg"><b>Super vision transformer</b>   
<br />\(\cdot\) <i>M. Lin, M. Chen, Y. Zhang, C. Shen, R. Ji, L. Cao</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2205.11397" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Lin2023IJCVSuper.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Super+Vision+Transformer+Lin,+Mingbao+and+Chen,+Mengzhao+and+Zhang,+Yuxin+and+Shen,+Chunhua+and+Ji,+Rongrong+and+Cao,+Liujuan" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Super+Vision+Transformer" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/lmbxmu/SuperViT" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>SAI: an efficient and user-friendly tool for measurement of stomatal pores and density using deep computer vision</b>   
<br />\(\cdot\) <i>N. Sai, J. Bockman, H. Chen, N. Watson-Haigh, B. Xu, X. Feng, A. Piechatzek, C. Shen, M. Gilliham</i>.
<br />\(\cdot\) <i>New Phytologist (NPH), 2023</i>.
<br />\(\cdot\) <a href="https://doi.org/10.1101/2022.02.07.479482" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Sai2023NPJ.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SAI}:+An+efficient+and+user-friendly+tool+for+measurement+of+stomatal+pores+and+density+using+deep+computer+vision+Sai,+Na+and+Bockman,+James+Paul+and+Chen,+Hao+and+Watson-Haigh,+Nathan+and+Xu,+Bo+and+Feng,+Xueying+and+Piechatzek,+Adriane+and+Shen,+Chunhua+and+Gilliham,+Matthew" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SAI}:+An+efficient+and+user-friendly+tool+for+measurement+of+stomatal+pores+and+density+using+deep+computer+vision" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Xie2023DodNetxxxarXiv.jpg"><b>Learning from partially labeled data for multi-organ and tumor segmentation</b>   
<br />\(\cdot\) <i>Y. Xie, J. Zhang, Y. Xia, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2211.06894" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Xie2023DodNet.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+from+partially+labeled+data+for+multi-organ+and+tumor+segmentation+Xie,+Yutong+and+Zhang,+Jianpeng+and+Xia,+Yong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+from+partially+labeled+data+for+multi-organ+and+tumor+segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/DoDNet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Sun2023TPAMIxxxarXiv.jpg"><b>SC-DepthV3: robust self-supervised monocular depth estimation for dynamic scenes</b>   
<br />\(\cdot\) <i>L. Sun, J. Bian, H. Zhan, W. Yin, I. Reid, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2211.03660" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Sun2023TPAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SC-DepthV3}:+Robust+Self-supervised+Monocular+Depth+Estimation+for+Dynamic+Scenes+Sun,+Libo+and+Bian,+Jia-Wang+and+Zhan,+Huangying+and+Yin,+Wei+and+Reid,+Ian+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SC-DepthV3}:+Robust+Self-supervised+Monocular+Depth+Estimation+for+Dynamic+Scenes" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/JiawangBian/sc_depth_pl" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/SPTSv2xxxarXiv.jpg"><b>SPTS v2: single-point scene text spotting</b>   
<br />\(\cdot\) <i>Y. Liu, J. Zhang, D. Peng, M. Huang, X. Wang, J. Tang, C. Huang, D. Lin, C. Shen, X. Bai, L. Jin</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2301.01635" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/SPTSv2.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SPTS+v2}:+Single-Point+Scene+Text+Spotting+Liu,+Yuliang+and+Zhang,+Jiaxin+and+Peng,+Dezhi+and+Huang,+Mingxin+and+Wang,+Xinyu+and+Tang,+Jingqun+and+Huang,+Can+and+Lin,+Dahua+and+Shen,+Chunhua+and+Bai,+Xiang+and+Jin,+Lianwen" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SPTS+v2}:+Single-Point+Scene+Text+Spotting" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/Yuliang-Liu/SPTSv2" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2023TPAMIxxxarXiv.jpg"><b>Single-path bit sharing for automatic loss-aware model compression</b>   
<br />\(\cdot\) <i>J. Liu, B. Zhuang, P. Chen, C. Shen, J. Cai, M. Tan</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2101.04935" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Liu2023TPAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Single-path+Bit+Sharing+for+Automatic+Loss-aware+Model+Compression+Liu,+Jing+and+Zhuang,+Bohan+and+Chen,+Peng+and+Shen,+Chunhua+and+Cai,+Jianfei+and+Tan,+Mingkui" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Single-path+Bit+Sharing+for+Automatic+Loss-aware+Model+Compression" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>FoPro: few-shot guided robust webly-supervised prototypical learning</b>   
<br />\(\cdot\) <i>Y. Qin, X. Chen, C. Chen, Y. Shen, B. Ren, Y. Gu, J. Yang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="data/bibtex/FoPro2023AAAI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FoPro}:+Few-Shot+Guided+Robust+Webly-Supervised+Prototypical+Learning+Qin,+Yulei+and+Chen,+Xinyu+and+Chen,+Chao+and+Shen,+Yunhang+and+Ren,+Bo+and+Gu,+Yun+and+Yang,+Jie+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={FoPro}:+Few-Shot+Guided+Robust+Webly-Supervised+Prototypical+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Point-Teaching: weakly semi-supervised object detection with point annotations</b>   
<br />\(\cdot\) <i>Y. Ge, Q. Zhou, X. Wang, Z. Wang, H. Li, C. Shen</i>.
<br />\(\cdot\) <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="data/bibtex/PointTeaching2023AAAI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={Point-Teaching}:+Weakly+Semi-Supervised+Object+Detection+with+Point+Annotations+Ge,+Yongtao+and+Zhou,+Qiang+and+Wang,+Xinlong+and+Wang,+Zhibin+and+Li,+Hao+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={Point-Teaching}:+Weakly+Semi-Supervised+Object+Detection+with+Point+Annotations" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR2023aWangxxxarXiv.jpg"><b>Images speak in images: a generalist painter for in-context visual learning</b>   
<br />\(\cdot\) <i>X. Wang, W. Wang, Y. Cao, C. Shen, T. Huang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2212.02499" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2023aWang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Images+Speak+in+Images:+A+Generalist+Painter+for+In-Context+Visual+Learning+Wang,+Xinlong+and+Wang,+Wen+and+Cao,+Yue+and+Shen,+Chunhua+and+Huang,+Tiejun" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Images+Speak+in+Images:+A+Generalist+Painter+for+In-Context+Visual+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/baaivision/Painter" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Learning conditional attributes for compositional zero-shot learning</b>   
<br />\(\cdot\) <i>Q. Wang, L. Liu, C. Jing, H. Chen, G. Liang, P. Wang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="data/bibtex/CVPR2023B.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Conditional+Attributes+for+Compositional+Zero-Shot+Learning+Wang,+Qingsheng+and+Liu,+Lingqiao+and+Jing,+Chenchen+and+Chen,+Hao+and+Liang,+Guoqiang+and+Wang,+Peng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Conditional+Attributes+for+Compositional+Zero-Shot+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Segprompt: boosting open-world segmentation via category-level prompt learning</b>   
<br />\(\cdot\) <i>M. Zhu, H. Li, H. Chen, C. Fan, W. Mao, C. Jing, Y. Liu, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhu2023ICCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=SegPrompt:+Boosting+Open-World+Segmentation+via+Category-level+Prompt+Learning+Zhu,+Muzhi+and+Li,+Hengtao+and+Chen,+Hao+and+Fan,+Chengxiang+and+Mao,+Weian+and+Jing,+Chenchen+and+Liu,+Yifan+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=SegPrompt:+Boosting+Open-World+Segmentation+via+Category-level+Prompt+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/YZhao2023ICCVxxxarXiv.jpg"><b>Generative prompt model for weakly supervised object localization</b>   
<br />\(\cdot\) <i>Y. Zhao, Q. Ye, W. Wu, C. Shen, F. Wan</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2307.09756" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/YZhao2023ICCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Generative+Prompt+Model+for+Weakly+Supervised+Object+Localization+Zhao,+Yuzhong+and+Ye,+Qixiang+and+Wu,+Weijia+and+Shen,+Chunhua+and+Wan,+Fang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Generative+Prompt+Model+for+Weakly+Supervised+Object+Localization" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Robust geometry-preserving depth estimation using differentiable rendering</b>   
<br />\(\cdot\) <i>C. Zhang, W. Yin, G. Yu, Z. Wang, T. Chen, B. Fu, J. Zhou, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="data/bibtex/ZhangC2023ICCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Robust+Geometry-Preserving+Depth+Estimation+Using+Differentiable+Rendering+Zhang,+Chi+and+Yin,+Wei+and+Yu,+Gang+and+Wang,+Zhibin+and+Chen,+Tao+and+Fu,+Bin+and+Zhou,+Joey+Tianyi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Robust+Geometry-Preserving+Depth+Estimation+Using+Differentiable+Rendering" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/KYQZ2023ICCVxxxarXiv.jpg"><b>CTVIS: consistent training for online video instance segmentation</b>   
<br />\(\cdot\) <i>K. Ying, Q. Zhong, W. Mao, Z. Wang, H. Chen, L. Wu, Y. Liu, C. Fan, Y. Zhuge, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2307.12616" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/KYQZ2023ICCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={CTVIS}:+Consistent+Training+for+Online+Video+Instance+Segmentation+Ying,+Kaining+and+Zhong,+Qing+and+Mao,+Weian+and+Wang,+Zhenhua+and+Chen,+Hao+and+Wu,+Lin+Yuanbo+and+Liu,+Yifan+and+Fan,+Chengxiang+and+Zhuge,+Yunzhi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={CTVIS}:+Consistent+Training+for+Online+Video+Instance+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/KainingYing/CTVIS" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Yinwei2023ICCVxxxarXiv.jpg"><b>Metric3D: towards zero-shot metric 3d prediction from a single image</b>   
<br />\(\cdot\) <i>W. Yin, C. Zhang, H. Chen, Z. Cai, G. Yu, K. Wang, X. Chen, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2307.10984" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Yinwei2023ICCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={Metric3D}:+Towards+Zero-shot+Metric+3D+Prediction+from+A+Single+Image+Yin,+Wei+and+Zhang,+Chi+and+Chen,+Hao+and+Cai,+Zhipeng+and+Yu,+Gang+and+Wang,+Kaixuan+and+Chen,+Xiaozhi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={Metric3D}:+Towards+Zero-shot+Metric+3D+Prediction+from+A+Single+Image" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Pose-free 3d scene reconstruction with frozen depth models</b>   
<br />\(\cdot\) <i>G. Xu, W. Yin, H. Chen, C. Shen, K. Cheng, F. Zhao</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="data/bibtex/XuG2023ICCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Pose-free+3D+Scene+Reconstruction+with+Frozen+Depth+Models+Xu,+Guangkai+and+Yin,+Wei+and+Chen,+Hao+and+Shen,+Chunhua+and+Cheng,+Kai+and+Zhao,+Feng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Pose-free+3D+Scene+Reconstruction+with+Frozen+Depth+Models" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/DiffuMask2023ICCVxxxarXiv.jpg"><b>Diffumask: synthesizing images with pixel-level annotations for semantic segmentation using diffusion models</b>   
<br />\(\cdot\) <i>W. Wu, Y. Zhao, M. Shou, H. Zhou, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2303.11681" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/DiffuMask2023ICCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Diffumask:+Synthesizing+images+with+pixel-level+annotations+for+semantic+segmentation+using+diffusion+models+Wu,+Weijia+and+Zhao,+Yuzhong+and+Shou,+Mike+Zheng+and+Zhou,+Hong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Diffumask:+Synthesizing+images+with+pixel-level+annotations+for+semantic+segmentation+using+diffusion+models" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/SegGPT2023ICCVxxxarXiv.jpg"><b>SegGPT: towards segmenting everything in context</b>   
<br />\(\cdot\) <i>X. Wang, X. Zhang, Y. Cao, W. Wang, C. Shen, T. Huang</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2304.03284" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/SegGPT2023ICCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SegGPT}:+Towards+Segmenting+Everything+In+Context+Wang,+Xinlong+and+Zhang,+Xiaosong+and+Cao,+Yue+and+Wang,+Wen+and+Shen,+Chunhua+and+Huang,+Tiejun" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SegGPT}:+Towards+Segmenting+Everything+In+Context" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Zolly: zoom focal length correctly for perspective-distorted human mesh reconstruction</b>   
<br />\(\cdot\) <i>W. Wang, Y. Ge, H. Mei, Z. Cai, Q. Sun, C. Shen, Y. Wang, L. Yang, T. Komura</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="data/bibtex/WangW2023ICCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Zolly:+Zoom+Focal+Length+Correctly+for+Perspective-Distorted+Human+Mesh+Reconstruction+Wang,+Wenjia+and+Ge,+Yongtao+and+Mei,+Haiyi+and+Cai,+Zhongang+and+Sun,+Qingping+and+Shen,+Chunhua+and+Wang,+Yanjun+and+Yang,+Lei+and+Komura,+Taku" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Zolly:+Zoom+Focal+Length+Correctly+for+Perspective-Distorted+Human+Mesh+Reconstruction" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CPE2023ICLRxxxarXiv.jpg"><b>Conditional positional encodings for vision transformers</b>   
<br />\(\cdot\) <i>X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, C. Shen</i>.
<br />\(\cdot\) <i>Proc. International Conference on Learning Representations (ICLR&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2102.10882" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CPE2023ICLR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Conditional+Positional+Encodings+for+Vision+Transformers+Chu,+Xiangxiang+and+Tian,+Zhi+and+Zhang,+Bo+and+Wang,+Xinlong+and+Wei,+Xiaolin+and+Xia,+Huaxia+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Conditional+Positional+Encodings+for+Vision+Transformers" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Deep weakly-supervised anomaly detection</b>   
<br />\(\cdot\) <i>G. Pang, C. Shen, H. Jin, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="data/bibtex/Pang2023KDD.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Weakly-supervised+Anomaly+Detection+Pang,+Guansong+and+Shen,+Chunhua+and+Jin,+Huidong+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Weakly-supervised+Anomaly+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Weijia2023DDMxxxarXiv.jpg"><b>DatasetDM: synthesizing data with perception annotations using diffusion models</b>   
<br />\(\cdot\) <i>W. Wu, Y. Zhao, H. Chen, Y. Gu, R. Zhao, Y. He, H. Zhou, M. Shou, C. Shen</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;23), 2023</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2308.06160" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Weijia2023DDM.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={DatasetDM}:+Synthesizing+Data+with+Perception+Annotations+Using+Diffusion+Models+Wu,+Weijia+and+Zhao,+Yuzhong+and+Chen,+Hao+and+Gu,+Yuchao+and+Zhao,+Rui+and+He,+Yefei+and+Zhou,+Hong+and+Shou,+Mike+Zheng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={DatasetDM}:+Synthesizing+Data+with+Perception+Annotations+Using+Diffusion+Models" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://weijiawu.github.io/DatasetDM_page/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
</ol>
<h1>2022</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><b>Effective eyebrow matting with domain adaptation</b>   
<br />\(\cdot\) <i>L. Wang, H. Zhang, Q. Xiao, H. Xu, C. Shen, X. Jin</i>.
<br />\(\cdot\) <i>Computer Graphics Forum (CGF), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Wang2022CGF.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Effective+Eyebrow+Matting+with+Domain+Adaptation+Wang,+Luyuan+and+Zhang,+Hanyuan+and+Xiao,+Qinjie+and+Xu,+Hao+and+Shen,+Chunhua+and+Jin,+Xiaogang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Effective+Eyebrow+Matting+with+Domain+Adaptation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhuang2022IJCVxxxarXiv.jpg"><b>Structured binary neural networks for image recognition</b>   
<br />\(\cdot\) <i>B. Zhuang, C. Shen, M. Tan, P. Chen, L. Liu, I. Reid</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2022</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1909.09934" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhuang2022IJCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Binary+Neural+Networks+for+Image+Recognition+Zhuang,+Bohan+and+Shen,+Chunhua+and+Tan,+Mingkui+and+Chen,+Peng+and+Liu,+Lingqiao+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+Binary+Neural+Networks+for+Image+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Arbitrarily shaped scene text detection with dynamic convolution</b>   
<br />\(\cdot\) <i>Y. Cai, Y. Liu, C. L. Jin, Y. Li, D. Ergu</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Cai2022PR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Arbitrarily+shaped+scene+text+detection+with+dynamic+convolution+Cai,+Ying+and+Liu,+Yuliang+and+ChunhuaShen+and+Jin,+Lianwen+and+Li,+Yidong+and+Ergu,+Daji" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Arbitrarily+shaped+scene+text+detection+with+dynamic+convolution" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>TSGB: target-selective gradient backprop for probing CNN visual saliency</b>   
<br />\(\cdot\) <i>L. Cheng, P. Fang, Y. Liang, L. Zhang, C. Shen, H. Wang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/TSGB2022TIP.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={TSGB}:+Target-selective+gradient+backprop+for+probing+{CNN}+visual+saliency+Cheng,+Lin+and+Fang,+Pengfei+and+Liang,+Yanjie+and+Zhang,+Liao+and+Shen,+Chunhua+and+Wang,+Hanzi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={TSGB}:+Target-selective+gradient+backprop+for+probing+{CNN}+visual+saliency" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Chi2022TPAMIxxxarXiv.jpg"><b>DeepEMD: differentiable earth mover's distance for few-shot learning</b>   
<br />\(\cdot\) <i>C. Zhang, Y. Cai, G. Lin, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2003.06777" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Chi2022TPAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={DeepEMD}:+Differentiable+Earth+Mover's+Distance+for+Few-Shot+Learning+Zhang,+Chi+and+Cai,+Yujun+and+Lin,+Guosheng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={DeepEMD}:+Differentiable+Earth+Mover's+Distance+for+Few-Shot+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Weiyin2022TPAMIxxxarXiv.jpg"><b>Towards accurate reconstruction of 3D scene shape from a single monocular image</b>   
<br />\(\cdot\) <i>W. Yin, J. Zhang, O. Wang, S. Niklaus, S. Chen, Y. Liu, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2208.13241" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Weiyin2022TPAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Accurate+Reconstruction+of+{3D}+Scene+Shape+from+A+Single+Monocular+Image+Yin,+Wei+and+Zhang,+Jianming+and+Wang,+Oliver+and+Niklaus,+Simon+and+Chen,+Simon+and+Liu,+Yifan+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+Accurate+Reconstruction+of+{3D}+Scene+Shape+from+A+Single+Monocular+Image" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/aim-uofa/depth/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CondInst2022TianxxxarXiv.jpg"><b>Instance and panoptic segmentation using conditional convolutions</b>   
<br />\(\cdot\) <i>Z. Tian, B. Zhang, H. Chen, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2102.03026" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CondInst2022Tian.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Instance+and+Panoptic+Segmentation+Using+Conditional+Convolutions+Tian,+Zhi+and+Zhang,+Bowen+and+Chen,+Hao+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Instance+and+Panoptic+Segmentation+Using+Conditional+Convolutions" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/aim-uofa/AdelaiDet/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Dynamic convolution for 3D point cloud instance segmentation</b>   
<br />\(\cdot\) <i>T. He, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2107.08392" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Tong2022TPAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Dynamic+Convolution+for+{3D}+Point+Cloud+Instance+Segmentation+He,+Tong+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Dynamic+Convolution+for+{3D}+Point+Cloud+Instance+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Improving monocular visual odometry using learned depth</b>   
<br />\(\cdot\) <i>L. Sun, W. Yin, E. Xie, Z. Li, C. Sun, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Robotics (TRO), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Sun2022TRO.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Improving+Monocular+Visual+Odometry+Using+Learned+Depth+Sun,+Libo+and+Yin,+Wei+and+Xie,+Enze+and+Li,+Zhengrong+and+Sun,+Changming+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Improving+Monocular+Visual+Odometry+Using+Learned+Depth" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>DenseCL: a simple framework for self-supervised dense visual pre-training</b>   
<br />\(\cdot\) <i>X. Wang, R. Zhang, C. Shen, T. Kong</i>.
<br />\(\cdot\) <i>Visual Informatics (VI), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Wang2022VI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={DenseCL}:+A+simple+framework+for+self-supervised+dense+visual+pre-training+Wang,+Xinlong+and+Zhang,+Rufeng+and+Shen,+Chunhua+and+Kong,+Tao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={DenseCL}:+A+simple+framework+for+self-supervised+dense+visual+pre-training" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><img class="imgP  right"   src="data/thumbnail/Peng2022MMxxxarXiv.jpg"><b>SPTS: single-point text spotting</b>   
<br />\(\cdot\) <i>D. Peng, X. Wang, Y. Liu, J. Zhang, M. Huang, S. Lai, S. Zhu, J. Li, D. Lin, C. Shen, X. Bai, L. Jin</i>.
<br />\(\cdot\) <i>Proc. ACM International Conference on Multimedia (ACMMM&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2112.07917" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Peng2022MM.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SPTS}:+Single-Point+Text+Spotting+Peng,+Dezhi+and+Wang,+Xinyu+and+Liu,+Yuliang+and+Zhang,+Jiaxin+and+Huang,+Mingxin+and+Lai,+Songxuan+and+Zhu,+Shenggao+and+Li,+Jing+and+Lin,+Dahua+and+Shen,+Chunhua+and+Bai,+Xiang+and+Jin,+Lianwen" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SPTS}:+Single-Point+Text+Spotting" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>TopFormer: token pyramid transformer for mobile semantic segmentation</b>   
<br />\(\cdot\) <i>W. Zhang, Z. Huang, G. Yu, T. Chen, G. Luo, X. Wang, W. Liu, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Topformer2022.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={TopFormer}:+Token+Pyramid+Transformer+for+Mobile+Semantic+Segmentation+Zhang,+Wenqiang+and+Huang,+Zilong+and+Yu,+Gang+and+Chen,+Tao+and+Luo,+Guozhong+and+Wang,+Xinggang+and+Liu,+Wenyu+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={TopFormer}:+Token+Pyramid+Transformer+for+Mobile+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2022SoloxxxarXiv.jpg"><b>FreeSOLO: learning to segment objects without annotations</b>   
<br />\(\cdot\) <i>X. Wang, Z. Yu, S. De Mello, J. Kautz, A. Anandkumar, C. Shen, J. Alvarez</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2202.12181" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wang2022Solo.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FreeSOLO}:+Learning+to+Segment+Objects+without+Annotations+Wang,+Xinlong+and+Yu,+Zhiding+and+{De+Mello},+Shalini+and+Kautz,+Jan+and+Anandkumar,+Anima+and+Shen,+Chunhua+and+Alvarez,+Jose" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={FreeSOLO}:+Learning+to+Segment+Objects+without+Annotations" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/AdelaiDet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Long2022RACxxxarXiv.jpg"><b>Retrieval augmented classification for long-tail visual recognition</b>   
<br />\(\cdot\) <i>A. Long, W. Yin, T. Ajanthan, V. Nguyen, P. Purkait, R. Garg, A. Blair, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2202.11233" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Long2022RAC.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Retrieval+Augmented+Classification+for+Long-Tail+Visual+Recognition+Long,+Alexander+and+Yin,+Wei+and+Ajanthan,+Thalaiyasingam+and+Nguyen,+Vu+and+Purkait,+Pulak+and+Garg,+Ravi+and+Blair,+Alan+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Retrieval+Augmented+Classification+for+Long-Tail+Visual+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>RigidFlow: self-supervised scene flow learning on point clouds by local rigidity prior</b>   
<br />\(\cdot\) <i>R. Li, C. Zhang, G. Lin, Z. Wang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Li2022Rigid.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={RigidFlow}:+Self-Supervised+Scene+Flow+Learning+on+Point+Clouds+by+Local+Rigidity+Prior+Li,+Ruibo+and+Zhang,+Chi+and+Lin,+Guosheng+and+Wang,+Zhe+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={RigidFlow}:+Self-Supervised+Scene+Flow+Learning+on+Point+Clouds+by+Local+Rigidity+Prior" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Catching both gray and black swans: open-set supervised anomaly detection</b>   
<br />\(\cdot\) <i>C. Ding, G. Pan, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Ding2022Catching.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Catching+Both+Gray+and+Black+Swans:+Open-set+Supervised+Anomaly+Detection+Ding,+Choubo+and+Pan,+Guansong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Catching+Both+Gray+and+Black+Swans:+Open-set+Supervised+Anomaly+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Dai2022MattingxxxarXiv.jpg"><b>Boosting robustness of image matting with context assembling and strong data augmentation</b>   
<br />\(\cdot\) <i>Y. Dai, B. Price, H. Zhang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2201.06889" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Dai2022Matting.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Boosting+Robustness+of+Image+Matting+with+Context+Assembling+and+Strong+Data+Augmentation+Dai,+Yutong+and+Price,+Brian+and+Zhang,+He+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Boosting+Robustness+of+Image+Matting+with+Context+Assembling+and+Strong+Data+Augmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Poseur2022DetectionxxxarXiv.jpg"><b>Poseur: direct human pose regression with transformers</b>   
<br />\(\cdot\) <i>W. Mao, Y. Ge, C. Shen, Z. Tian, X. Wang, Z. Wang, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2201.07412" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Poseur2022Detection.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={Poseur}:+Direct+Human+Pose+Regression+with+Transformers+Mao,+Weian+and+Ge,+Yongtao+and+Shen,+Chunhua+and+Tian,+Zhi+and+Wang,+Xinlong+and+Wang,+Zhibin+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={Poseur}:+Direct+Human+Pose+Regression+with+Transformers" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/aim-uofa/Poseur" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Tong2022DetectionxxxarXiv.jpg"><b>PointInst3D: segmenting 3D instances by points</b>   
<br />\(\cdot\) <i>T. He, W. Yin, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2204.11402" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Tong2022Detection.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={PointInst3D}:+Segmenting+{3D}+Instances+by+Points+He,+Tong+and+Yin,+Wei+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={PointInst3D}:+Segmenting+{3D}+Instances+by+Points" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>DisCo: remedying self-supervised learning on lightweight models with distilled contrastive learning</b>   
<br />\(\cdot\) <i>Y. Gao, J. Zhuang, S. Lin, H. Cheng, X. Sun, K. Li, C. Shen</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Gao2022DisCO.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={DisCo}:+Remedying+Self-supervised+Learning+on+Lightweight+Models+with+Distilled+Contrastive+Learning+Gao,+Yuting+and+Zhuang,+Jia-Xin+and+Lin,+Shaohui+and+Cheng,+Hao+and+Sun,+Xing+and+Li,+Ke+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={DisCo}:+Remedying+Self-supervised+Learning+on+Lightweight+Models+with+Distilled+Contrastive+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/Yuting-Gao/DisCo-pytorch" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Peixian2022DetectionxxxarXiv.jpg"><b>Efficient decoder-free object detection with transformers</b>   
<br />\(\cdot\) <i>P. Chen, M. Zhang, Y. Shen, K. Sheng, Y. Gao, X. Sun, K. Li, C. Shen</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2206.06829" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Peixian2022Detection.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Decoder-free+Object+Detection+with+Transformers+Chen,+Peixian+and+Zhang,+Mengdan+and+Shen,+Yunhang+and+Sheng,+Kekai+and+Gao,+Yuting+and+Sun,+Xing+and+Li,+Ke+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+Decoder-free+Object+Detection+with+Transformers" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>DENSE: data-free one-shot federated learning</b>   
<br />\(\cdot\) <i>J. Zhang, C. Chen, B. Li, L. Lyu, S. Wu, S. Ding, C. Shen, C. Wu</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhang2022DENSE.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={DENSE}:+Data-Free+One-Shot+Federated+Learning+Zhang,+Jie+and+Chen,+Chen+and+Li,+Bo+and+Lyu,+Lingjuan+and+Wu,+Shuang+and+Ding,+Shouhong+and+Shen,+Chunhua+and+Wu,+Chao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={DENSE}:+Data-Free+One-Shot+Federated+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Hierarchical normalization for robust monocular depth estimation</b>   
<br />\(\cdot\) <i>C. Zhang, W. Yin, Z. Wang, G. Yu, B. Fu, C. Shen</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Chi2022Depth.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Hierarchical+Normalization+for+Robust+Monocular+Depth+Estimation+Zhang,+Chi+and+Yin,+Wei+and+Wang,+Zhibin+and+Yu,+Gang+and+Fu,+Bin+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Hierarchical+Normalization+for+Robust+Monocular+Depth+Estimation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>SegViT: semantic segmentation with plain vision transformers</b>   
<br />\(\cdot\) <i>B. Zhang, Z. Tian, Q. Tang, X. Chu, X. Wei, C. Shen, Y. Liu</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhang2022ViTb.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SegViT}:+Semantic+Segmentation+with+Plain+Vision+Transformers+Zhang,+Bowen+and+Tian,+Zhi+and+Tang,+Quan+and+Chu,+Xiangxiang+and+Wei,+Xiaolin+and+Shen,+Chunhua+and+Liu,+Yifan" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SegViT}:+Semantic+Segmentation+with+Plain+Vision+Transformers" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Fully convolutional one-stage 3D object detection on LiDAR range images</b>   
<br />\(\cdot\) <i>Z. Tian, X. Chu, X. Wang, X. Wei, C. Shen</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Tian2022FCOSLidar.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fully+Convolutional+One-Stage+{3D}+Object+Detection+on+{LiDAR}+Range+Images+Tian,+Zhi+and+Chu,+Xiangxiang+and+Wang,+Xiaoming+and+Wei,+Xiaolin+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fully+Convolutional+One-Stage+{3D}+Object+Detection+on+{LiDAR}+Range+Images" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Text-adaptive multiple visual prototype matching for video-text retrieval</b>   
<br />\(\cdot\) <i>C. Lin, A. Wu, J. Liang, J. Zhang, W. Ge, W. Zheng, C. Shen</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Lin2022TVRc.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Text-Adaptive+Multiple+Visual+Prototype+Matching+for+Video-Text+Retrieval+Lin,+Chengzhi+and+Wu,+Ancong+and+Liang,+Junwei+and+Zhang,+Jun+and+Ge,+Wenhang+and+Zheng,+Wei-Shi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Text-Adaptive+Multiple+Visual+Prototype+Matching+for+Video-Text+Retrieval" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Multi-dataset training of transformers for robust action recognition</b>   
<br />\(\cdot\) <i>J. Liang, E. Zhang, J. Zhang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Liang2022Action.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-dataset+Training+of+Transformers+for+Robust+Action+Recognition+Liang,+Junwei+and+Zhang,+Enwei+and+Zhang,+Jun+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Multi-dataset+Training+of+Transformers+for+Robust+Action+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Adv-attribute: inconspicuous and transferable adversarial attack on face recognition</b>   
<br />\(\cdot\) <i>S. Jia, B. Yin, T. Yao, S. Ding, C. Shen, X. Yang, C. Ma</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Jia2022AdvA.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adv-Attribute:+Inconspicuous+and+Transferable+Adversarial+Attack+on+Face+Recognition+Jia,+Shuai+and+Yin,+Bangjie+and+Yao,+Taiping+and+Ding,+Shouhong+and+Shen,+Chunhua+and+Yang,+Xiaokang+and+Ma,+Chao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Adv-Attribute:+Inconspicuous+and+Transferable+Adversarial+Attack+on+Face+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>PyramidCLIP: hierarchical feature alignment for vision-language model pretraining</b>   
<br />\(\cdot\) <i>Y. Gao, J. Liu, Z. Xu, J. Zhang, K. Li, R. Ji, C. Shen</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;22), 2022</i>.
<br />\(\cdot\) <a href="data/bibtex/Gao2022CLIP.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={PyramidCLIP}:+Hierarchical+Feature+Alignment+for+Vision-language+Model+Pretraining+Gao,+Yuting+and+Liu,+Jinfeng+and+Xu,+Zihan+and+Zhang,+Jun+and+Li,+Ke+and+Ji,+Rongrong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={PyramidCLIP}:+Hierarchical+Feature+Alignment+for+Vision-language+Model+Pretraining" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h1>2021</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><img class="imgP  right"   src="data/thumbnail/Haokui2021NASxxxarXiv.jpg"><b>Memory-efficient hierarchical neural architecture search for image restoration</b>   
<br />\(\cdot\) <i>H. Zhang, Y. Li, H. Chen, C. Gong, Z. Bai, C. Shen</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2012.13212" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Haokui2021NAS.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Memory-Efficient+Hierarchical+Neural+Architecture+Search+for+Image+Restoration+Zhang,+Haokui+and+Li,+Ying+and+Chen,+Hao+and+Gong,+Chengrong+and+Bai,+Zongwen+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Memory-Efficient+Hierarchical+Neural+Architecture+Search+for+Image+Restoration" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/hkzhang91/HiNAS" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Yu2021BiSegV2xxxarXiv.jpg"><b>BiSeNet v2: bilateral network with guided aggregation for real-time semantic segmentation</b>   
<br />\(\cdot\) <i>C. Yu, C. Gao, J. Wang, G. Yu, C. Shen, N. Sang</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2004.02147" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Yu2021BiSegV2.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={BiSeNet}+v2:+Bilateral+Network+with+Guided+Aggregation+for+Real-time+Semantic+Segmentation+Yu,+Changqian+and+Gao,+Changxin+and+Wang,+Jingbo+and+Yu,+Gang+and+Shen,+Chunhua+and+Sang,+Nong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={BiSeNet}+v2:+Bilateral+Network+with+Guided+Aggregation+for+Real-time+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>A dual-attention-guided network for ghost-free high dynamic range imaging</b>   
<br />\(\cdot\) <i>Q. Yan, D. Gong, Q. Shi, A. van den Hengel, C. Shen, I. Reid, Y. Zhang</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Yan2021Ghostfree.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Dual-Attention-guided+network+for+ghost-free+high+dynamic+range+imaging+Yan,+Qingsen+and+Gong,+Dong+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Reid,+Ian+and+Zhang,+Yanning" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Dual-Attention-guided+network+for+ghost-free+high+dynamic+range+imaging" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/qingsenyangit/AHDRNet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>NAS-FCOS: efficient search for object detection architectures</b>   
<br />\(\cdot\) <i>N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, Y. Zhang</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Wang2021IJCV_NAS.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={NAS-FCOS}:+Efficient+Search+for+Object+Detection+Architectures+Wang,+Ning+and+Gao,+Yang+and+Chen,+Hao+and+Wang,+Peng+and+Tian,+Zhi+and+Shen,+Chunhua+and+Zhang,+Yanning" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={NAS-FCOS}:+Efficient+Search+for+Object+Detection+Architectures" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/Lausannen/NAS-FCOS" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/IJCV2021LiuylxxxarXiv.jpg"><b>Exploring the capacity of an orderless box discretization network for multi-orientation scene text detection</b>   
<br />\(\cdot\) <i>Y. Liu, T. He, H. Chen, X. Wang, C. Luo, S. Zhang, C. Shen, L. Jin</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1912.09629" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/IJCV2021Liuyl.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploring+the+Capacity+of+an+Orderless+Box+Discretization+Network+for+Multi-orientation+Scene+Text+Detection+Liu,+Yuliang+and+He,+Tong+and+Chen,+Hao+and+Wang,+Xinyu+and+Luo,+Canjie+and+Zhang,+Shuaitao+and+Shen,+Chunhua+and+Jin,+Lianwen" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Exploring+the+Capacity+of+an+Orderless+Box+Discretization+Network+for+Multi-orientation+Scene+Text+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/TextDet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Joint classification and regression for visual tracking with fully convolutional Siamese networks</b>   
<br />\(\cdot\) <i>Y. Cui, D. Guo, Y. Shao, Z. Wang, C. Shen, L. Zhang, S. Chen</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Cui2021Joint.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Joint+classification+and+regression+for+visual+tracking+with+fully+convolutional+{S}iamese+networks+Cui,+Ying+and+Guo,+Dongyan+and+Shao,+Yanyan+and+Wang,+Zhenhua+and+Shen,+Chunhua+and+Zhang,+Liyan+and+Chen,+Shengyong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Joint+classification+and+regression+for+visual+tracking+with+fully+convolutional+{S}iamese+networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2105.11610.pdf"><img class="imgP  right"   src="data/thumbnail/Bian2021IJCVxxxarXiv.jpg"></a><b>Unsupervised scale-consistent depth learning from video</b>   
<br />\(\cdot\) <i>J. Bian, H. Zhan, N. Wang, Z. Li, L. Zhang, C. Shen, M. Cheng, I. Reid</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2105.11610" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Bian2021IJCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Scale-consistent+Depth+Learning+from+Video+Bian,+Jia-Wang+and+Zhan,+Huangying+and+Wang,+Naiyan+and+Li,+Zhichao+and+Zhang,+Le+and+Shen,+Chunhua+and+Cheng,+Ming-Ming+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Scale-consistent+Depth+Learning+from+Video" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/JiawangBian/SC-SfMLearner-Release" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Learning discriminative region representation for person retrieval</b>   
<br />\(\cdot\) <i>Y. Zhao, X. Yu, Y. Gao, C. Shen</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhao2021PRLearning.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Discriminative+Region+Representation+for+Person+Retrieval+Zhao,+Yang+and+Yu,+Xiaohan+and+Gao,+Yongsheng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Discriminative+Region+Representation+for+Person+Retrieval" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Learning deep part-aware embedding for person retrieval</b>   
<br />\(\cdot\) <i>Y. Zhao, C. Shen, X. Yu, H. Chen, Y. Gao, S. Xiong</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhao2021PR1.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Deep+Part-Aware+Embedding+for+Person+Retrieval+Zhao,+Yang+and+Shen,+Chunhua+and+Yu,+Xiaohan+and+Chen,+Hao+and+Gao,+Yongsheng+and+Xiong,+Shengwu" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Deep+Part-Aware+Embedding+for+Person+Retrieval" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>An adversarial human pose estimation network injected with graph structure</b>   
<br />\(\cdot\) <i>L. Tian, P. Wang, G. Liang, C. Shen</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Tian2021Adversarial.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+Adversarial+Human+Pose+Estimation+Network+Injected+with+Graph+Structure+Tian,+Lei+and+Wang,+Peng+and+Liang,+Guoqiang+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=An+Adversarial+Human+Pose+Estimation+Network+Injected+with+Graph+Structure" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Intra- and inter-pair consistency for semi-supervised gland segmentation</b>   
<br />\(\cdot\) <i>Y. Xie, J. Zhang, Z. Liao, J. Verjans, C. Shen, Y. Xia</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Xie2021Intra.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Intra-+and+Inter-pair+Consistency+for+Semi-supervised+Gland+Segmentation+Xie,+Yutong+and+Zhang,+Jianpeng+and+Liao,+Zhibin+and+Verjans,+Johan+and+Shen,+Chunhua+and+Xia,+Yong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Intra-+and+Inter-pair+Consistency+for+Semi-supervised+Gland+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhuang2021QuantizationxxxarXiv.jpg"><b>Effective training of convolutional neural networks with low-bitwidth weights and activations</b>   
<br />\(\cdot\) <i>B. Zhuang, J. Liu, M. Tan, L. Liu, I. Reid, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1908.04680" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhuang2021Quantization.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Effective+Training+of+Convolutional+Neural+Networks+with+Low-bitwidth+Weights+and+Activations+Zhuang,+Bohan+and+Liu,+Jing+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Effective+Training+of+Convolutional+Neural+Networks+with+Low-bitwidth+Weights+and+Activations" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Yin2021PAMIvnxxxarXiv.jpg"><b>Virtual normal: enforcing geometric constraints for accurate and robust depth prediction</b>   
<br />\(\cdot\) <i>W. Yin, Y. Liu, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2103.04216" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Yin2021PAMIvn.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Virtual+Normal:+Enforcing+Geometric+Constraints+for+Accurate+and+Robust+Depth+Prediction+Yin,+Wei+and+Liu,+Yifan+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Virtual+Normal:+Enforcing+Geometric+Constraints+for+Accurate+and+Robust+Depth+Prediction" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/Depth" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/WXL2021SOLOxxxarXiv.jpg"><b>SOLO: a simple framework for instance segmentation</b>   
<br />\(\cdot\) <i>X. Wang, R. Zhang, C. Shen, T. Kong, L. Li</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2106.15947" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/WXL2021SOLO.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SOLO}:+A+Simple+Framework+for+Instance+Segmentation+Wang,+Xinlong+and+Zhang,+Rufeng+and+Shen,+Chunhua+and+Kong,+Tao+and+Li,+Lei" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SOLO}:+A+Simple+Framework+for+Instance+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/AdelaiDet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2021PANplusxxxarXiv.jpg"><b>PAN++: towards efficient and accurate end-to-end spotting of arbitrarily-shaped text</b>   
<br />\(\cdot\) <i>W. Wang, E. Xie, X. Li, X. Liu, D. Liang, Z. Yang, T. Lu, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2105.00405" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wang2021PANplus.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={PAN++}:+Towards+Efficient+and+Accurate+End-to-End+Spotting+of+Arbitrarily-Shaped+Text+Wang,+Wenhai+and+Xie,+Enze+and+Li,+Xiang+and+Liu,+Xuebo+and+Liang,+Ding+and+Yang,+Zhibo+and+Lu,+Tong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={PAN++}:+Towards+Efficient+and+Accurate+End-to-End+Spotting+of+Arbitrarily-Shaped+Text" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Li2021TextxxxarXiv.jpg"><b>Towards end-to-end text spotting in natural scenes</b>   
<br />\(\cdot\) <i>P. Wang, H. Li, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1906.06013" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Li2021Text.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+End-to-End+Text+Spotting+in+Natural+Scenes+Wang,+Peng+and+Li,+Hui+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+End-to-End+Text+Spotting+in+Natural+Scenes" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2021ABCNetv2xxxarXiv.jpg"><b>ABCNet v2: adaptive bezier-curve network for real-time end-to-end text spotting</b>   
<br />\(\cdot\) <i>Y. Liu, C. Shen, L. Jin, T. He, P. Chen, C. Liu, H. Chen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2105.03620" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Liu2021ABCNetv2.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={ABCNet}+v2:+Adaptive+Bezier-Curve+Network+for+Real-time+End-to-end+Text+Spotting+Liu,+Yuliang+and+Shen,+Chunhua+and+Jin,+Lianwen+and+He,+Tong+and+Chen,+Peng+and+Liu,+Chongyu+and+Chen,+Hao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={ABCNet}+v2:+Adaptive+Bezier-Curve+Network+for+Real-time+End-to-end+Text+Spotting" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/AdelaiDet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Auto-rectify network for unsupervised indoor depth estimation</b>   
<br />\(\cdot\) <i>J. Bian, H. Zhan, N. Wang, T. Chin, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Autorectify2021Bian.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Auto-Rectify+Network+for+Unsupervised+Indoor+Depth+Estimation+Bian,+Jia-Wang+and+Zhan,+Huangying+and+Wang,+Naiyan+and+Chin,+Tat-Jun+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Auto-Rectify+Network+for+Unsupervised+Indoor+Depth+Estimation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>Diverse knowledge distillation for end-to-end person search</b>   
<br />\(\cdot\) <i>X. Zhang, X. Wang, J. Bian, C. Shen, M. You</i>.
<br />\(\cdot\) <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/ZhangPerson2021AAAI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Diverse+Knowledge+Distillation+for+End-to-end+Person+Search+Zhang,+Xinyu+and+Wang,+Xinlong+and+Bian,+Jia-Wang+and+Shen,+Chunhua+and+You,+Minyu" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Diverse+Knowledge+Distillation+for+End-to-end+Person+Search" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>SA-BNN: state-aware binary neural network</b>   
<br />\(\cdot\) <i>C. Liu, P. Chen, B. Zhuang, C. Shen, B. Zhang, W. Ding</i>.
<br />\(\cdot\) <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/LiuBNN2021AAAI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SA-BNN}:+State-Aware+Binary+Neural+Network+Liu,+Chunlei+and+Chen,+Peng+and+Zhuang,+Bohan+and+Shen,+Chunhua+and+Zhang,+Baochang+and+Ding,+Wenrui" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SA-BNN}:+State-Aware+Binary+Neural+Network" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Deep reasoning network for few-shot semantic segmentation</b>   
<br />\(\cdot\) <i>Y. Zhuge, C. Shen</i>.
<br />\(\cdot\) <i>Proc. ACM International Conference on Multimedia (ACMMM&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/MM2021B.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Reasoning+Network+for+Few-shot+Semantic+Segmentation+Zhuge,+Yuzhi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Reasoning+Network+for+Few-shot+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/MM2021AxxxarXiv.jpg"><b>Fully quantized image super-resolution networks</b>   
<br />\(\cdot\) <i>H. Wang, P. Chen, B. Zhuang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. ACM International Conference on Multimedia (ACMMM&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2011.14265" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/MM2021A.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fully+Quantized+Image+Super-Resolution+Networks+Wang,+Hu+and+Chen,+Peng+and+Zhuang,+Bohan+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fully+Quantized+Image+Super-Resolution+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhang2021CVPR1xxxarXiv.jpg"><b>DoDNet: learning to segment multi-organ and tumors from multiple partially labeled datasets</b>   
<br />\(\cdot\) <i>J. Zhang, Y. Xie, Y. Xia, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2011.10217" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhang2021CVPR1.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={DoDNet}:+Learning+to+segment+multi-organ+and+tumors+from+multiple+partially+labeled+datasets+Zhang,+Jianpeng+and+Xie,+Yutong+and+Xia,+Yong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={DoDNet}:+Learning+to+segment+multi-organ+and+tumors+from+multiple+partially+labeled+datasets" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/aim-uofa/partially-labelled" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Yin2021CVPR6xxxarXiv.jpg"><b>Learning to recover 3D scene shape from a single image</b>   
<br />\(\cdot\) <i>W. Yin, J. Zhang, O. Wang, S. Niklaus, L. Mai, S. Chen, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2012.09365" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Yin2021CVPR6.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+to+Recover+{3D}+Scene+Shape+from+a+Single+Image+Yin,+Wei+and+Zhang,+Jianming+and+Wang,+Oliver+and+Niklaus,+Simon+and+Mai,+Long+and+Chen,+Simon+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+to+Recover+{3D}+Scene+Shape+from+a+Single+Image" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/Depth" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p>Listed as one of the Best Paper Candidates, 32 out of about 6000 submissions.
</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2021CVPR11xxxarXiv.jpg"><b>End-to-end video instance segmentation with Transformers</b>   
<br />\(\cdot\) <i>Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, H. Xia</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2011.14503" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wang2021CVPR11.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=End-to-End+Video+Instance+Segmentation+with+{T}ransformers+Wang,+Yuqing+and+Xu,+Zhaoliang+and+Wang,+Xinlong+and+Shen,+Chunhua+and+Cheng,+Baoshan+and+Shen,+Hao+and+Xia,+Huaxia" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=End-to-End+Video+Instance+Segmentation+with+{T}ransformers" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2021CVPR13xxxarXiv.jpg"><b>Dense contrastive learning for self-supervised visual pre-training</b>   
<br />\(\cdot\) <i>X. Wang, R. Zhang, C. Shen, T. Kong, L. Li</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2011.09157" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wang2021CVPR13.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Dense+Contrastive+Learning+for+Self-Supervised+Visual+Pre-Training+Wang,+Xinlong+and+Zhang,+Rufeng+and+Shen,+Chunhua+and+Kong,+Tao+and+Li,+Lei" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Dense+Contrastive+Learning+for+Self-Supervised+Visual+Pre-Training" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/AdelaiDet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Tian2021CVPR2xxxarXiv.jpg"><b>BoxInst: high-performance instance segmentation with box annotations</b>   
<br />\(\cdot\) <i>Z. Tian, C. Shen, X. Wang, H. Chen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2012.02310" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Tian2021CVPR2.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={BoxInst}:+High-Performance+Instance+Segmentation+with+Box+Annotations+Tian,+Zhi+and+Shen,+Chunhua+and+Wang,+Xinlong+and+Chen,+Hao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={BoxInst}:+High-Performance+Instance+Segmentation+with+Box+Annotations" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/AdelaiDet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Learning spatial-semantic relationship for facial attribute recognition with limited labeled data</b>   
<br />\(\cdot\) <i>Y. Shu, Y. Yan, S. Chen, J. Xue, C. Shen, H. Wang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Shu2021CVPR10.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Spatial-Semantic+Relationship+for+Facial+Attribute+Recognition+with+Limited+Labeled+Data+Shu,+Ying+and+Yan,+Yan+and+Chen,+Si+and+Xue,+Jing-Hao+and+Shen,+Chunhua+and+Wang,+Hanzi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Spatial-Semantic+Relationship+for+Facial+Attribute+Recognition+with+Limited+Labeled+Data" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Feature decomposition and reconstruction learning for effective facial expression recognition</b>   
<br />\(\cdot\) <i>D. Ruan, Y. Yan, S. Lai, Z. Chai, C. Shen, H. Wang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Ruan2021CVPR9.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Feature+Decomposition+and+Reconstruction+Learning+for+Effective+Facial+Expression+Recognition+Ruan,+Delian+and+Yan,+Yan+and+Lai,+Shenqi+and+Chai,+Zhenhua+and+Shen,+Chunhua+and+Wang,+Hanzi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Feature+Decomposition+and+Reconstruction+Learning+for+Effective+Facial+Expression+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>FCPose: fully convolutional multi-person pose estimation with dynamic instance-aware convolutions</b>   
<br />\(\cdot\) <i>W. Mao, Z. Tian, X. Wang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Mao2021CVPR4.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FCPose}:+Fully+Convolutional+Multi-Person+Pose+Estimation+with+Dynamic+Instance-Aware+Convolutions+Mao,+Weian+and+Tian,+Zhi+and+Wang,+Xinlong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={FCPose}:+Fully+Convolutional+Multi-Person+Pose+Estimation+with+Dynamic+Instance-Aware+Convolutions" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/AdelaiDet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Generic perceptual loss for modelling structured output dependencies</b>   
<br />\(\cdot\) <i>Y. Liu, W. Yin, Y. Chen, H. Chen, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Liu2021CVPR5.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Generic+Perceptual+Loss+for+Modelling+Structured+Output+Dependencies+Liu,+Yifan+and+Yin,+Wei+and+Chen,+Yu+and+Chen,+Hao+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Generic+Perceptual+Loss+for+Modelling+Structured+Output+Dependencies" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>HCRF-Flow: scene flow from point clouds with continuous high-order CRFs and position-aware flow embedding</b>   
<br />\(\cdot\) <i>R. Li, G. Lin, T. He, F. Liu, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Li2021CVPR12.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={HCRF-Flow}:+Scene+Flow+from+Point+Clouds+with+Continuous+High-order+{CRFs}+and+Position-aware+Flow+Embedding+Li,+Ruibo+and+Lin,+Guosheng+and+He,+Tong+and+Liu,+Fayao+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={HCRF-Flow}:+Scene+Flow+from+Point+Clouds+with+Continuous+High-order+{CRFs}+and+Position-aware+Flow+Embedding" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/He2021CVPR7xxxarXiv.jpg"><b>DyCo3D: robust instance segmentation of 3d point clouds through dynamic convolution</b>   
<br />\(\cdot\) <i>T. He, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2011.13328" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/He2021CVPR7.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={DyCo3D}:+Robust+Instance+Segmentation+of+3D+Point+Clouds+through+Dynamic+Convolution+He,+Tong+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={DyCo3D}:+Robust+Instance+Segmentation+of+3D+Point+Clouds+through+Dynamic+Convolution" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/DyCo3D" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2011.11204.pdf"><img class="imgP  right"   src="data/thumbnail/Guo2021CVPR14xxxarXiv.jpg"></a><b>Graph attention tracking</b>   
<br />\(\cdot\) <i>D. Guo, Y. Shao, Y. Cui, Z. Wang, L. Zhang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2011.11204" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Guo2021CVPR14.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Graph+Attention+Tracking+Guo,+Dongyan+and+Shao,+Yanyan+and+Cui,+Ying+and+Wang,+Zhenhua+and+Zhang,+Liyan+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Graph+Attention+Tracking" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/SiamGAT" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2011.14288.pdf"><img class="imgP  right"   src="data/thumbnail/Dai2021CVPR8xxxarXiv.jpg"></a><b>Learning affinity-aware upsampling for deep image matting</b>   
<br />\(\cdot\) <i>Y. Dai, H. Lu, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2011.14288" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Dai2021CVPR8.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Affinity-Aware+Upsampling+for+Deep+Image+Matting+Dai,+Yutong+and+Lu,+Hao+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Affinity-Aware+Upsampling+for+Deep+Image+Matting" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2007.06919.pdf"><img class="imgP  right"   src="data/thumbnail/Chen2021CVPR3xxxarXiv.jpg"></a><b>AQD: towards accurate quantized object detection</b>   
<br />\(\cdot\) <i>P. Chen, J. Liu, B. Zhuang, M. Tan, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2007.06919" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Chen2021CVPR3.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={AQD}:+Towards+Accurate+Quantized+Object+Detection+Chen,+Peng+and+Liu,+Jing+and+Zhuang,+Bohan+and+Tan,+Mingkui+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={AQD}:+Towards+Accurate+Quantized+Object+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Chizhang2021ICCVMetaxxxarXiv.jpg"><b>Meta navigator: search for a good adaptation policy for few-shot learning</b>   
<br />\(\cdot\) <i>C. Zhang, H. Ding, G. Lin, R. Li, C. Wang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2109.05749" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Chizhang2021ICCVMeta.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Meta+Navigator:+Search+for+a+Good+Adaptation+Policy+for+Few-shot+Learning+Zhang,+Chi+and+Ding,+Henghui+and+Lin,+Guosheng+and+Li,+Ruibo+and+Wang,+Changhu+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Meta+Navigator:+Search+for+a+Good+Adaptation+Policy+for+Few-shot+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Yuan2021ICCVSimplexxxarXiv.jpg"><b>A simple baseline for semi-supervised semantic segmentation with strong data augmentation</b>   
<br />\(\cdot\) <i>J. Yuan, Y. Liu, C. Shen, Z. Wang, H. Li</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2104.07256" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Yuan2021ICCVSimple.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Simple+Baseline+for+Semi-supervised+Semantic+Segmentation+with+Strong+Data+Augmentation+Yuan,+Jianlong+and+Liu,+Yifan+and+Shen,+Chunhua+and+Wang,+Zhibin+and+Li,+Hao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Simple+Baseline+for+Semi-supervised+Semantic+Segmentation+with+Strong+Data+Augmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>BV-Person: a large-scale dataset for bird-view person re-identification</b>   
<br />\(\cdot\) <i>C. Yan, G. Pang, L. Wang, J. Jiao, X. Feng, C. Shen, J. Li</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Yan2021ICCVBVPerson.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={BV-Person}:+A+Large-scale+Dataset+for+Bird-view+Person+Re-identification+Yan,+Cheng+and+Pang,+Guansong+and+Wang,+Lei+and+Jiao,+Jile+and+Feng,+Xuetao+and+Shen,+Chunhua+and+Li,+Jingjing" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={BV-Person}:+A+Large-scale+Dataset+for+Bird-view+Person+Re-identification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Occluded person re-identification with single-scale global representations</b>   
<br />\(\cdot\) <i>C. Yan, G. Pang, J. Jiao, X. Bai, X. Feng, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/Yan2021ICCVOccl.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Occluded+Person+Re-Identification+with+Single-scale+Global+Representations+Yan,+Cheng+and+Pang,+Guansong+and+Jiao,+Jile+and+Bai,+Xiao+and+Feng,+Xuetao+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Occluded+Person+Re-Identification+with+Single-scale+Global+Representations" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shu2021ICCVKDxxxarXiv.jpg"><b>Channel-wise knowledge distillation for dense prediction</b>   
<br />\(\cdot\) <i>C. Shu, Y. Liu, J. Gao, L. Xu, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2011.13256" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Shu2021ICCVKD.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Channel-wise+Knowledge+Distillation+for+Dense+Prediction+Shu,+Changyong+and+Liu,+Yifan+and+Gao,+Jianfei+and+Xu,+Lin+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Channel-wise+Knowledge+Distillation+for+Dense+Prediction" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2008.05101.pdf"><img class="imgP  right"   src="data/thumbnail/Chen2021ICCVxxxarXiv.jpg"></a><b>FATNN: fast and accurate ternary neural networks</b>   
<br />\(\cdot\) <i>P. Chen, B. Zhuang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2008.05101" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Chen2021ICCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FATNN}:+Fast+and+Accurate+Ternary+Neural+Networks+Chen,+Peng+and+Zhuang,+Bohan+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={FATNN}:+Fast+and+Accurate+Ternary+Neural+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Kong2021ICRAxxxarXiv.jpg"><b>FastFlowNet: a lightweight network for fast optical flow estimation</b>   
<br />\(\cdot\) <i>L. Kong, C. Shen, J. Yang</i>.
<br />\(\cdot\) <i>Proc. International Conference on Robotics and Automation (ICRA&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2103.04524" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Kong2021ICRA.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FastFlowNet}:+A+Lightweight+Network+for+Fast+Optical+Flow+Estimation+Kong,+Lingtong+and+Shen,+Chunhua+and+Yang,+Jie" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={FastFlowNet}:+A+Lightweight+Network+for+Fast+Optical+Flow+Estimation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/fastflow" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Pang2021KDDxxxarXiv.jpg"><b>Toward deep supervised anomaly detection: reinforcement learning from partially labeled anomaly data</b>   
<br />\(\cdot\) <i>G. Pang, A. van den Hengel, C. Shen, L. Cao</i>.
<br />\(\cdot\) <i>Proc. ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2009.06847" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Pang2021KDD.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Toward+Deep+Supervised+Anomaly+Detection:+Reinforcement+Learning+from+Partially+Labeled+Anomaly+Data+Pang,+Guansong+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Cao,+Longbing" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Toward+Deep+Supervised+Anomaly+Detection:+Reinforcement+Learning+from+Partially+Labeled+Anomaly+Data" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>CoTr: efficient 3D medical image segmentation by bridging CNN and transformer</b>   
<br />\(\cdot\) <i>Y. Xie, J. Zhang, C. Shen, Y. Xia</i>.
<br />\(\cdot\) <i>Proc. International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="data/bibtex/YXie2021MICCAI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={CoTr}:+Efficient+{3D}+Medical+Image+Segmentation+by+bridging+{CNN}+and+Transformer+Xie,+Yutong+and+Zhang,+Jianpeng+and+Shen,+Chunhua+and+Xia,+Yong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={CoTr}:+Efficient+{3D}+Medical+Image+Segmentation+by+bridging+{CNN}+and+Transformer" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/YtongXie/CoTr" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/DNRD2021ZhangxxxarXiv.jpg"><b>Dynamic neural representational decoders for high-resolution semantic segmentation</b>   
<br />\(\cdot\) <i>B. Zhang, Y. Liu, Z. Tian, C. Shen</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2107.14428" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/DNRD2021Zhang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Dynamic+Neural+Representational+Decoders+for+High-Resolution+Semantic+Segmentation+Zhang,+Bowen+and+Liu,+Yifan+and+Tian,+Zhi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Dynamic+Neural+Representational+Decoders+for+High-Resolution+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Twins2021ChuxxxarXiv.jpg"><b>Twins: revisiting the design of spatial attention in vision transformers</b>   
<br />\(\cdot\) <i>X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, C. Shen</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;21), 2021</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2104.13840" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Twins2021Chu.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Twins:+Revisiting+the+Design+of+Spatial+Attention+in+Vision+Transformers+Chu,+Xiangxiang+and+Tian,+Zhi+and+Wang,+Yuqing+and+Zhang,+Bo+and+Ren,+Haibing+and+Wei,+Xiaolin+and+Xia,+Huaxia+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Twins:+Revisiting+the+Design+of+Spatial+Attention+in+Vision+Transformers" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/Meituan-AutoML/Twins" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
</ol>
<h1>2020</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><img class="imgP  right"   src="data/thumbnail/Pan2020ACMSurveyxxxarXiv.jpg"><b>Deep learning for anomaly detection: a review</b>   
<br />\(\cdot\) <i>G. Pang, C. Shen, L. Cao, A. van den Hengel</i>.
<br />\(\cdot\) <i>ACM Computing Surveys (ACMSurvey), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2007.02500" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Pan2020ACMSurvey.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Learning+for+Anomaly+Detection:+A+Review+Pang,+Guansong+and+Shen,+Chunhua+and+Cao,+Longbing+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Learning+for+Anomaly+Detection:+A+Review" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Towards light-weight portrait matting via parameter sharing</b>   
<br />\(\cdot\) <i>Y. Dai, H. Lu, C. Shen</i>.
<br />\(\cdot\) <i>Computer Graphics Forum (CGF), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/Daiyt2020.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Light-Weight+Portrait+Matting+via+Parameter+Sharing+Dai,+Yutong+and+Lu,+Hao+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+Light-Weight+Portrait+Matting+via+Parameter+Sharing" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Luo2020IJCVxxxarXiv.jpg"><b>Separating content from style using adversarial learning for recognizing text in the wild</b>   
<br />\(\cdot\) <i>C. Luo, Q. Lin, Y. Liu, L. Jin, C. Shen</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2001.04189" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Luo2020IJCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Separating+Content+from+Style+Using+Adversarial+Learning+for+Recognizing+Text+in+the+Wild+Luo,+Canjie+and+Lin,+Qingxiang+and+Liu,+Yuliang+and+Jin,+Lianwen+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Separating+Content+from+Style+Using+Adversarial+Learning+for+Recognizing+Text+in+the+Wild" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>TasselNetv2: in-field counting of wheat spikes with context-augmented local regression networks</b>   
<br />\(\cdot\) <i>H. Xiong, Z. Cao, H. Lu, S. Madec, L. Liu, C. Shen</i>.
<br />\(\cdot\) <i>Plant Methods (PLME), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/TasselNet2020.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={TasselNetv2}:+in-field+counting+of+wheat+spikes+with+context-augmented+local+regression+networks+Xiong,+Haipeng+and+Cao,+Zhiguo+and+Lu,+Hao+and+Madec,+Simon+and+Liu,+Liang+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={TasselNetv2}:+in-field+counting+of+wheat+spikes+with+context-augmented+local+regression+networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/MobileFAN2020xxxarXiv.jpg"><b>MobileFAN: transferring deep hidden representation for face alignment</b>   
<br />\(\cdot\) <i>Y. Zhao, Y. Liu, C. Shen, Y. Gao, S. Xiong</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1908.03839" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/MobileFAN2020.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={MobileFAN}:+Transferring+Deep+Hidden+Representation+for+Face+Alignment+Zhao,+Yang+and+Liu,+Yifan+and+Shen,+Chunhua+and+Gao,+Yongsheng+and+Xiong,+Shengwu" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={MobileFAN}:+Transferring+Deep+Hidden+Representation+for+Face+Alignment" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhangx2020T-ITSxxxarXiv.jpg"><b>Part-guided attention learning for vehicle instance retrieval</b>   
<br />\(\cdot\) <i>X. Zhang, R. Zhang, J. Cao, D. Gong, M. You, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1909.06023" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhangx2020T-ITS.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Part-Guided+Attention+Learning+for+Vehicle+Instance+Retrieval+Zhang,+Xinyu+and+Zhang,+Rufeng+and+Cao,+Jiewei+and+Gong,+Dong+and+You,+Mingyu+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Part-Guided+Attention+Learning+for+Vehicle+Instance+Retrieval" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>A robust attentional framework for license plate recognition in the wild</b>   
<br />\(\cdot\) <i>L. Zhang, P. Wang, H. Li, Z. Li, C. Shen, Y. Zhang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/Li2020Carlicense.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+robust+attentional+framework+for+license+plate+recognition+in+the+wild+Zhang,+Linjiang+and+Wang,+Peng+and+Li,+Hui+and+Li,+Zhen+and+Shen,+Chunhua+and+Zhang,+Yanning" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+robust+attentional+framework+for+license+plate+recognition+in+the+wild" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Real-time high-performance semantic image segmentation of urban street scenes</b>   
<br />\(\cdot\) <i>G. Dong, Y. Yan, C. Shen, H. Wang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/Dong2020segmentation.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-time+high-performance+semantic+image+segmentation+of+urban+street+scenes+Dong,+Genshun+and+Yan,+Yan+and+Shen,+Chunhua+and+Wang,+Hanzi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Real-time+high-performance+semantic+image+segmentation+of+urban+street+scenes" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Towards effective deep embedding for zero-shot learning</b>   
<br />\(\cdot\) <i>L. Zhang, P. Wang, L. Liu, C. Shen, W. Wei, Y. Zhang, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhang2020Zeroshot.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Effective+Deep+Embedding+for+Zero-Shot+Learning+Zhang,+Lei+and+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Wei,+Wei+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+Effective+Deep+Embedding+for+Zero-Shot+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>NSSNet: scale-aware object counting with non-scale suppression</b>   
<br />\(\cdot\) <i>L. Liu, Z. Cao, H. Lu, H. Xiong, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/LiuL2020CSVT.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={NSSNet}:+Scale-aware+object+counting+with+non-scale+suppression+Liu,+Liang+and+Cao,+Zhiguo+and+Lu,+Hao+and+Xiong,+Haipeng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={NSSNet}:+Scale-aware+object+counting+with+non-scale+suppression" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhang2020CovidxxxarXiv.jpg"><b>Viral pneumonia screening on chest x-ray images using confidence-aware anomaly detection</b>   
<br />\(\cdot\) <i>J. Zhang, Y. Xie, Z. Liao, G. Pang, J. Verjans, W. Li, Z. Sun, J. He, Y. Li, C. Shen, Y. Xia</i>.
<br />\(\cdot\) <i>IEEE Transactions on Medical Imaging (TMI), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2003.12338" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhang2020Covid.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Viral+Pneumonia+Screening+on+Chest+X-ray+Images+Using+Confidence-Aware+Anomaly+Detection+Zhang,+Jianpeng+and+Xie,+Yutong+and+Liao,+Zhibin+and+Pang,+Guansong+and+Verjans,+Johan+and+Li,+Wenxin+and+Sun,+Zongji+and+He,+Jian+and+Li,+Yi+and+Shen,+Chunhua+and+Xia,+Yong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Viral+Pneumonia+Screening+on+Chest+X-ray+Images+Using+Confidence-Aware+Anomaly+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Xie2020TMIaxxxarXiv.jpg"><b>A mutual bootstrapping model for automated skin lesion segmentation and classification</b>   
<br />\(\cdot\) <i>Y. Xie, J. Zhang, Y. Xia, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Medical Imaging (TMI), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1903.03313" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Xie2020TMIa.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Mutual+Bootstrapping+Model+for+Automated+Skin+Lesion+Segmentation+and+Classification+Xie,+Yutong+and+Zhang,+Jianpeng+and+Xia,+Yong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Mutual+Bootstrapping+Model+for+Automated+Skin+Lesion+Segmentation+and+Classification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>SESV: accurate medical image segmentation by predicting and correcting errors</b>   
<br />\(\cdot\) <i>Y. Xie, J. Zhang, H. Lu, C. Shen, Y. Xia</i>.
<br />\(\cdot\) <i>IEEE Transactions on Medical Imaging (TMI), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/Xie2020TMIb.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SESV}:+Accurate+Medical+Image+Segmentation+by+Predicting+and+Correcting+Errors+Xie,+Yutong+and+Zhang,+Jianpeng+and+Lu,+Hao+and+Shen,+Chunhua+and+Xia,+Yong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SESV}:+Accurate+Medical+Image+Segmentation+by+Predicting+and+Correcting+Errors" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>OPMP: an omni-directional pyramid mask proposal network for arbitrary-shape scene text detection</b>   
<br />\(\cdot\) <i>S. Zhang, Y. Liu, L. Jin, Z. Wei, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Multimedia (TMM), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/ShengZhang2020TMM.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={OPMP}:+An+Omni-directional+Pyramid+Mask+Proposal+Network+for+Arbitrary-shape+Scene+Text+Detection+Zhang,+Sheng+and+Liu,+Yuliang+and+Jin,+Lianwen+and+Wei,+Zhongrong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={OPMP}:+An+Omni-directional+Pyramid+Mask+Proposal+Network+for+Arbitrary-shape+Scene+Text+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Joint deep learning of facial expression synthesis and recognition</b>   
<br />\(\cdot\) <i>Y. Yan, Y. Huang, S. Chen, C. Shen, H. Wang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Multimedia (TMM), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/Yan2020TMM.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Joint+deep+learning+of+facial+expression+synthesis+and+recognition+Yan,+Yan+and+Huang,+Ying+and+Chen,+Si+and+Shen,+Chunhua+and+Wang,+Hanzi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Joint+deep+learning+of+facial+expression+synthesis+and+recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Accurate tensor completion via adaptive low-rank representation</b>   
<br />\(\cdot\) <i>L. Zhang, W. Wei, Q. Shi, C. Shen, A. van den Hengel, Y. Zhang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhang2020TNNLS.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Accurate+Tensor+Completion+via+Adaptive+Low-Rank+Representation+Zhang,+Lei+and+Wei,+Wei+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Zhang,+Yanning" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Accurate+Tensor+Completion+via+Adaptive+Low-Rank+Representation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Deep clustering with sample-assignment invariance prior</b>   
<br />\(\cdot\) <i>X. Peng, H. Zhu, J. Feng, C. Shen, H. Zhang, J. Zhou</i>.
<br />\(\cdot\) <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/Peng2020TNNLS.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Clustering+with+Sample-Assignment+Invariance+Prior+Peng,+Xi+and+Zhu,+Hongyuan+and+Feng,+Jiashi+and+Shen,+Chunhua+and+Zhang,+Haixian+and+Zhou,+Joey" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Clustering+with+Sample-Assignment+Invariance+Prior" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Gong2020TNNLSxxxarXiv.jpg"><b>Learning deep gradient descent optimization for image deconvolution</b>   
<br />\(\cdot\) <i>D. Gong, Z. Zhang, Q. Shi, A. van den Hengel, C. Shen, Y. Zhang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1804.03368" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Gong2020TNNLS.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Deep+Gradient+Descent+Optimization+for+Image+Deconvolution+Gong,+Dong+and+Zhang,+Zhen+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Zhang,+Yanning" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Deep+Gradient+Descent+Optimization+for+Image+Deconvolution" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2020TOGxxxarXiv.jpg"><b>Real-time image smoothing via iterative least squares</b>   
<br />\(\cdot\) <i>W. Liu, P. Zhang, X. Huang, J. Yang, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>ACM Transactions on Graphics (TOG), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2003.07504" target=&ldquo;blank&rdquo;>arXiv</a><a href="https://doi.org/10.1145/3388887" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Liu2020TOG.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-time+image+smoothing+via+iterative+least+squares+Liu,+Wei+and+Zhang,+Pingping+and+Huang,+Xiaolin+and+Yang,+Jie+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Real-time+image+smoothing+via+iterative+least+squares" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/wliusjtu/Real-time-Image-Smoothing-via-Iterative-Least-Squares" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Plenty is plague: fine-grained learning for visual question answering</b>   
<br />\(\cdot\) <i>Y. Zhou, R. Ji, J. Su, X. Sun, D. Meng, Y. Gao, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br />\(\cdot\) <a href="https://doi.org/10.1109/TPAMI.2019.2956699" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Zhou2020TPAMIZhou.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Plenty+Is+Plague:+Fine-Grained+Learning+for+Visual+Question+Answering+Zhou,+Yiyi+and+Ji,+Rongrong+and+Su,+Jinsong+and+Sun,+Xiaoshuai+and+Meng,+Deyu+and+Gao,+Yue+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Plenty+Is+Plague:+Fine-Grained+Learning+for+Visual+Question+Answering" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhang2020OrderlessReIDxxxarXiv.jpg"><b>Ordered or orderless: a revisit for video based person re-identification</b>   
<br />\(\cdot\) <i>L. Zhang, Z. Shi, J. Zhou, M. Cheng, Y. Liu, J. Bian, Z. Zeng, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1912.11236" target=&ldquo;blank&rdquo;>arXiv</a><a href="https://doi.org/10.1109/TPAMI.2020.2976969" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Zhang2020OrderlessReID.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Ordered+or+Orderless:+A+Revisit+for+Video+based+Person+Re-Identification+Zhang,+Le+and+Shi,+Zenglin+and+Zhou,+Joey+Tianyi+and+Cheng,+Ming-Ming+and+Liu,+Yun+and+Bian,+Jia-Wang+and+Zeng,+Zeng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Ordered+or+Orderless:+A+Revisit+for+Video+based+Person+Re-Identification" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/ZhangLeUestc/VideoReid-TPAMI2020" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Lu2020PAMIIndexNetxxxarXiv.jpg"><b>Index networks</b>   
<br />\(\cdot\) <i>H. Lu, Y. Dai, C. Shen, S. Xu</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1908.09895" target=&ldquo;blank&rdquo;>arXiv</a><a href="https://doi.org/10.1109/TPAMI.2020.3004474" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Lu2020PAMIIndexNet.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Index+Networks+Lu,+Hao+and+Dai,+Yutong+and+Shen,+Chunhua+and+Xu,+Songcen" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Index+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/IndexNet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2020PAMIxxxarXiv.jpg"><b>Structured knowledge distillation for dense prediction</b>   
<br />\(\cdot\) <i>Y. Liu, C. Shun, J. Wang, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1903.04197" target=&ldquo;blank&rdquo;>arXiv</a><a href="https://ieeexplore.ieee.org/document/9115859" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Liu2020PAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Knowledge+Distillation+for+Dense+Prediction+Liu,+Yifan+and+Shun,+Changyong+and+Wang,+Jingdong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+Knowledge+Distillation+for+Dense+Prediction" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/irfanICMLL/structure_knowledge_distillation" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1711.00253.pdf"><img class="imgP  right"   src="data/thumbnail/Chen2019PAMIxxxarXiv.jpg"></a><b>Adversarial learning of structure-aware fully convolutional networks for landmark localization</b>   
<br />\(\cdot\) <i>Y. Chen, C. Shen, H. Chen, X. Wei, L. Liu, J. Yang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1711.00253" target=&ldquo;blank&rdquo;>arXiv</a><a href="https://doi.org/10.1109/TPAMI.2019.2901875" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Chen2019PAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adversarial+Learning+of+Structure-Aware+Fully+Convolutional+Networks+for+Landmark+Localization+Chen,+Yu+and+Shen,+Chunhua+and+Chen,+Hao+and+Wei,+Xiu-Shen+and+Liu,+Lingqiao+and+Yang,+Jian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Adversarial+Learning+of+Structure-Aware+Fully+Convolutional+Networks+for+Landmark+Localization" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2008.00942.pdf"><img class="imgP  right"   src="data/thumbnail/Cao2020GANxxxarXiv.jpg"></a><b>Improving generative adversarial networks with local coordinate coding</b>   
<br />\(\cdot\) <i>J. Cao, Y. Guo, Q. Wu, C. Shen, J. Huang, M. Tan</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2008.00942" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Cao2020GAN.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Improving+Generative+Adversarial+Networks+with+Local+Coordinate+Coding+Cao,+Jiezhang+and+Guo,+Yong+and+Wu,+Qingyao+and+Shen,+Chunhua+and+Huang,+Junzhou+and+Tan,+Mingkui" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Improving+Generative+Adversarial+Networks+with+Local+Coordinate+Coding" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/SCUTjinchengli/LCCGAN-v2" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1909.07701.pdf"><img class="imgP  right"   src="data/thumbnail/AAAI20WangxxxarXiv.jpg"></a><b>Task-aware monocular depth estimation for 3D object detection</b>   
<br />\(\cdot\) <i>X. Wang, W. Yin, T. Kong, Y. Jiang, L. Li, C. Shen</i>.
<br />\(\cdot\) <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1909.07701" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/AAAI20Wang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Task-Aware+Monocular+Depth+Estimation+for+{3D}+Object+Detection+Wang,+Xinlong+and+Yin,+Wei+and+Kong,+Tao+and+Jiang,+Yuning+and+Li,+Lei+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Task-Aware+Monocular+Depth+Estimation+for+{3D}+Object+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1907.12271.pdf"><img class="imgP  right"   src="data/thumbnail/AAAI20TeneyxxxarXiv.jpg"></a><b>V-PROM: a benchmark for visual reasoning using visual progressive matrices</b>   
<br />\(\cdot\) <i>D. Teney, P. Wang, J. Cao, L. Liu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1907.12271" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/AAAI20Teney.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={V-PROM}:+A+Benchmark+for+Visual+Reasoning+Using+Visual+Progressive+Matrices+Teney,+Damien+and+Wang,+Peng+and+Cao,+Jiewei+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={V-PROM}:+A+Benchmark+for+Visual+Reasoning+Using+Visual+Progressive+Matrices" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.11236.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Zhuang5xxxarXiv.jpg"></a><b>Training quantized neural networks with a full-precision auxiliary module</b>   
<br />\(\cdot\) <i>B. Zhuang, L. Liu, M. Tan, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1903.11236" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2020Zhuang5.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Training+Quantized+Neural+Networks+with+a+Full-precision+Auxiliary+Module+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Tan,+Mingkui+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Training+Quantized+Neural+Networks+with+a+Full-precision+Auxiliary+Module" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.11712.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Zhang3xxxarXiv.jpg"></a><b>Mask encoding for single shot instance segmentation</b>   
<br />\(\cdot\) <i>R. Zhang, Z. Tian, C. Shen, M. You, Y. Yan</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2003.11712" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2020Zhang3.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mask+Encoding+for+Single+Shot+Instance+Segmentation+Zhang,+Rufeng+and+Tian,+Zhi+and+Shen,+Chunhua+and+You,+Mingyu+and+Yan,+Youliang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Mask+Encoding+for+Single+Shot+Instance+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/aim-uofa/AdelaiDet/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1909.08228.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020NAS11xxxarXiv.jpg"></a><b>Memory-efficient hierarchical neural architecture search for image denoising</b>   
<br />\(\cdot\) <i>H. Zhang, Y. Li, H. Chen, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1909.08228" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2020NAS11.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Memory-Efficient+Hierarchical+Neural+Architecture+Search+for+Image+Denoising+Zhang,+Haokui+and+Li,+Ying+and+Chen,+Hao+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Memory-Efficient+Hierarchical+Neural+Architecture+Search+for+Image+Denoising" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.06777.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020EMD9xxxarXiv.jpg"></a><b>DeepEMD: few-shot image classification with differentiable earth mover's distance and structured classifiers</b>   
<br />\(\cdot\) <i>C. Zhang, Y. Cai, G. Lin, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2003.06777" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2020EMD9.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={DeepEMD}:+Few-Shot+Image+Classification+with+Differentiable+Earth+Mover's+Distance+and+Structured+Classifiers+Zhang,+Chi+and+Cai,+Yujun+and+Lin,+Guosheng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={DeepEMD}:+Few-Shot+Image+Classification+with+Differentiable+Earth+Mover's+Distance+and+Structured+Classifiers" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/icoz69/DeepEMD" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2004.01547.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Yu2xxxarXiv.jpg"></a><b>Context prior for scene segmentation</b>   
<br />\(\cdot\) <i>C. Yu, J. Wang, C. Gao, G. Yu, C. Shen, N. Sang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2004.01547" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2020Yu2.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Context+Prior+for+Scene+Segmentation+Yu,+Changqian+and+Wang,+Jingbo+and+Gao,+Changxin+and+Yu,+Gang+and+Shen,+Chunhua+and+Sang,+Nong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Context+Prior+for+Scene+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1909.13226.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Xie8xxxarXiv.jpg"></a><b>PolarMask: single shot instance segmentation with polar representation</b>   
<br />\(\cdot\) <i>E. Xie, P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen, P. Luo</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1909.13226" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2020Xie8.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={PolarMask}:+Single+Shot+Instance+Segmentation+with+Polar+Representation+Xie,+Enze+and+Sun,+Peize+and+Song,+Xiaoge+and+Wang,+Wenhai+and+Liu,+Xuebo+and+Liang,+Ding+and+Shen,+Chunhua+and+Luo,+Ping" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={PolarMask}:+Single+Shot+Instance+Segmentation+with+Polar+Representation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/xieenze/PolarMask" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2002.10215.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Wang4xxxarXiv.jpg"></a><b>On the general value of evidence, and bilingual scene-text visual question answering</b>   
<br />\(\cdot\) <i>X. Wang, Y. Liu, C. Shen, C. Ng, C. Luo, L. Jin, C. Chan, A. van den Hengel, L. Wang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2002.10215" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2020Wang4.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=On+the+General+Value+of+Evidence,+and+Bilingual+Scene-Text+Visual+Question+Answering+Wang,+Xinyu+and+Liu,+Yuliang+and+Shen,+Chunhua+and+Ng,+Chun+Chet+and+Luo,+Canjie+and+Jin,+Lianwen+and+Chan,+Chee+Seng+and+{van+den+Hengel},+Anton+and+Wang,+Liangwei" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=On+the+General+Value+of+Evidence,+and+Bilingual+Scene-Text+Visual+Question+Answering" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1906.04423.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020NASFCOS10xxxarXiv.jpg"></a><b>NAS-FCOS: fast neural architecture search for object detection</b>   
<br />\(\cdot\) <i>N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, Y. Zhang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1906.04423" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2020NASFCOS10.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={NAS-FCOS}:+Fast+Neural+Architecture+Search+for+Object+Detection+Wang,+Ning+and+Gao,+Yang+and+Chen,+Hao+and+Wang,+Peng+and+Tian,+Zhi+and+Shen,+Chunhua+and+Zhang,+Yanning" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={NAS-FCOS}:+Fast+Neural+Architecture+Search+for+Object+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/Lausannen/NAS-FCOS" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1904.10151.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020REVERIE1xxxarXiv.jpg"></a><b>REVERIE: remote embodied visual referring expression in real indoor environments</b>   
<br />\(\cdot\) <i>Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Wang, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1904.10151" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2020REVERIE1.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={REVERIE}:+Remote+Embodied+Visual+Referring+Expression+in+Real+Indoor+Environments+Qi,+Yuankai+and+Wu,+Qi+and+Anderson,+Peter+and+Wang,+Xin+and+Wang,+William+Yang+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={REVERIE}:+Remote+Embodied+Visual+Referring+Expression+in+Real+Indoor+Environments" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.06780.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Self12xxxarXiv.jpg"></a><b>Self-trained deep ordinal regression for end-to-end video anomaly detection</b>   
<br />\(\cdot\) <i>G. Pang, C. Yan, C. Shen, A. van den Hengel, X. Bai</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2003.06780" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2020Self12.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Self-trained+Deep+Ordinal+Regression+for+End-to-End+Video+Anomaly+Detection+Pang,+Guansong+and+Yan,+Cheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Bai,+Xiao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Self-trained+Deep+Ordinal+Regression+for+End-to-End+Video+Anomaly+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2002.10200.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Liu6xxxarXiv.jpg"></a><b>ABCNet: arbitrarily-shaped scene text spotting with adaptive Bezier-curve network in real time</b>   
<br />\(\cdot\) <i>Y. Liu, H. Chen, C. Shen, T. He, L. Jin, L. Wang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2002.10200" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2020Liu6.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={ABCNet}:+Arbitrarily-Shaped+Scene+Text+Spotting+with+Adaptive+{B}ezier-Curve+Network+in+Real+Time+Liu,+Yuliang+and+Chen,+Hao+and+Shen,+Chunhua+and+He,+Tong+and+Jin,+Lianwen+and+Wang,+Liangwei" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={ABCNet}:+Arbitrarily-Shaped+Scene+Text+Spotting+with+Adaptive+{B}ezier-Curve+Network+in+Real+Time" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/aim-uofa/AdelaiDet/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2001.00309.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020BlendMask7xxxarXiv.jpg"></a><b>BlendMask: top-down meets bottom-up for instance segmentation</b>   
<br />\(\cdot\) <i>H. Chen, K. Sun, Z. Tian, C. Shen, Y. Huang, Y. Yan</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2001.00309" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2020BlendMask7.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={BlendMask}:+Top-Down+Meets+Bottom-Up+for+Instance+Segmentation+Chen,+Hao+and+Sun,+Kunyang+and+Tian,+Zhi+and+Shen,+Chunhua+and+Huang,+Yongming+and+Yan,+Youliang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={BlendMask}:+Top-Down+Meets+Bottom-Up+for+Instance+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/aim-uofa/AdelaiDet/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Yu2020RepGraphNetxxxarXiv.jpg"><b>Representative graph neural network</b>   
<br />\(\cdot\) <i>C. Yu, Y. Liu, C. Gao, C. Shen, N. Sang</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2008.05202" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Yu2020RepGraphNet.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Representative+Graph+Neural+Network+Yu,+Changqian+and+Liu,+Yifan+and+Gao,+Changxin+and+Shen,+Chunhua+and+Sang,+Nong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Representative+Graph+Neural+Network" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/ycszen/RepGraph" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Segmenting transparent objects in the wild</b>   
<br />\(\cdot\) <i>E. Xie, W. Wang, W. Wang, M. Ding, C. Shen, P. Luo</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2003.13948" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Xie2020Segtransp.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Segmenting+Transparent+Objects+in+the+Wild+Xie,+Enze+and+Wang,+Wenjia+and+Wang,+Wenhai+and+Ding,+Mingyu+and+Shen,+Chunhua+and+Luo,+Ping" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Segmenting+Transparent+Objects+in+the+Wild" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2020SOLOxxxarXiv.jpg"><b>SOLO: segmenting objects by locations</b>   
<br />\(\cdot\) <i>X. Wang, T. Kong, C. Shen, Y. Jiang, L. Li</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1912.04488" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wang2020SOLO.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SOLO}:+Segmenting+Objects+by+Locations+Wang,+Xinlong+and+Kong,+Tao+and+Shen,+Chunhua+and+Jiang,+Yuning+and+Li,+Lei" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SOLO}:+Segmenting+Objects+by+Locations" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/aim-uofa/adet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2020SuperResxxxarXiv.jpg"><b>Scene text image super-resolution in the wild</b>   
<br />\(\cdot\) <i>W. Wang, E. Xie, X. Liu, W. Wang, D. Liang, C. Shen, X. Bai</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2005.03341" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wang2020SuperRes.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Scene+Text+Image+Super-Resolution+in+the+Wild+Wang,+Wenjia+and+Xie,+Enze+and+Liu,+Xuebo+and+Wang,+Wenhai+and+Liang,+Ding+and+Shen,+Chunhua+and+Bai,+Xiang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Scene+Text+Image+Super-Resolution+in+the+Wild" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>AE TextSpotter: learning visual and linguistic representation for ambiguous text spotting</b>   
<br />\(\cdot\) <i>W. Wang, X. Liu, X. Ji, E. Xie, D. Liang, Z. Yang, T. Lu, C. Shen, P. Luo</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/Wang2020AET.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={AE+TextSpotter}:+Learning+Visual+and+Linguistic+Representation+for+Ambiguous+Text+Spotting+Wang,+Wenhai+and+Liu,+Xuebo+and+Ji,+Xiaozhong+and+Xie,+Enze+and+Liang,+Ding+and+Yang,+ZhiBo+and+Lu,+Tong+and+Shen,+Chunhua+and+Luo,+Ping" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={AE+TextSpotter}:+Learning+Visual+and+Linguistic+Representation+for+Ambiguous+Text+Spotting" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Soft expert reward learning for vision-and-language navigation</b>   
<br />\(\cdot\) <i>H. Wang, Q. Wu, C. Shen</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/Wang2020Soft.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Soft+Expert+Reward+Learning+for+Vision-and-Language+Navigation+Wang,+Hu+and+Wu,+Qi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Soft+Expert+Reward+Learning+for+Vision-and-Language+Navigation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Tian2020CondInstxxxarXiv.jpg"><b>Conditional convolutions for instance segmentation</b>   
<br />\(\cdot\) <i>Z. Tian, C. Shen, H. Chen</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2003.05664" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Tian2020CondInst.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Conditional+Convolutions+for+Instance+Segmentation+Tian,+Zhi+and+Shen,+Chunhua+and+Chen,+Hao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Conditional+Convolutions+for+Instance+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/aim-uofa/adet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2020EfficientSemanticxxxarXiv.jpg"><b>Efficient semantic video segmentation with per-frame inference</b>   
<br />\(\cdot\) <i>Y. Liu, C. Shen, C. Yu, J. Wang</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2002.11433" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Liu2020EfficientSemantic.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Semantic+Video+Segmentation+with+Per-frame+Inference+Liu,+Yifan+and+Shen,+Chunhua+and+Yu,+Changqian+and+Wang,+Jingdong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+Semantic+Video+Segmentation+with+Per-frame+Inference" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://tinyurl.com/segment-video" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2020WeightingRLxxxarXiv.jpg"><b>Weighing counts: sequential crowd counting by reinforcement learning</b>   
<br />\(\cdot\) <i>L. Liu, H. Lu, H. Zou, H. Xiong, Z. Cao, C. Shen</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2007.08260" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Liu2020WeightingRL.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Weighing+Counts:+Sequential+Crowd+Counting+by+Reinforcement+Learning+Liu,+Liang+and+Lu,+Hao+and+Zou,+Hongwei+and+Xiong,+Haipeng+and+Cao,+Zhiguo+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Weighing+Counts:+Sequential+Crowd+Counting+by+Reinforcement+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/poppinace/libranet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Instance-aware embedding for point cloud instance segmentation</b>   
<br />\(\cdot\) <i>T. He, Y. Liu, C. Shen, X. Wang, C. Sun</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/He2020InstanceAware.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Instance-Aware+Embedding+for+Point+Cloud+Instance+Segmentation+He,+Tong+and+Liu,+Yifan+and+Shen,+Chunhua+and+Wang,+Xinlong+and+Sun,+Changming" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Instance-Aware+Embedding+for+Point+Cloud+Instance+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Learning and memorizing representative prototypes for 3D point cloud semantic and instance segmentation</b>   
<br />\(\cdot\) <i>T. He, D. Gong, Z. Tian, C. Shen</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/He2020PC1.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+and+Memorizing+Representative+Prototypes+for+{3D}+Point+Cloud+Semantic+and+Instance+Segmentation+He,+Tong+and+Gong,+Dong+and+Tian,+Zhi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+and+Memorizing+Representative+Prototypes+for+{3D}+Point+Cloud+Semantic+and+Instance+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2020IJCAIxxxarXiv.jpg"><b>Unsupervised representation learning by predicting random distances</b>   
<br />\(\cdot\) <i>H. Wang, G. Pang, C. Shen, C. Ma</i>.
<br />\(\cdot\) <i>Proc. International Joint Conferences on Artificial Intelligence (IJCAI&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1912.12186" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wang2020IJCAI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Representation+Learning+by+Predicting+Random+Distances+Wang,+Hu+and+Pang,+Guansong+and+Shen,+Chunhua+and+Ma,+Congbo" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Representation+Learning+by+Predicting+Random+Distances" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Pairwise relation learning for semi-supervised gland segmentation</b>   
<br />\(\cdot\) <i>Y. Xie, J. Zhang, Z. Liao, C. Shen, J. Verjans, Y. Xia</i>.
<br />\(\cdot\) <i>Proc. International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="data/bibtex/YXie2020MICCAI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Pairwise+Relation+Learning+for+Semi-supervised+Gland+Segmentation+Xie,+Yutong+and+Zhang,+Jianpeng+and+Liao,+Zhibin+and+Shen,+Chunhua+and+Verjans,+Johan+and+Xia,+Yong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Pairwise+Relation+Learning+for+Semi-supervised+Gland+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/SoloV22020xxxarXiv.jpg"><b>SOLOv2: dynamic and fast instance segmentation</b>   
<br />\(\cdot\) <i>X. Wang, R. Zhang, T. Kong, L. Li, C. Shen</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;20), 2020</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/2003.10152" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/SoloV22020.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SOLOv2}:+Dynamic+and+Fast+Instance+Segmentation+Wang,+Xinlong+and+Zhang,+Rufeng+and+Kong,+Tao+and+Li,+Lei+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={SOLOv2}:+Dynamic+and+Fast+Instance+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://git.io/AdelaiDet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
</ol>
<h1>2019</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1806.01576.pdf"><img class="imgP  right"   src="data/thumbnail/Adaptive2019ZhangxxxarXiv.jpg"></a><b>Adaptive importance learning for improving lightweight image super-resolution network</b>   
<br />\(\cdot\) <i>L. Zhang, P. Wang, C. Shen, L. Liu, W. Wei, Y. Zhang, A. van den Hengel</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1806.01576" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Adaptive2019Zhang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adaptive+Importance+Learning+for+Improving+Lightweight+Image+Super-resolution+Network+Zhang,+Lei+and+Wang,+Peng+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Wei,+Wei+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Adaptive+Importance+Learning+for+Improving+Lightweight+Image+Super-resolution+Network" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://tinyurl.com/Super-resolution-Network" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Accurate imagery recovery using a multi-observation patch model</b>   
<br />\(\cdot\) <i>L. Zhang, W. Wei, Q. Shen, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Information Sciences (IS), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhang2019Accurate.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Accurate+Imagery+Recovery+Using+a+Multi-Observation+Patch+Model+Zhang,+Lei+and+Wei,+Wei+and+Shen,+Qiang+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Accurate+Imagery+Recovery+Using+a+Multi-Observation+Patch+Model" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Heritage image annotation via collective knowledge</b>   
<br />\(\cdot\) <i>J. Zhang, Q. Wu, J. Zhang, C. Shen, J. Lu, Q. Wu</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhang2019PR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Heritage+Image+Annotation+via+Collective+Knowledge+Zhang,+Junjie+and+Wu,+Qi+and+Zhang,+Jian+and+Shen,+Chunhua+and+Lu,+Jianfeng+and+Wu,+Qiang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Heritage+Image+Annotation+via+Collective+Knowledge" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wu2019PRxxxarXiv.jpg"><b>Wider or deeper: revisiting the ResNet model for visual recognition</b>   
<br />\(\cdot\) <i>Z. Wu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1611.10080" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wu2019PR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Wider+or+Deeper:+Revisiting+the+{ResNet}+Model+for+Visual+Recognition+Wu,+Zifeng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Wider+or+Deeper:+Revisiting+the+{ResNet}+Model+for+Visual+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Order-aware convolutional pooling for video based action recognition</b>   
<br />\(\cdot\) <i>P. Wang, L. Liu, C. Shen, H. Shen</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/Wang2019PR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Order-aware+Convolutional+Pooling+for+Video+Based+Action+Recognition+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Shen,+Heng+Tao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Order-aware+Convolutional+Pooling+for+Video+Based+Action+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Structural analysis of attributes for vehicle re-identification and retrieval</b>   
<br />\(\cdot\) <i>Y. Zhao, C. Shen, H. Wang, S. Chen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhao2019Structural.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structural+Analysis+of+Attributes+for+Vehicle+Re-identification+and+Retrieval+Zhao,+Yanzhu+and+Shen,+Chunhua+and+Wang,+Huibing+and+Chen,+Shengyong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Structural+Analysis+of+Attributes+for+Vehicle+Re-identification+and+Retrieval" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Human detection aided by deeply learned semantic masks</b>   
<br />\(\cdot\) <i>X. Wang, C. Shen, H. Li, S. Xu</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/Wangxy2019CSVT.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Human+Detection+Aided+by+Deeply+Learned+Semantic+Masks+Wang,+Xinyu+and+Shen,+Chunhua+and+Li,+Hanxi+and+Xu,+Shugong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Human+Detection+Aided+by+Deeply+Learned+Semantic+Masks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Embedding bilateral filter in least squares for efficient edge-preserving image smoothing</b>   
<br />\(\cdot\) <i>W. Liu, P. Zhang, X. Chen, C. Shen, X. Huang, J. Yang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/Liu2019CSVT.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Embedding+Bilateral+Filter+in+Least+Squares+for+Efficient+Edge-preserving+Image+Smoothing+Liu,+Wei+and+Zhang,+Pingping+and+Chen,+Xiaogang+and+Shen,+Chunhua+and+Huang,+Xiaolin+and+Yang,+Jie" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Embedding+Bilateral+Filter+in+Least+Squares+for+Efficient+Edge-preserving+Image+Smoothing" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Counting objects by blockwise classification</b>   
<br />\(\cdot\) <i>L. Liu, H. Lu, H. Xiong, K. Xian, Z. Cao, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/Counting2019CSVT.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Counting+Objects+by+Blockwise+Classification+Liu,+Liang+and+Lu,+Hao+and+Xiong,+Haipeng+and+Xian,+Ke+and+Cao,+Zhiguo+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Counting+Objects+by+Blockwise+Classification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Hyperspectral classification based on lightweight 3D-CNN with transfer learning</b>   
<br />\(\cdot\) <i>H. Zhang, Y. Li, Y. Jiang, P. Wang, Q. Shen, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhang2019Lightweight.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Hyperspectral+Classification+Based+on+Lightweight+{3D-CNN}+With+Transfer+Learning+Zhang,+Haokui+and+Li,+Ying+and+Jiang,+Yenan+and+Wang,+Peng+and+Shen,+Qiang+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Hyperspectral+Classification+Based+on+Lightweight+{3D-CNN}+With+Transfer+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Salient object detection with lossless feature reflection and weighted structural loss</b>   
<br />\(\cdot\) <i>P. Zhang, W. Liu, H. Lu, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhang2019Salient.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Salient+Object+Detection+with+Lossless+Feature+Reflection+and+Weighted+Structural+Loss+Zhang,+Pingping+and+Liu,+Wei+and+Lu,+Huchuan+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Salient+Object+Detection+with+Lossless+Feature+Reflection+and+Weighted+Structural+Loss" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wei2019TIPxxxarXiv.jpg"><b>Piecewise classifier mappings: learning fine-grained learners for novel categories with few examples</b>   
<br />\(\cdot\) <i>X. Wei, P. Wang, L. Liu, C. Shen, J. Wu</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1805.04288" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wei2019TIP.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Piecewise+classifier+mappings:+Learning+fine-grained+learners+for+novel+categories+with+few+examples+Wei,+Xiu-Shen+and+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Wu,+Jianxin" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Piecewise+classifier+mappings:+Learning+fine-grained+learners+for+novel+categories+with+few+examples" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Multiple instance learning with emerging novel class</b>   
<br />\(\cdot\) <i>X. Wei, H. Ye, X. Mu, J. Wu, C. Shen, Z. Zhou</i>.
<br />\(\cdot\) <i>IEEE Transactions on Knowledge and Data Engineering (TKDE), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/Wei2019TKDE.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multiple+Instance+Learning+with+Emerging+Novel+Class+Wei,+Xiu-Shen+and+Ye,+Han-Jia+and+Mu,+Xin+and+Wu,+Jianxin+and+Shen,+Chunhua+and+Zhou,+Zhi-Hua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Multiple+Instance+Learning+with+Emerging+Novel+Class" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Attention residual learning for skin lesion classification</b>   
<br />\(\cdot\) <i>J. Zhang, Y. Xie, Y. Xia, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Medical Imaging (TMI), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhang2019Attn.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Attention+residual+learning+for+skin+lesion+classification+Zhang,+Jianpeng+and+Xie,+Yutong+and+Xia,+Yong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Attention+residual+learning+for+skin+lesion+classification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TZhang2019TMMxxxarXiv.jpg"><b>Decoupled spatial neural attention for weakly supervised semantic segmentation</b>   
<br />\(\cdot\) <i>T. Zhang, G. Lin, J. Cai, T. Shen, C. Shen, A. Kot</i>.
<br />\(\cdot\) <i>IEEE Transactions on Multimedia (TMM), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1803.02563" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/TZhang2019TMM.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Decoupled+Spatial+Neural+Attention+for+Weakly+Supervised+Semantic+Segmentation+Zhang,+Tianyi+and+Lin,+Guosheng+and+Cai,+Jianfei+and+Shen,+Tong+and+Shen,+Chunhua+and+Kot,+Alex+C." target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Decoupled+Spatial+Neural+Attention+for+Weakly+Supervised+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>RefineNet: multi-path refinement networks for dense prediction</b>   
<br />\(\cdot\) <i>G. Lin, F. Liu, A. Milan, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2019</i>.
<br />\(\cdot\) <a href="https://doi.org/10.1109/TPAMI.2019.2893630" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Fayao2019PAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={RefineNet}:+Multi-Path+Refinement+Networks+for+Dense+Prediction+Lin,+Guosheng+and+Liu,+Fayao+and+Milan,+Anton+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={RefineNet}:+Multi-Path+Refinement+Networks+for+Dense+Prediction" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/guosheng/refinenet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p>Pytorch code is <a href="https://github.com/DrSleep/refinenet-pytorch" target=&ldquo;blank&rdquo;>here</a>.
</p>
</li>
</ol>

</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1811.00751.pdf"><img class="imgP  right"   src="data/thumbnail/AAAI19LixxxarXiv.jpg"></a><b>Show, attend and read: a simple and strong baseline for irregular text recognition</b>   
<br />\(\cdot\) <i>H. Li, P. Wang, C. Shen, G. Zhang</i>.
<br />\(\cdot\) <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1811.00751" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/AAAI19Li.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Show,+attend+and+read:+a+simple+and+strong+baseline+for+irregular+text+recognition+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua+and+Zhang,+Guyu" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Show,+attend+and+read:+a+simple+and+strong+baseline+for+irregular+text+recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Deep hashing by discriminating hard examples</b>   
<br />\(\cdot\) <i>C. Yan, G. Pang, X. Bai, C. Shen, J. Zhou, E. Hancock</i>.
<br />\(\cdot\) <i>Proc. ACM International Conference on Multimedia (ACMMM&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/MM2019Yan.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Hashing+by+Discriminating+Hard+Examples+Yan,+Cheng+and+Pang,+Guansong+and+Bai,+Xiao+and+Shen,+Chunhua+and+Zhou,+Jun+and+Hancock,+Edwin" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Hashing+by+Discriminating+Hard+Examples" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1811.10413.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Zhuang3xxxarXiv.jpg"></a><b>Structured binary neural networks for accurate image classification and semantic segmentation</b>   
<br />\(\cdot\) <i>B. Zhuang, C. Shen, M. Tan, L. Liu, I. Reid</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1811.10413" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR19Zhuang3.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Binary+Neural+Networks+for+Accurate+Image+Classification+and+Semantic+Segmentation+Zhuang,+Bohan+and+Shen,+Chunhua+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+Binary+Neural+Networks+for+Accurate+Image+Classification+and+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://bitbucket.org/jingruixiaozhuang/group-net-image-classification/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR19Zhang6xxxPDF.jpg"><b>Mind your neighbours: image annotation with metadata neighbourhood graph co-attention networks</b>   
<br />\(\cdot\) <i>J. Zhang, Q. Wu, J. Zhang, C. Shen, J. Lu</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Mind_Your_Neighbours_Image_Annotation_With_Metadata_Neighbourhood_Graph_Co-Attention_CVPR_2019_paper.pdf" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/CVPR19Zhang6.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mind+Your+Neighbours:+Image+Annotation+with+Metadata+Neighbourhood+Graph+Co-Attention+Networks+Zhang,+Junjie+and+Wu,+Qi+and+Zhang,+Jian+and+Shen,+Chunhua+and+Lu,+Jianfeng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Mind+Your+Neighbours:+Image+Annotation+with+Metadata+Neighbourhood+Graph+Co-Attention+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.02351.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Zhang5xxxarXiv.jpg"></a><b>CANet: class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</b>   
<br />\(\cdot\) <i>C. Zhang, G. Lin, F. Liu, R. Yao, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1903.02351" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR19Zhang5.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={CANet}:+Class-Agnostic+Segmentation+Networks+with+Iterative+Refinement+and+Attentive+Few-Shot+Learning+Zhang,+Chi+and+Lin,+Guosheng+and+Liu,+Fayao+and+Yao,+Rui+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={CANet}:+Class-Agnostic+Segmentation+Networks+with+Iterative+Refinement+and+Attentive+Few-Shot+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1904.10293.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Yan9xxxarXiv.jpg"></a><b>Attention-guided network for ghost-free high dynamic range imaging</b>   
<br />\(\cdot\) <i>Q. Yan, D. Gong, Q. Shi, A. van den Hengel, C. Shen, I. Reid, Y. Zhang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1904.10293" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR19Yan9.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Attention-guided+Network+for+Ghost-free+High+Dynamic+Range+Imaging+Yan,+Qingsen+and+Gong,+Dong+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Reid,+Ian+and+Zhang,+Yanning" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Attention-guided+Network+for+Ghost-free+High+Dynamic+Range+Imaging" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1902.09852.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Wang4xxxarXiv.jpg"></a><b>Associatively segmenting instances and semantics in point clouds</b>   
<br />\(\cdot\) <i>X. Wang, S. Liu, X. Shen, C. Shen, J. Jia</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1902.09852" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR19Wang4.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Associatively+Segmenting+Instances+and+Semantics+in+Point+Clouds+Wang,+Xinlong+and+Liu,+Shu+and+Shen,+Xiaoyong+and+Shen,+Chunhua+and+Jia,+Jiaya" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Associatively+Segmenting+Instances+and+Semantics+in+Point+Clouds" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1812.04794.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19PengWang0xxxarXiv.jpg"></a><b>Neighbourhood watch: referring expression comprehension via language-guided graph attention networks</b>   
<br />\(\cdot\) <i>P. Wang, Q. Wu, J. Cao, C. Shen, L. Gao, A. vanden Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1812.04794" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR19PengWang0.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Neighbourhood+Watch:+Referring+Expression+Comprehension+via+Language-guided+Graph+Attention+Networks+Wang,+Peng+and+Wu,+Qi+and+Cao,+Jiewei+and+Shen,+Chunhua+and+Gao,+Lianli+and+{vanden+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Neighbourhood+Watch:+Referring+Expression+Comprehension+via+Language-guided+Graph+Attention+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.02120.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Tian7xxxarXiv.jpg"></a><b>Decoders matter for semantic segmentation: data-dependent decoding enables flexible feature aggregation</b>   
<br />\(\cdot\) <i>Z. Tian, T. He, C. Shen, Y. Yan</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1903.02120" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR19Tian7.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Decoders+Matter+for+Semantic+Segmentation:+Data-Dependent+Decoding+Enables+Flexible+Feature+Aggregation+Tian,+Zhi+and+He,+Tong+and+Shen,+Chunhua+and+Yan,+Youliang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Decoders+Matter+for+Semantic+Segmentation:+Data-Dependent+Decoding+Enables+Flexible+Feature+Aggregation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1810.10804.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Nekrasov1xxxarXiv.jpg"></a><b>Fast neural architecture search of compact semantic segmentation models via auxiliary cells</b>   
<br />\(\cdot\) <i>V. Nekrasov, H. Chen, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1810.10804" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR19Nekrasov1.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Neural+Architecture+Search+of+Compact+Semantic+Segmentation+Models+via+Auxiliary+Cells+Nekrasov,+Vladimir+and+Chen,+Hao+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+Neural+Architecture+Search+of+Compact+Semantic+Segmentation+Models+via+Auxiliary+Cells" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Visual question answering as reading comprehension</b>   
<br />\(\cdot\) <i>H. Li, P. Wang, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1811.11903" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR19HuiLi2.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+Question+Answering+as+Reading+Comprehension+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Visual+Question+Answering+as+Reading+Comprehension" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.04688.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19He8xxxarXiv.jpg"></a><b>Knowledge adaptation for efficient semantic segmentation</b>   
<br />\(\cdot\) <i>T. He, C. Shen, Z. Tian, D. Gong, C. Sun, Y. Yan</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1903.04688" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR19He8.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Knowledge+Adaptation+for+Efficient+Semantic+Segmentation+He,+Tong+and+Shen,+Chunhua+and+Tian,+Zhi+and+Gong,+Dong+and+Sun,+Changming+and+Yan,+Youliang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Knowledge+Adaptation+for+Efficient+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/PersonReID2019ZhangxxxarXiv.jpg"><b>Self-training with progressive augmentation for unsupervised cross-domain person re-identification</b>   
<br />\(\cdot\) <i>X. Zhang, J. Cao, C. Shen, M. You</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1907.13315" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/PersonReID2019Zhang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Self-Training+with+Progressive+Augmentation+for+Unsupervised+Cross-Domain+Person+Re-Identification+Zhang,+Xinyu+and+Cao,+Jiewei+and+Shen,+Chunhua+and+You,+Mingyu" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Self-Training+with+Progressive+Augmentation+for+Unsupervised+Cross-Domain+Person+Re-Identification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Temporal2019ZhangxxxarXiv.jpg"><b>Exploiting temporal consistency for real-time video depth estimation</b>   
<br />\(\cdot\) <i>H. Zhang, C. Shen, Y. Li, Y. Cao, Y. Liu, Y. Yan</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1908.03706" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Temporal2019Zhang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploiting+temporal+consistency+for+real-time+video+depth+estimation+Zhang,+Haokui+and+Shen,+Chunhua+and+Li,+Ying+and+Cao,+Yuanzhouhan+and+Liu,+Yu+and+Yan,+Youliang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Exploiting+temporal+consistency+for+real-time+video+depth+estimation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/VNL2019YinxxxarXiv.jpg"><b>Enforcing geometric constraints of virtual normal for depth prediction</b>   
<br />\(\cdot\) <i>W. Yin, Y. Liu, C. Shen, Y. Yan</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1907.12209" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/VNL2019Yin.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Enforcing+geometric+constraints+of+virtual+normal+for+depth+prediction+Yin,+Wei+and+Liu,+Yifan+and+Shen,+Chunhua+and+Yan,+Youliang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Enforcing+geometric+constraints+of+virtual+normal+for+depth+prediction" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/YvanYin/VNL_Monocular_Depth_Prediction" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/OpenSet2019XiongxxxarXiv.jpg"><b>From open set to closed set: counting objects by spatial divide-and-conquer</b>   
<br />\(\cdot\) <i>H. Xiong, H. Lu, C. Liu, L. Liu, Z. Cao, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1908.06473" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/OpenSet2019Xiong.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=From+Open+Set+to+Closed+Set:+Counting+Objects+by+Spatial+Divide-and-Conquer+Xiong,+Haipeng+and+Lu,+Hao+and+Liu,+Chengxin+and+Liu,+Liang+and+Cao,+Zhiguo+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=From+Open+Set+to+Closed+Set:+Counting+Objects+by+Spatial+Divide-and-Conquer" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/xhp-hust-2018-2011/S-DCNet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Efficient and accurate arbitrary-shaped text detection with pixel aggregation network</b>   
<br />\(\cdot\) <i>W. Wang, E. Xie, X. Song, Y. Zang, W. Wang, T. Lu, G. Yu, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/TextDet2019Wang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+and+Accurate+Arbitrary-Shaped+Text+Detection+with+Pixel+Aggregation+Network+Wang,+Wenhai+and+Xie,+Enze+and+Song,+Xiaoge+and+Zang,+Yuhang+and+Wang,+Wenjia+and+Lu,+Tong+and+Yu,+Gang+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+and+Accurate+Arbitrary-Shaped+Text+Detection+with+Pixel+Aggregation+Network" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1904.01355.pdf"><img class="imgP  right"   src="data/thumbnail/FCOS2019TianxxxarXiv.jpg"></a><b>FCOS: fully convolutional one-stage object detection</b>   
<br />\(\cdot\) <i>Z. Tian, C. Shen, H. Chen, T. He</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1904.01355" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/FCOS2019Tian.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FCOS}:+Fully+Convolutional+One-Stage+Object+Detection+Tian,+Zhi+and+Shen,+Chunhua+and+Chen,+Hao+and+He,+Tong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={FCOS}:+Fully+Convolutional+One-Stage+Object+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://tinyurl.com/FCOSv1" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Matting2019LuxxxarXiv.jpg"><b>Indices matter: learning to index for deep image matting</b>   
<br />\(\cdot\) <i>H. Lu, Y. Dai, C. Shen, S. Xu</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1908.00672" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Matting2019Lu.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Indices+Matter:+Learning+to+Index+for+Deep+Image+Matting+Lu,+Hao+and+Dai,+Yutong+and+Shen,+Chunhua+and+Xu,+Songcen" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Indices+Matter:+Learning+to+Index+for+Deep+Image+Matting" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Real-time joint semantic segmentation and depth estimation using asymmetric annotations</b>   
<br />\(\cdot\) <i>V. Nekrasov, T. Dharmasiri, A. Spek, T. Drummond, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>Proc. International Conference on Robotics and Automation (ICRA&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/ICRA19Nekrasov.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-Time+Joint+Semantic+Segmentation+and+Depth+Estimation+Using+Asymmetric+Annotations+Nekrasov,+Vladimir+and+Dharmasiri,+Thanuja+and+Spek,+Andrew+and+Drummond,+Tom+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Real-Time+Joint+Semantic+Segmentation+and+Depth+Estimation+Using+Asymmetric+Annotations" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Light-weight hybrid convolutional network for liver tumor segmentation</b>   
<br />\(\cdot\) <i>J. Zhang, Y. Xie, P. Zhang, H. Chen, Y. Xia, C. Shen</i>.
<br />\(\cdot\) <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/IJCAI19Zhang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Light-Weight+Hybrid+Convolutional+Network+for+Liver+Tumor+Segmentation+Zhang,+Jianpeng+and+Xie,+Yutong+and+Zhang,+Pingping+and+Chen,+Hao+and+Xia,+Yong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Light-Weight+Hybrid+Convolutional+Network+for+Liver+Tumor+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Deep anomaly detection with deviation networks</b>   
<br />\(\cdot\) <i>G. Pang, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/KDD19Pang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Anomaly+Detection+with+Deviation+Networks+Pang,+Guansong+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Anomaly+Detection+with+Deviation+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Deep segmentation-emendation model for gland instance segmentation</b>   
<br />\(\cdot\) <i>Y. Xie, H. Lu, J. Zhang, C. Shen, Y. Xia</i>.
<br />\(\cdot\) <i>Proc. International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="data/bibtex/MICCAI2019Xie.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Segmentation-Emendation+Model+for+Gland+Instance+Segmentation+Xie,+Yutong+and+Lu,+Hao+and+Zhang,+Jianpeng+and+Shen,+Chunhua+and+Xia,+Yong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Segmentation-Emendation+Model+for+Gland+Instance+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1911.00888.pdf"><img class="imgP  right"   src="data/thumbnail/Cao2019GANxxxarXiv.jpg"></a><b>Multi-marginal wasserstein GAN</b>   
<br />\(\cdot\) <i>J. Cao, L. Mo, Y. Zhang, K. Jia, C. Shen, M. Tan</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1911.00888" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Cao2019GAN.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-marginal+Wasserstein+{GAN}+Cao,+Jiezhang+and+Mo,+Langyuan+and+Zhang,+Yifan+and+Jia,+Kui+and+Shen,+Chunhua+and+Tan,+Mingkui" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Multi-marginal+Wasserstein+{GAN}" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Scale2019BianxxxarXiv.jpg"><b>Unsupervised scale-consistent depth and ego-motion learning from monocular video</b>   
<br />\(\cdot\) <i>J. Bian, Z. Li, N. Wang, H. Zhan, C. Shen, M. Cheng, I. Reid</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;19), 2019</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1908.10553" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Scale2019Bian.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Scale-consistent+Depth+and+Ego-motion+Learning+from+Monocular+Video+Bian,+Jia-Wang+and+Li,+Zhichao+and+Wang,+Naiyan+and+Zhan,+Huangying+and+Shen,+Chunhua+and+Cheng,+Ming-Ming+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Scale-consistent+Depth+and+Ego-motion+Learning+from+Monocular+Video" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/JiawangBian/SC-SfMLearner-Release" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
</ol>
<h1>2018</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><b>Cluster sparsity field: an internal hyperspectral imagery prior for reconstruction</b>   
<br />\(\cdot\) <i>L. Zhang, W. Wei, Y. Zhang, C. Shen, A. van den Hengel, Q. Shi</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2018</i>.
<br />\(\cdot\) <a href="https://www.researchgate.net/publication/323914969_Cluster_Sparsity_Field_An_Internal_Hyperspectral_Imagery_Prior_for_Reconstruction" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Zhang2018IJCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Cluster+Sparsity+Field:+An+Internal+Hyperspectral+Imagery+Prior+for+Reconstruction+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Cluster+Sparsity+Field:+An+Internal+Hyperspectral+Imagery+Prior+for+Reconstruction" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Li2018IVCxxxarXiv.jpg"><b>Reading car license plates using deep neural networks</b>   
<br />\(\cdot\) <i>H. Li, P. Wang, M. You, C. Shen</i>.
<br />\(\cdot\) <i>Image and Vision Computing (IVC), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1601.05610" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Li2018IVC.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Reading+Car+License+Plates+Using+Deep+Neural+Networks+Li,+Hui+and+Wang,+Peng+and+You,+Mingyu+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Reading+Car+License+Plates+Using+Deep+Neural+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhuang2018PRxxxarXiv.jpg"><b>Multi-label learning based deep transfer neural network for facial attribute classification</b>   
<br />\(\cdot\) <i>N. Zhuang, Y. Yan, S. Chen, H. Wang, C. Shen</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1805.01282" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhuang2018PR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-label+Learning+Based+Deep+Transfer+Neural+Network+for+Facial+Attribute+Classification+Zhuang,+Ni+and+Yan,+Yan+and+Chen,+Si+and+Wang,+Hanzi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Multi-label+Learning+Based+Deep+Transfer+Neural+Network+for+Facial+Attribute+Classification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wei2018PRxxxarXiv.jpg"><b>Unsupervised object discovery and co-localization by deep descriptor transforming</b>   
<br />\(\cdot\) <i>X. Wei, C. Zhang, J. Wu, C. Shen, Z. Zhou</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1707.06397" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wei2018PR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Object+Discovery+and+Co-Localization+by+Deep+Descriptor+Transforming+Wei,+Xiu-Shen+and+Zhang,+Chen-Lin+and+Wu,+Jianxin+and+Shen,+Chunhua+and+Zhou,+Zhi-Hua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Object+Discovery+and+Co-Localization+by+Deep+Descriptor+Transforming" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>An extended filtered channel framework for pedestrian detection</b>   
<br />\(\cdot\) <i>M. You, Y. Zhang, C. Shen, X. Zhang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2018</i>.
<br />\(\cdot\) <a href="data/bibtex/You2018T-ITS.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+extended+filtered+channel+framework+for+pedestrian+detection+You,+Minyu+and+Zhang,+Yubin+and+Shen,+Chunhua+and+Zhang,+Xinyu" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=An+extended+filtered+channel+framework+for+pedestrian+detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Towards end-to-end car license plates detection and recognition with deep neural networks</b>   
<br />\(\cdot\) <i>H. Li, P. Wang, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2018</i>.
<br />\(\cdot\) <a href="data/bibtex/Li2018T-ITSa.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+End-to-End+Car+License+Plates+Detection+and+Recognition+with+Deep+Neural+Networks+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+End-to-End+Car+License+Plates+Detection+and+Recognition+with+Deep+Neural+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Unsupervised domain adaptation using robust class-wise matching</b>   
<br />\(\cdot\) <i>L. Zhang, P. Wang, W. Wei, H. Lu, C. Shen, A. van den Hengel, Y. Zhang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhang2018TCSVT.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Domain+Adaptation+Using+Robust+Class-Wise+Matching+Zhang,+Lei+and+Wang,+Peng+and+Wei,+Wei+and+Lu,+Hao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Zhang,+Yanning" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Domain+Adaptation+Using+Robust+Class-Wise+Matching" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Semantics-aware visual object tracking</b>   
<br />\(\cdot\) <i>R. Yao, G. Lin, C. Shen, Y. Zhang, Q. Shi</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018</i>.
<br />\(\cdot\) <a href="data/bibtex/Yao2018TCSVT.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Semantics-Aware+Visual+Object+Tracking+Yao,+Rui+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+Shi,+Qinfeng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Semantics-Aware+Visual+Object+Tracking" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TCSVT2017HuxxxarXiv.jpg"><b>Pushing the limits of deep CNNs for pedestrian detection</b>   
<br />\(\cdot\) <i>Q. Hu, P. Wang, C. Shen, A. van den Hengel, F. Porikli</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1603.04525" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/TCSVT2017Hu.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Pushing+the+Limits+of+Deep+{CNNs}+for+Pedestrian+Detection+Hu,+Qichang+and+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Porikli,+Fatih" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Pushing+the+Limits+of+Deep+{CNNs}+for+Pedestrian+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>An embarrassingly simple approach to visual domain adaptation</b>   
<br />\(\cdot\) <i>H. Lu, C. Shen, Z. Cao, Y. Xiao, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2018</i>.
<br />\(\cdot\) <a href="data/bibtex/Lu2018TIP.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+Embarrassingly+Simple+Approach+to+Visual+Domain+Adaptation+Lu,+Hao+and+Shen,+Chunhua+and+Cao,+Zhiguo+and+Xiao,+Yang+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=An+Embarrassingly+Simple+Approach+to+Visual+Domain+Adaptation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/poppinace/ldada" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhang2018TMMxxxarXiv.jpg"><b>Multi-label image classification with regional latent semantic dependencies</b>   
<br />\(\cdot\) <i>J. Zhang, Q. Wu, C. Shen, J. Zhang, J. Lu</i>.
<br />\(\cdot\) <i>IEEE Transactions on Multimedia (TMM), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1612.01082" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhang2018TMM.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-Label+Image+Classification+with+Regional+Latent+Semantic+Dependencies+Zhang,+Junjie+and+Wu,+Qi+and+Shen,+Chunhua+and+Zhang,+Jian+and+Lu,+Jianfeng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Multi-Label+Image+Classification+with+Regional+Latent+Semantic+Dependencies" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1712.09048.pdf"><img class="imgP  right"   src="data/thumbnail/Guo2018TMMxxxarXiv.jpg"></a><b>Automatic image cropping for visual aesthetic enhancement using deep neural networks and cascaded regression</b>   
<br />\(\cdot\) <i>G. Guo, H. Wang, C. Shen, Y. Yan, H. Liao</i>.
<br />\(\cdot\) <i>IEEE Transactions on Multimedia (TMM), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1712.09048" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Guo2018TMM.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Automatic+image+cropping+for+visual+aesthetic+enhancement+using+deep+neural+networks+and+cascaded+regression+Guo,+Guanjun+and+Wang,+Hanzi+and+Shen,+Chunhua+and+Yan,+Yan+and+Liao,+Hong-Yuan" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Automatic+image+cropping+for+visual+aesthetic+enhancement+using+deep+neural+networks+and+cascaded+regression" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2017FVQAxxxarXiv.jpg"><b>FVQA: fact-based visual question answering</b>   
<br />\(\cdot\) <i>P. Wang, Q. Wu, C. Shen, A. Dick, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1606.05433" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wang2017FVQA.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FVQA}:+Fact-based+Visual+Question+Answering+Wang,+Peng+and+Wu,+Qi+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={FVQA}:+Fact-based+Visual+Question+Answering" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Ordinal constraint binary coding for approximate nearest neighbor search</b>   
<br />\(\cdot\) <i>H. Liu, R. Ji, J. Wang, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018</i>.
<br />\(\cdot\) <a href="https://www.researchgate.net/publication/324053386_Ordinal_Constraint_Binary_Coding_for_Approximate_Nearest_Neighbor_Search" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/HLiu2018TPAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Ordinal+Constraint+Binary+Coding+for+Approximate+Nearest+Neighbor+Search+Liu,+Hong+and+Ji,+Rongrong+and+Wang,+Jingdong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Ordinal+Constraint+Binary+Coding+for+Approximate+Nearest+Neighbor+Search" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>HCVRD: a benchmark for large-scale human-centered visual relationship detection</b>   
<br />\(\cdot\) <i>B. Zhuang, Q. Wu, C. Shen, I. Reid, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="data/bibtex/AAAI2018Zhuang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={HCVRD}:+a+benchmark+for+large-scale+Human-Centered+Visual+Relationship+Detection+Zhuang,+Bohan+and+Wu,+Qi+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={HCVRD}:+a+benchmark+for+large-scale+Human-Centered+Visual+Relationship+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Kill two birds with one stone: weakly-supervised neural network for image annotation and tag refinement</b>   
<br />\(\cdot\) <i>J. Zhang, Q. Wu, J. Zhang, C. Shen, J. Lu</i>.
<br />\(\cdot\) <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="data/bibtex/AAAI2018Zhang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Kill+Two+Birds+with+One+Stone:+Weakly-Supervised+Neural+Network+for+Image+Annotation+and+Tag+Refinement+Zhang,+Junjie+and+Wu,+Qi+and+Zhang,+Jian+and+Shen,+Chunhua+and+Lu,+Jianfeng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Kill+Two+Birds+with+One+Stone:+Weakly-Supervised+Neural+Network+for+Image+Annotation+and+Tag+Refinement" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Coarse-to-fine: a RNN-based hierarchical attention model for vehicle re-identification</b>   
<br />\(\cdot\) <i>X. Wei, C. Zhang, L. Liu, C. Shen, J. Wu</i>.
<br />\(\cdot\) <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="data/bibtex/ACCV18Wei.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Coarse-to-fine:+A+{RNN}-based+hierarchical+attention+model+for+vehicle+re-identification+Wei,+Xiu-Shen+and+Zhang,+Chen-Lin+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Wu,+Jianxin" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Coarse-to-fine:+A+{RNN}-based+hierarchical+attention+model+for+vehicle+re-identification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1807.03959.pdf"><img class="imgP  right"   src="data/thumbnail/ACCV18LixxxarXiv.jpg"></a><b>Deep attention-based classification network for robust depth prediction</b>   
<br />\(\cdot\) <i>R. Li, K. Xian, C. Shen, Z. Cao, H. Lu, L. Hang</i>.
<br />\(\cdot\) <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1807.03959" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ACCV18Li.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+attention-based+classification+network+for+robust+depth+prediction+Li,+Ruibo+and+Xian,+Ke+and+Shen,+Chunhua+and+Cao,+Zhiguo+and+Lu,+Hao+and+Hang,+Lingxiao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+attention-based+classification+network+for+robust+depth+prediction" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Light-weight refinenet for real-time semantic segmentation</b>   
<br />\(\cdot\) <i>V. Nekrasov, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>Proc. British Machine Vision Conference (BMVC&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="data/bibtex/BMVC18Nekrasov.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Light-Weight+RefineNet+for+Real-Time+Semantic+Segmentation+Nekrasov,+Vladimir+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Light-Weight+RefineNet+for+Real-Time+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/DrSleep/light-weight-refinenet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>A hybrid probabilistic model for camera relocalization</b>   
<br />\(\cdot\) <i>M. Cai, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>Proc. British Machine Vision Conference (BMVC&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="data/bibtex/BMVC18Cai.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Hybrid+Probabilistic+Model+for+Camera+Relocalization+Cai,+Ming+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Hybrid+Probabilistic+Model+for+Camera+Relocalization" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhuang2018CVPR_axxxarXiv.jpg"><b>Parallel attention: a unified framework for visual object discovery through dialogs and queries</b>   
<br />\(\cdot\) <i>B. Zhuang, Q. Wu, C. Shen, I. Reid, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1711.06370" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhuang2018CVPR_a.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Parallel+Attention:+A+Unified+Framework+for+Visual+Object+Discovery+through+Dialogs+and+Queries+Zhuang,+Bohan+and+Wu,+Qi+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Parallel+Attention:+A+Unified+Framework+for+Visual+Object+Discovery+through+Dialogs+and+Queries" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhuang2018CVPR_bxxxarXiv.jpg"><b>Towards effective low-bitwidth convolutional neural networks</b>   
<br />\(\cdot\) <i>B. Zhuang, C. Shen, M. Tan, L. Liu, I. Reid</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1711.00205" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhuang2018CVPR_b.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Effective+Low-bitwidth+Convolutional+Neural+Networks+Zhuang,+Bohan+and+Shen,+Chunhua+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+Effective+Low-bitwidth+Convolutional+Neural+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Monocular relative depth perception with web stereo data supervision</b>   
<br />\(\cdot\) <i>K. Xian, C. Shen, Z. Cao, H. Lu, Y. Xiao, R. Li, Z. Luo</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="data/bibtex/Xian2018CVPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Monocular+Relative+Depth+Perception+with+Web+Stereo+Data+Supervision+Xian,+Ke+and+Shen,+Chunhua+and+Cao,+Zhiguo+and+Lu,+Hao+and+Xiao,+Yang+and+Li,+Ruibo+and+Luo,+Zhenbo" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Monocular+Relative+Depth+Perception+with+Web+Stereo+Data+Supervision" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/QWu2018CVPRxxxarXiv.jpg"><b>Are you talking to me? reasoned visual dialog generation through adversarial learning</b>   
<br />\(\cdot\) <i>Q. Wu, P. Wang, C. Shen, I. Reid, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1711.07613" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/QWu2018CVPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Are+You+Talking+to+Me?+Reasoned+Visual+Dialog+Generation+through+Adversarial+Learning+Wu,+Qi+and+Wang,+Peng+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Are+You+Talking+to+Me?+Reasoned+Visual+Dialog+Generation+through+Adversarial+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wang2018CVPRxxxarXiv.jpg"><b>Repulsion loss: detecting pedestrians in a crowd</b>   
<br />\(\cdot\) <i>X. Wang, T. Xiao, Y. Jiang, S. Shao, J. Sun, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1711.07752" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wang2018CVPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Repulsion+Loss:+Detecting+Pedestrians+in+a+Crowd+Wang,+Xinlong+and+Xiao,+Tete+and+Jiang,+Yuning+and+Shao,+Shuai+and+Sun,+Jian+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Repulsion+Loss:+Detecting+Pedestrians+in+a+Crowd" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Others have implemented our paper: <a href="https://github.com/bailvwangzi/repulsion_loss_ssd" target=&ldquo;blank&rdquo;>Repulsion loss in SSD</a> and <a href="https://github.com/rainofmine/Repulsion_Loss" target=&ldquo;blank&rdquo;>Repulsion loss in RetinaNet</a>.
</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Song2018CVPRxxxarXiv.jpg"><b>VITAL: visual tracking via adversarial learning</b>   
<br />\(\cdot\) <i>Y. Song, C. Ma, X. Wu, L. Gong, L. Bao, W. Zuo, C. Shen, R. Lau, M. Yang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1804.04273" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Song2018CVPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={VITAL}:+VIsual+Tracking+via+Adversarial+Learning+Song,+Yibing+and+Ma,+Chao+and+Wu,+Xiaohe+and+Gong,+Lijun+and+Bao,+Linchao+and+Zuo,+Wangmeng+and+Shen,+Chunhua+and+Lau,+Rynson+and+Yang,+Ming-Hsuan" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={VITAL}:+VIsual+Tracking+via+Adversarial+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://ybsong00.github.io/cvpr18_tracking/index" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Bootstrapping the performance of webly supervised semantic segmentation</b>   
<br />\(\cdot\) <i>T. Shen, G. Lin, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="data/bibtex/TongShen2018CVPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Bootstrapping+the+Performance+of+Webly+Supervised+Semantic+Segmentation+Shen,+Tong+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Bootstrapping+the+Performance+of+Webly+Supervised+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/ascust/BDWSS" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Ma2018CVPR_axxxarXiv.jpg"><b>Visual question answering with memory-augmented networks</b>   
<br />\(\cdot\) <i>C. Ma, C. Shen, A. Dick, Q. Wu, P. Wang, A. van den Hengel, I. Reid</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1707.04968" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Ma2018CVPR_a.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+Question+Answering+with+Memory-Augmented+Networks+Ma,+Chao+and+Shen,+Chunhua+and+Dick,+Anthony+and+Wu,+Qi+and+Wang,+Peng+and+{van+den+Hengel},+Anton+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Visual+Question+Answering+with+Memory-Augmented+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/He2018CVPRxxxarXiv.jpg"><b>An end-to-end textspotter with explicit alignment and attention</b>   
<br />\(\cdot\) <i>T. He, Z. Tian, W. Huang, C. Shen, Y. Qiao, C. Sun</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1803.03474" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/He2018CVPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+end-to-end+TextSpotter+with+Explicit+Alignment+and+Attention+He,+Tong+and+Tian,+Zhi+and+Huang,+Weilin+and+Shen,+Chunhua+and+Qiao,+Yu+and+Sun,+Changming" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=An+end-to-end+TextSpotter+with+Explicit+Alignment+and+Attention" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/tonghe90/textspotter" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1711.10703.pdf"><img class="imgP  right"   src="data/thumbnail/Chen2018CVPRxxxarXiv.jpg"></a><b>FSRNet: end-to-end learning face super-resolution with facial priors</b>   
<br />\(\cdot\) <i>Y. Chen, Y. Tai, X. Liu, C. Shen, J. Yang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1711.10703" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Chen2018CVPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FSRNet}:+End-to-End+Learning+Face+Super-Resolution+with+Facial+Priors+Chen,+Yu+and+Tai,+Ying+and+Liu,+Xiaoming+and+Shen,+Chunhua+and+Yang,+Jian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={FSRNet}:+End-to-End+Learning+Face+Super-Resolution+with+Facial+Priors" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/tyshiwo/FSRNet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhang2018ECCVxxxarXiv.jpg"><b>Goal-oriented visual question generation via intermediate rewards</b>   
<br />\(\cdot\) <i>J. Zhang, Q. Wu, C. Shen, J. Zhang, J. Lu, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1711.07614" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhang2018ECCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Goal-Oriented+Visual+Question+Generation+via+Intermediate+Rewards+Zhang,+Junjie+and+Wu,+Qi+and+Shen,+Chunhua+and+Zhang,+Jian+and+Lu,+Jianfeng+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Goal-Oriented+Visual+Question+Generation+via+Intermediate+Rewards" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1807.10097.pdf"><img class="imgP  right"   src="data/thumbnail/Deng2018ECCVxxxarXiv.jpg"></a><b>Learning to predict crisp boundaries</b>   
<br />\(\cdot\) <i>R. Deng, C. Shen, S. Liu, H. Wang, X. Liu</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1807.10097" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Deng2018ECCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+to+Predict+Crisp+Boundaries+Deng,+Ruoxi+and+Shen,+Chunhua+and+Liu,+Shengjun+and+Wang,+Huibing+and+Liu,+Xinru" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+to+Predict+Crisp+Boundaries" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICASSP2018DongxxxarXiv.jpg"><b>Learning deep representations using convolutional auto-encoders with symmetric skip connections</b>   
<br />\(\cdot\) <i>L. Dong, Y. Gan, X. Mao, Y. Yang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1611.09119" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ICASSP2018Dong.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Deep+Representations+Using+Convolutional+Auto-Encoders+with+Symmetric+Skip+Connections+Dong,+Lian-Feng+and+Gan,+Yuan-Zhu+and+Mao,+Xiao-Liao+and+Yang,+Yu-Bin+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Deep+Representations+Using+Convolutional+Auto-Encoders+with+Symmetric+Skip+Connections" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1806.04895.pdf"><img class="imgP  right"   src="data/thumbnail/Cao2018ICMLxxxarXiv.jpg"></a><b>Adversarial learning with local coordinate coding</b>   
<br />\(\cdot\) <i>J. Cao, Y. Guo, Q. Wu, C. Shen, J. Huang, M. Tan</i>.
<br />\(\cdot\) <i>Proc. International Conference on Machine Learning (ICML&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1806.04895" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Cao2018ICML.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adversarial+Learning+with+Local+Coordinate+Coding+Cao,+Jiezhang+and+Guo,+Yong+and+Wu,+Qingyao+and+Shen,+Chunhua+and+Huang,+Junzhou+and+Tan,+Mingkui" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Adversarial+Learning+with+Local+Coordinate+Coding" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhang2018IJCAIxxxarXiv.jpg"><b>Salient object detection by lossless feature reflection</b>   
<br />\(\cdot\) <i>P. Zhang, W. Liu, H. Lu, C. Shen</i>.
<br />\(\cdot\) <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI&rsquo;18), 2018</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1802.06527" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhang2018IJCAI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Salient+Object+Detection+by+Lossless+Feature+Reflection+Zhang,+Pingping+and+Liu,+Wei+and+Lu,+Huchuan+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Salient+Object+Detection+by+Lossless+Feature+Reflection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h1>2017</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1607.05910.pdf"><img class="imgP  right"   src="data/thumbnail/CVIU2017VQAxxxarXiv.jpg"></a><b>Visual question answering: a survey of methods and datasets</b>   
<br />\(\cdot\) <i>Q. Wu, D. Teney, P. Wang, C. Shen, A. Dick, A. van den Hengel</i>.
<br />\(\cdot\) <i>Computer Vision and Image Understanding (CVIU), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1607.05910" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVIU2017VQA.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+question+answering:+A+survey+of+methods+and+datasets+Wu,+Qi+and+Teney,+Damien+and+Wang,+Peng+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Visual+question+answering:+A+survey+of+methods+and+datasets" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/IJCV2017LinxxxarXiv.jpg"><b>Structured learning of binary codes with column generation for optimizing ranking measures</b>   
<br />\(\cdot\) <i>G. Lin, F. Liu, C. Shen, J. Wu, H. Shen</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1602.06654" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/IJCV2017Lin.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Learning+of+Binary+Codes+with+Column+Generation+for+Optimizing+Ranking+Measures+Lin,+Guosheng+and+Liu,+Fayao+and+Shen,+Chunhua+and+Wu,+Jianxin+and+Shen,+Heng+Tao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+Learning+of+Binary+Codes+with+Column+Generation+for+Optimizing+Ranking+Measures" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://bitbucket.org/guosheng/structhash" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Removal of optically thick clouds from high-resolution satellite imagery using dictionary group learning and interdictionary nonlocal joint sparse coding</b>   
<br />\(\cdot\) <i>Y. Li, W. Li, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (JSTAEORS), 2017</i>.
<br />\(\cdot\) <a href="data/bibtex/Li2017Removal.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Removal+of+Optically+Thick+Clouds+From+High-resolution+Satellite+Imagery+Using+Dictionary+Group+Learning+and+Interdictionary+Nonlocal+Joint+Sparse+Coding+Li,+Ying+and+Li,+Wenbo+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Removal+of+Optically+Thick+Clouds+From+High-resolution+Satellite+Imagery+Using+Dictionary+Group+Learning+and+Interdictionary+Nonlocal+Joint+Sparse+Coding" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Lu2017CountingxxxarXiv.jpg"><b>TasselNet: counting maize tassels in the wild via local counts regression network</b>   
<br />\(\cdot\) <i>H. Lu, Z. Cao, Y. Xiao, B. Zhuang, C. Shen</i>.
<br />\(\cdot\) <i>Plant Methods (PLME), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1707.02290" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Lu2017Counting.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={TasselNet}:+Counting+maize+tassels+in+the+wild+via+local+counts+regression+network+Lu,+Hao+and+Cao,+Zhiguo+and+Xiao,+Yang+and+Zhuang,+Bohan+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={TasselNet}:+Counting+maize+tassels+in+the+wild+via+local+counts+regression+network" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wu2017PRxxxarXiv.jpg"><b>Deep linear discriminant analysis on Fisher networks: a hybrid architecture for person re-identification</b>   
<br />\(\cdot\) <i>L. Wu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1606.01595" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wu2017PR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Linear+Discriminant+Analysis+on+{F}isher+Networks:+A+Hybrid+Architecture+for+Person+Re-identification+Wu,+Lin+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Linear+Discriminant+Analysis+on+{F}isher+Networks:+A+Hybrid+Architecture+for+Person+Re-identification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Mask-CNN: localizing parts and selecting descriptors for bird species categorization</b>   
<br />\(\cdot\) <i>X. Wei, C. Xie, J. Wu, C. Shen</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2017</i>.
<br />\(\cdot\) <a href="data/bibtex/Wei2017PR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mask-{CNN}:+Localizing+parts+and+selecting+descriptors+for+bird+species+categorization+Wei,+Xiu-Shen+and+Xie,+Chen-Wei+and+Wu,+Jianxin+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Mask-{CNN}:+Localizing+parts+and+selecting+descriptors+for+bird+species+categorization" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/PR2017QiaoxxxarXiv.jpg"><b>Learning discriminative trajectorylet detector sets for accurate skeleton-based action recognition</b>   
<br />\(\cdot\) <i>R. Qiao, L. Liu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1504.04923" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/PR2017Qiao.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+discriminative+trajectorylet+detector+sets+for+accurate+skeleton-based+action+recognition+Qiao,+Ruizhi+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+discriminative+trajectorylet+detector+sets+for+accurate+skeleton-based+action+recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Deep CNNs with spatially weighted pooling for fine-grained car recognition</b>   
<br />\(\cdot\) <i>Q. Hu, H. Wang, T. Li, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2017</i>.
<br />\(\cdot\) <a href="data/bibtex/SWP2017Hu.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+{CNNs}+with+Spatially+Weighted+Pooling+for+Fine-grained+Car+Recognition+Hu,+Qichang+and+Wang,+Huibing+and+Li,+Teng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+{CNNs}+with+Spatially+Weighted+Pooling+for+Fine-grained+Car+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TCSVT2017ShengxxxarXiv.jpg"><b>Crowd counting via weighted VLAD on dense attribute feature maps</b>   
<br />\(\cdot\) <i>B. Sheng, C. Shen, G. Lin, J. Li, W. Yang, C. Sun</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1604.08660" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/TCSVT2017Sheng.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Crowd+Counting+via+Weighted+{VLAD}+on+Dense+Attribute+Feature+Maps+Sheng,+Biyun+and+Shen,+Chunhua+and+Lin,+Guosheng+and+Li,+Jun+and+Yang,+Wankou+and+Sun,+Changyin" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Crowd+Counting+via+Weighted+{VLAD}+on+Dense+Attribute+Feature+Maps" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1605.02305.pdf"><img class="imgP  right"   src="data/thumbnail/Cao2017xxxarXiv.jpg"></a><b>Estimating depth from monocular images as classification using deep fully convolutional residual networks</b>   
<br />\(\cdot\) <i>Y. Cao, Z. Wu, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1605.02305" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Cao2017.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Estimating+Depth+from+Monocular+Images+as+Classification+Using+Deep+Fully+Convolutional+Residual+Networks+Cao,+Yuanzhouhan+and+Wu,+Zifeng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Estimating+Depth+from+Monocular+Images+as+Classification+Using+Deep+Fully+Convolutional+Residual+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TIP2017LiuxxxarXiv.jpg"><b>Discriminative training of deep fully-connected continuous CRF with task-specific loss</b>   
<br />\(\cdot\) <i>F. Liu, G. Lin, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1601.07649" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/TIP2017Liu.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Discriminative+Training+of+Deep+Fully-connected+Continuous+{CRF}+with+Task-specific+Loss+Liu,+Fayao+and+Lin,+Guosheng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Discriminative+Training+of+Deep+Fully-connected+Continuous+{CRF}+with+Task-specific+Loss" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TIP2016CaoxxxarXiv.jpg"><b>Exploiting depth from single monocular images for object detection and semantic segmentation</b>   
<br />\(\cdot\) <i>Y. Cao, C. Shen, H. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1610.01706" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/TIP2016Cao.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploiting+Depth+from+Single+Monocular+Images+for+Object+Detection+and+Semantic+Segmentation+Cao,+Yuanzhouhan+and+Shen,+Chunhua+and+Shen,+Heng+Tao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Exploiting+Depth+from+Single+Monocular+Images+for+Object+Detection+and+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TNNLS2017LiuxxxarXiv.jpg"><b>Structured learning of tree potentials in CRF for image segmentation</b>   
<br />\(\cdot\) <i>F. Liu, G. Lin, R. Qiao, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1703.08764" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/TNNLS2017Liu.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Learning+of+Tree+Potentials+in+{CRF}+for+Image+Segmentation+Liu,+Fayao+and+Lin,+Guosheng+and+Qiao,+Ruizhi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+Learning+of+Tree+Potentials+in+{CRF}+for+Image+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Wu2017ExternalxxxarXiv.jpg"><b>Image captioning and visual question answering based on attributes and external knowledge</b>   
<br />\(\cdot\) <i>Q. Wu, C. Shen, P. Wang, A. Dick, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1603.02814" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Wu2017External.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Image+Captioning+and+Visual+Question+Answering+Based+on+Attributes+and+External+Knowledge+Wu,+Qi+and+Shen,+Chunhua+and+Wang,+Peng+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Image+Captioning+and+Visual+Question+Answering+Based+on+Attributes+and+External+Knowledge" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TPAMI2017LiuxxxarXiv.jpg"><b>Compositional model based Fisher vector coding for image classification</b>   
<br />\(\cdot\) <i>L. Liu, P. Wang, C. Shen, L. Wang, A. van den Hengel, C. Wang, H. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1601.04143" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/TPAMI2017Liu.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Compositional+Model+based+{F}isher+Vector+Coding+for+Image+Classification+Liu,+Lingqiao+and+Wang,+Peng+and+Shen,+Chunhua+and+Wang,+Lei+and+{van+den+Hengel},+Anton+and+Wang,+Chao+and+Shen,+Heng+Tao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Compositional+Model+based+{F}isher+Vector+Coding+for+Image+Classification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1510.00921.pdf"><img class="imgP  right"   src="data/thumbnail/Cross2017LiuxxxarXiv.jpg"></a><b>Cross-convolutional-layer pooling for image recognition</b>   
<br />\(\cdot\) <i>L. Liu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1510.00921" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7779086" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Cross2017Liu.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Cross-convolutional-layer+Pooling+for+Image+Recognition+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Cross-convolutional-layer+Pooling+for+Image+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Lin2017SemanticxxxarXiv.jpg"><b>Exploring context with deep structured models for semantic segmentation</b>   
<br />\(\cdot\) <i>G. Lin, C. Shen, A. van den Hengel, I. Reid</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1603.03183" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Lin2017Semantic.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploring+Context+with+Deep+Structured+models+for+Semantic+Segmentation+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Exploring+Context+with+Deep+Structured+models+for+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>Auxiliary tasks to improve trip hazard affordance detection</b>   
<br />\(\cdot\) <i>S. McMahon, T. Shen, N. Sunderhauf, I. Reid, C. Shen, M. Milford</i>.
<br />\(\cdot\) <i>Proc. Australasian Conference on Robotics and Automation (ACRA&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="data/bibtex/ACRA2017McMahon.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Auxiliary+Tasks+To+Improve+Trip+Hazard+Affordance+Detection+{McMahon},+Sean+and+Shen,+Tong+and+Sunderhauf,+Niko+and+Reid,+Ian+and+Shen,+Chunhua+and+Milford,+Michael" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Auxiliary+Tasks+To+Improve+Trip+Hazard+Affordance+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1701.07122.pdf"><img class="imgP  right"   src="data/thumbnail/BMVC2017TongxxxarXiv.jpg"></a><b>Weakly supervised semantic segmentation based on co-segmentation</b>   
<br />\(\cdot\) <i>T. Shen, G. Lin, L. Liu, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>Proc. British Machine Vision Conference (BMVC&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1701.07122" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/BMVC2017Tong.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Weakly+supervised+semantic+segmentation+based+on+co-segmentation+Shen,+Tong+and+Lin,+Guosheng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Weakly+supervised+semantic+segmentation+based+on+co-segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Visually aligned word embeddings for improving zero-shot learning</b>   
<br />\(\cdot\) <i>R. Qiao, L. Liu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. British Machine Vision Conference (BMVC&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="data/bibtex/BMVC17Zeroshot.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visually+Aligned+Word+Embeddings+for+Improving+Zero-shot+Learning+Qiao,+Ruizhi+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Visually+Aligned+Word+Embeddings+for+Improving+Zero-shot+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1611.09960.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017ZhuangxxxarXiv.jpg"></a><b>Attend in groups: a weakly-supervised deep learning framework for learning from web data</b>   
<br />\(\cdot\) <i>B. Zhuang, L. Liu, Y. Li, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1611.09960" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2017Zhuang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Attend+in+groups:+a+weakly-supervised+deep+learning+framework+for+learning+from+web+data+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Li,+Yao+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Attend+in+groups:+a+weakly-supervised+deep+learning+framework+for+learning+from+web+data" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1612.05386.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017WangVQAxxxarXiv.jpg"></a><b>The VQA-machine: learning how to use existing vision algorithms to answer new questions</b>   
<br />\(\cdot\) <i>P. Wang, Q. Wu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1612.05386" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2017WangVQA.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=The+{VQA}-Machine:+Learning+How+to+Use+Existing+Vision+Algorithms+to+Answer+New+Questions+Wang,+Peng+and+Wu,+Qi+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=The+{VQA}-Machine:+Learning+How+to+Use+Existing+Vision+Algorithms+to+Answer+New+Questions" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR2017WangAttendxxxPDF.jpg"><b>Multi-attention network for one shot learning</b>   
<br />\(\cdot\) <i>P. Wang, L. Liu, C. Shen, Z. Huang, A. van den Hengel, H. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Multi-Attention_Network_for_CVPR_2017_paper.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/CVPR2017WangAttend.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-attention+Network+for+One+Shot+Learning+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Huang,+Zi+and+{van+den+Hengel},+Anton+and+Shen,+Heng+Tao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Multi-attention+Network+for+One+Shot+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1611.06612.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017LinxxxarXiv.jpg"></a><b>RefineNet: multi-path refinement networks for high-resolution semantic segmentation</b>   
<br />\(\cdot\) <i>G. Lin, A. Milan, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1611.06612" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2017Lin.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={RefineNet}:+Multi-Path+Refinement+Networks+for+High-Resolution+Semantic+Segmentation+Lin,+Guosheng+and+Milan,+Anton+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={RefineNet}:+Multi-Path+Refinement+Networks+for+High-Resolution+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/guosheng/refinenet" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p><a href="https://github.com/DrSleep/light-weight-refinenet" target=&ldquo;blank&rdquo;>Light-weight RefineNet with Pytorch code</a>.
</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1611.09967.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017YaoLixxxarXiv.jpg"></a><b>Sequential person recognition in photo albums with a recurrent network</b>   
<br />\(\cdot\) <i>Y. Li, G. Lin, B. Zhuang, L. Liu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1611.09967" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2017YaoLi.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Sequential+Person+Recognition+in+Photo+Albums+with+a+Recurrent+Network+Li,+Yao+and+Lin,+Guosheng+and+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Sequential+Person+Recognition+in+Photo+Albums+with+a+Recurrent+Network" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1612.02583.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017GongxxxarXiv.jpg"></a><b>From motion blur to motion flow: a deep learning solution for removing heterogeneous motion blur</b>   
<br />\(\cdot\) <i>D. Gong, J. Yang, L. Liu, Y. Zhang, I. Reid, C. Shen, A. van den Hengel, Q. Shi</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1612.02583" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR2017Gong.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=From+Motion+Blur+to+Motion+Flow:+a+Deep+Learning+Solution+for+Removing+Heterogeneous+Motion+Blur+Gong,+Dong+and+Yang,+Jie+and+Liu,+Lingqiao+and+Zhang,+Yanning+and+Reid,+Ian+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=From+Motion+Blur+to+Motion+Flow:+a+Deep+Learning+Solution+for+Removing+Heterogeneous+Motion+Blur" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV2017ZhuangxxxarXiv.jpg"><b>Towards context-aware interaction recognition</b>   
<br />\(\cdot\) <i>B. Zhuang, L. Liu, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1703.06246" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ICCV2017Zhuang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Context-aware+Interaction+Recognition+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+Context-aware+Interaction+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>When unsupervised domain adaptation meets tensor representations</b>   
<br />\(\cdot\) <i>H. Lu, L. Zhang, Z. Cao, W. Wei, K. Xian, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="data/bibtex/ICCV2017Haolu.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=When+Unsupervised+Domain+Adaptation+Meets+Tensor+Representations+Lu,+Hao+and+Zhang,+Lei+and+Cao,+Zhiguo+and+Wei,+Wei+and+Xian,+Ke+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=When+Unsupervised+Domain+Adaptation+Meets+Tensor+Representations" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV2017WeiLiuxxxarXiv.jpg"><b>Semi-global weighted least squares in image filtering</b>   
<br />\(\cdot\) <i>W. Liu, X. Chen, C. Shen, Z. Liu, J. Yang</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1705.01674" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ICCV2017WeiLiu.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Semi-Global+Weighted+Least+Squares+in+Image+Filtering+Liu,+Wei+and+Chen,+Xiaogang+and+Shen,+Chuanhua+and+Liu,+Zhi+and+Yang,+Jie" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Semi-Global+Weighted+Least+Squares+in+Image+Filtering" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV2017HuiLixxxarXiv.jpg"><b>Towards end-to-end text spotting with convolutional recurrent neural networks</b>   
<br />\(\cdot\) <i>H. Li, P. Wang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1707.03985" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ICCV2017HuiLi.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+End-to-end+Text+Spotting+with+Convolutional+Recurrent+Neural+Networks+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+End-to-end+Text+Spotting+with+Convolutional+Recurrent+Neural+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV2017ChenxxxarXiv.jpg"><b>Adversarial PoseNet: a structure-aware convolutional network for human pose estimation</b>   
<br />\(\cdot\) <i>Y. Chen, C. Shen, X. Wei, L. Liu, J. Yang</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1705.00389" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ICCV2017Chen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adversarial+{PoseNet}:+A+Structure-aware+Convolutional+Network+for+Human+Pose+Estimation+Chen,+Yu+and+Shen,+Chunhua+and+Wei,+Xiu-Shen+and+Liu,+Lingqiao+and+Yang,+Jian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Adversarial+{PoseNet}:+A+Structure-aware+Convolutional+Network+for+Human+Pose+Estimation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICRA2017ChenxxxarXiv.jpg"><b>Deep learning features at scale for visual place recognition</b>   
<br />\(\cdot\) <i>Z. Chen, A. Jacobson, N. Sunderhauf, B. Upcroft, L. Liu, C. Shen, I. Reid, M. Milford</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Robotics and Automation (ICRA&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1701.05105" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ICRA2017Chen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Learning+Features+at+Scale+for+Visual+Place+Recognition+Chen,+Zetao+and+Jacobson,+Adam+and+Sunderhauf,+Niko+and+Upcroft,+Ben+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Reid,+Ian+and+Milford,+Michael" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Learning+Features+at+Scale+for+Visual+Place+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/IJCAI2017WeixxxarXiv.jpg"><b>Deep descriptor transforming for image co-localization</b>   
<br />\(\cdot\) <i>X. Wei, C. Zhang, Y. Li, C. Xie, J. Wu, C. Shen, Z. Zhou</i>.
<br />\(\cdot\) <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1705.02758" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/IJCAI2017Wei.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Descriptor+Transforming+for+Image+Co-Localization+Wei,+Xiu-Shen+and+Zhang,+Chen-Lin+and+Li,+Yao+and+Xie,+Chen-Wei+and+Wu,+Jianxin+and+Shen,+Chunhua+and+Zhou,+Zhi-Hua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Descriptor+Transforming+for+Image+Co-Localization" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/IJCAI2017WangxxxarXiv.jpg"><b>Explicit knowledge-based reasoning for visual question answering</b>   
<br />\(\cdot\) <i>P. Wang, Q. Wu, C. Shen, A. van den Hengel, A. Dick</i>.
<br />\(\cdot\) <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1511.02570" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/IJCAI2017Wang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Explicit+Knowledge-based+Reasoning+for+Visual+Question+Answering+Wang,+Peng+and+Wu,+Qi+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Dick,+Anthony" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Explicit+Knowledge-based+Reasoning+for+Visual+Question+Answering" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/IJCAI2017TongxxxarXiv.jpg"><b>Learning multi-level region consistency with dense multi-label networks for semantic segmentation</b>   
<br />\(\cdot\) <i>T. Shen, G. Lin, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI&rsquo;17), 2017</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1701.07122" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/IJCAI2017Tong.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Multi-level+Region+Consistency+with+Dense+Multi-label+Networks+for+Semantic+Segmentation+Shen,+Tong+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Multi-level+Region+Consistency+with+Dense+Multi-label+Networks+for+Semantic+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h1>2016</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1511.08531.pdf"><img class="imgP  right"   src="data/thumbnail/CVIU2016xxxarXiv.jpg"></a><b>Structured learning of metric ensembles with application to person re-identification</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, L. Wu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Computer Vision and Image Understanding (CVIU), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1511.08531" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVIU2016.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+learning+of+metric+ensembles+with+application+to+person+re-identification+Paisitkriangkrai,+Sakrapee+and+Wu,+Lin+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+learning+of+metric+ensembles+with+application+to+person+re-identification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zhang2015IJCVxxxarXiv.jpg"><b>Unsupervised feature learning for dense correspondences across scenes</b>   
<br />\(\cdot\) <i>C. Zhang, C. Shen, T. Shen</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1501.00642" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Zhang2015IJCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Feature+Learning+for+Dense+Correspondences+across+Scenes+Zhang,+Chao+and+Shen,+Chunhua+and+Shen,+Tingzhi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Feature+Learning+for+Dense+Correspondences+across+Scenes" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://bitbucket.org/chhshen/ufl" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1404.5009.pdf"><img class="imgP  right"   src="data/thumbnail/BnB2015WangxxxarXiv.jpg"></a><b>Efficient semidefinite branch-and-cut for MAP-MRF inference</b>   
<br />\(\cdot\) <i>P. Wang, C. Shen, A. van den Hengel, P. Torr</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1404.5009" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://doi.org/10.1007/s11263-015-0865-2" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/BnB2015Wang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Semidefinite+Branch-and-Cut+for+{MAP-MRF}+Inference+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Torr,+Philip" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+Semidefinite+Branch-and-Cut+for+{MAP-MRF}+Inference" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Yao2016IJCVxxxarXiv.jpg"><b>Mining mid-level visual patterns with deep CNN activations</b>   
<br />\(\cdot\) <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1506.06343" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://rdcu.be/j1mA" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Yao2016IJCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mining+Mid-level+Visual+Patterns+with+Deep+{CNN}+Activations+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Mining+Mid-level+Visual+Patterns+with+Deep+{CNN}+Activations" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/yaoliUoA/MDPM" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2016TrackingxxxarXiv.jpg"><b>Online unsupervised feature learning for visual tracking</b>   
<br />\(\cdot\) <i>F. Liu, C. Shen, I. Reid, A. van den Hengel</i>.
<br />\(\cdot\) <i>Image and Vision Computing (IVC), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1310.1690" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Liu2016Tracking.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Online+Unsupervised+Feature+Learning+for+Visual+Tracking+Liu,+Fayao+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Online+Unsupervised+Feature+Learning+for+Visual+Tracking" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Canonical principal angles correlation analysis for two-view data</b>   
<br />\(\cdot\) <i>S. Wang, J. Lu, X. Gu, C. Shen, R. Xia, J. Yang</i>.
<br />\(\cdot\) <i>Journal of Visual Communication and Image Representation (JVCIR), 2016</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1016/j.jvcir.2015.12.001" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Canonical2016Wang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Canonical+principal+angles+correlation+analysis+for+two-view+data+Wang,+Sheng+and+Lu,+Jianfeng+and+Gu,+Xingjian+and+Shen,+Chunhua+and+Xia,+Rui+and+Yang,+Jingyu" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Canonical+principal+angles+correlation+analysis+for+two-view+data" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/PRFace2016ShenxxxarXiv.jpg"><b>Face image classification by pooling raw features</b>   
<br />\(\cdot\) <i>F. Shen, C. Shen, X. Zhou, Y. Yang, H. Shen</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1406.6811" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/PRFace2016Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Face+Image+Classification+by+Pooling+Raw+Features+Shen,+Fumin+and+Shen,+Chunhua+and+Zhou,+Xiang+and+Yang,+Yang+and+Shen,+Heng+Tao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Face+Image+Classification+by+Pooling+Raw+Features" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/bd622/FacePooling" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1110.0264.pdf"><img class="imgP  right"   src="data/thumbnail/Face2016LixxxarXiv.jpg"></a><b>Face recognition using linear representation ensembles</b>   
<br />\(\cdot\) <i>H. Li, F. Shen, C. Shen, Y. Yang, Y. Gao</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1110.0264" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1016/j.patcog.2015.12.011" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Face2016Li.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Face+Recognition+Using+Linear+Representation+Ensembles+Li,+Hanxi+and+Shen,+Fumin+and+Shen,+Chunhua+and+Yang,+Yang+and+Gao,+Yongsheng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Face+Recognition+Using+Linear+Representation+Ensembles" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Fast detection of multiple objects in traffic scenes with a common detection framework</b>   
<br />\(\cdot\) <i>Q. Hu, S. Paisitkriangkrai, C. Shen, A. van den Hengel, F. Porikli</i>.
<br />\(\cdot\) <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2016</i>.
<br />\(\cdot\) <a href="data/bibtex/Hu2015T-ITS.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Detection+of+Multiple+Objects+in+Traffic+Scenes+with+a+Common+Detection+Framework+Hu,+Qichang+and+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Porikli,+Fatih" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+Detection+of+Multiple+Objects+in+Traffic+Scenes+with+a+Common+Detection+Framework" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Part-based robust tracking using online latent structured learning</b>   
<br />\(\cdot\) <i>R. Yao, Q. Shi, C. Shen, Y. Zhang, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2016</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/TCSVT.2016.2527358" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Part2016Yao.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Part-based+robust+tracking+using+online+latent+structured+learning+Yao,+Rui+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Part-based+robust+tracking+using+online+latent+structured+learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Pooling2016WangxxxarXiv.jpg"><b>Temporal pyramid pooling based convolutional neural network for action recognition</b>   
<br />\(\cdot\) <i>P. Wang, Y. Cao, C. Shen, L. Liu, H. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1503.01224" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Pooling2016Wang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Temporal+Pyramid+Pooling+Based+Convolutional+Neural+Network+for+Action+Recognition+Wang,+Peng+and+Cao,+Yuanzhouhan+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Shen,+Heng+Tao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Temporal+Pyramid+Pooling+Based+Convolutional+Neural+Network+for+Action+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Dictionary learning for promoting structured sparsity in hyerpsectral compressive sensing</b>   
<br />\(\cdot\) <i>L. Zhang, W. Wei, Y. Zhang, C. Shen, A. van den Hengel, Q. Shi</i>.
<br />\(\cdot\) <i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2016</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhang2016TGSE.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Dictionary+Learning+for+Promoting+Structured+Sparsity+in+Hyerpsectral+Compressive+Sensing+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Dictionary+Learning+for+Promoting+Structured+Sparsity+in+Hyerpsectral+Compressive+Sensing" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Scalable linear visual feature learning via online parallel nonnegative matrix factorization</b>   
<br />\(\cdot\) <i>X. Zhao, X. Li, Z. Zhang, C. Shen, L. Gao, X. Li</i>.
<br />\(\cdot\) <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2016</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/TNNLS.2015.2499273" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Zhao2015TNN.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Scalable+Linear+Visual+Feature+Learning+via+Online+Parallel+Nonnegative+Matrix+Factorization+Zhao,+Xueyi+and+Li,+Xi+and+Zhang,+Zhongfei+and+Shen,+Chunhua+and+Gao,+Lixin+and+Li,+Xuelong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Scalable+Linear+Visual+Feature+Learning+via+Online+Parallel+Nonnegative+Matrix+Factorization" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Large-scale binary quadratic optimization using semidefinite relaxation and applications</b>   
<br />\(\cdot\) <i>P. Wang, C. Shen, A. van den Hengel, P. Torr</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1411.7564" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2016.2541146" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/BQP2015Wang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Large-scale+Binary+Quadratic+Optimization+Using+Semidefinite+Relaxation+and+Applications+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Torr,+Philip+H.+S." target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Large-scale+Binary+Quadratic+Optimization+Using+Semidefinite+Relaxation+and+Applications" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Paisitkriangkrai2015TPAMIxxxarXiv.jpg"><b>Pedestrian detection with spatially pooled features and structured ensemble learning</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1409.5209" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://doi.org/10.1109/TPAMI.2015.2474388" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Paisitkriangkrai2015TPAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Pedestrian+Detection+with+Spatially+Pooled+Features+and+Structured+Ensemble+Learning+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Pedestrian+Detection+with+Spatially+Pooled+Features+and+Structured+Ensemble+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/chhshen/pedestrian-detection" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2015TPAMIxxxarXiv.jpg"><b>A generalized probabilistic framework for compact codebook creation</b>   
<br />\(\cdot\) <i>L. Liu, L. Wang, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1401.7713" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://doi.org/10.1109/TPAMI.2015.2441069" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Liu2015TPAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Generalized+Probabilistic+Framework+for+Compact+Codebook+Creation+Liu,+Lingqiao+and+Wang,+Lei+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Generalized+Probabilistic+Framework+for+Compact+Codebook+Creation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Depth2015LiuxxxarXiv.jpg"><b>Learning depth from single monocular images using deep convolutional neural fields</b>   
<br />\(\cdot\) <i>F. Liu, C. Shen, G. Lin, I. Reid</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1502.07411" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2015.2505283" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Depth2015Liu.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Depth+from+Single+Monocular+Images+Using+Deep+Convolutional+Neural+Fields+Liu,+Fayao+and+Shen,+Chunhua+and+Lin,+Guosheng+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Depth+from+Single+Monocular+Images+Using+Deep+Convolutional+Neural+Fields" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="http://goo.gl/rAKWrS" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Xi2015TPAMIxxxarXiv.jpg"><b>Online metric-weighted linear representations for robust visual tracking</b>   
<br />\(\cdot\) <i>X. Li, C. Shen, A. Dick, Z. Zhang, Y. Zhuang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1507.05737" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Xi2015TPAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Online+Metric-Weighted+Linear+Representations+for+Robust+Visual+Tracking+Li,+Xi+and+Shen,+Chunhua+and+Dick,+Anthony+and+Zhang,+Zhongfei+and+Zhuang,+Yueting" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Online+Metric-Weighted+Linear+Representations+for+Robust+Visual+Tracking" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1603.02844.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16BinaryxxxarXiv.jpg"></a><b>Fast training of triplet-based deep binary embedding networks</b>   
<br />\(\cdot\) <i>B. Zhuang, G. Lin, C. Shen, I. Reid</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1603.02844" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR16Binary.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Training+of+Triplet-based+Deep+Binary+Embedding+Networks+Zhuang,+Bohan+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+Training+of+Triplet-based+Deep+Binary+Embedding+Networks" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://bitbucket.org/jingruixiaozhuang/fast-training-of-triplet-based-deep-binary-embedding-networks" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1511.06973.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16AMAxxxarXiv.jpg"></a><b>Ask me anything: free-form visual question answering based on knowledge from external sources</b>   
<br />\(\cdot\) <i>Q. Wu, P. Wang, C. Shen, A. Dick, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1511.06973" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR16AMA.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Ask+Me+Anything:+Free-form+Visual+Question+Answering+Based+on+Knowledge+from+External+Sources+Wu,+Qi+and+Wang,+Peng+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Ask+Me+Anything:+Free-form+Visual+Question+Answering+Based+on+Knowledge+from+External+Sources" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1506.01144.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16WhatxxxarXiv.jpg"></a><b>What value do explicit high level concepts have in vision to language problems</b>   
<br />\(\cdot\) <i>Q. Wu, C. Shen, L. Liu, A. Dick, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1506.01144" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR16What.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=What+Value+Do+Explicit+High+Level+Concepts+Have+in+Vision+to+Language+Problems+Wu,+Qi+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=What+Value+Do+Explicit+High+Level+Concepts+Have+in+Vision+to+Language+Problems" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1602.04422.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16IrregularxxxarXiv.jpg"></a><b>What's wrong with that object? identifying irregular object from images by modelling the detection score distribution</b>   
<br />\(\cdot\) <i>P. Wang, L. Liu, C. Shen, Z. Huang, A. van den Hengel, H. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1602.04422" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR16Irregular.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=What's+Wrong+with+that+Object?+Identifying+Irregular+Object+From+Images+by+Modelling+the+Detection+Score+Distribution+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Huang,+Zi+and+{van+den+Hengel},+Anton+and+Shen,+Heng+Tao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=What's+Wrong+with+that+Object?+Identifying+Irregular+Object+From+Images+by+Modelling+the+Detection+Score+Distribution" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1604.01146.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16ZeroshotxxxarXiv.jpg"></a><b>Less is more: zero-shot learning from online textual documents with noise suppression</b>   
<br />\(\cdot\) <i>R. Qiao, L. Liu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1604.01146" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR16Zeroshot.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Less+is+More:+Zero-shot+Learning+from+Online+Textual+Documents+with+Noise+Suppression+Qiao,+Ruizhi+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Less+is+More:+Zero-shot+Learning+from+Online+Textual+Documents+with+Noise+Suppression" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1504.01013.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16labellingxxxarXiv.jpg"></a><b>Efficient piecewise training of deep structured models for semantic segmentation</b>   
<br />\(\cdot\) <i>G. Lin, C. Shen, A. van dan Hengel, I. Reid</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1504.01013" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR16labelling.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+piecewise+training+of+deep+structured+models+for+semantic+segmentation+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+dan+Hengel},+Anton+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+piecewise+training+of+deep+structured+models+for+semantic+segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Cluster sparsity field for hyperspectral imagery denoising</b>   
<br />\(\cdot\) <i>L. Zhang, W. Wei, Y. Zhang, C. Shen, A. van den Hengel, Q. Shi</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;16), 2016</i>.
<br />\(\cdot\) <a href="data/bibtex/ECCV16hyperspectral.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Cluster+Sparsity+Field+for+Hyperspectral+Imagery+Denoising+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Cluster+Sparsity+Field+for+Hyperspectral+Imagery+Denoising" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ECCV16LixxxarXiv.jpg"><b>Image co-localization by mimicking a good detector's confidence score distribution</b>   
<br />\(\cdot\) <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;16), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1603.04619" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ECCV16Li.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Image+Co-localization+by+Mimicking+a+Good+Detector's+Confidence+Score+Distribution+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Image+Co-localization+by+Mimicking+a+Good+Detector's+Confidence+Score+Distribution" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/NeurIPS2016xxxPDF.jpg"><b>Image restoration using very deep fully convolutional encoder-decoder networks with symmetric skip connections</b>   
<br />\(\cdot\) <i>X. Mao, C. Shen, Y. Yang</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;16), 2016</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1603.09056" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://papers.NeurIPS.cc/paper/6172-image-restoration-using-very-deep-convolutional-encoder-decoder-networks-with-symmetric-skip-connections.pdf" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/NeurIPS2016.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Image+Restoration+Using+Very+Deep+Fully+Convolutional+Encoder-Decoder+Networks+with+Symmetric+Skip+Connections+Mao,+Xiao-Jiao+and+Shen,+Chunhua+and+Yang,+Yu-Bin" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Image+Restoration+Using+Very+Deep+Fully+Convolutional+Encoder-Decoder+Networks+with+Symmetric+Skip+Connections" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://bitbucket.org/chhshen/image-denoising/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p>Others have <a href="https://github.com/titu1994/Image-Super-Resolution" target=&ldquo;blank&rdquo;>implemented our paper</a>.
</p>
</li>
</ol>

</li>
</ol>
<h1>2015</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1401.8126.pdf"><img class="imgP  right"   src="data/thumbnail/Harandi2015IJCVxxxarXiv.jpg"></a><b>Extrinsic methods for coding and dictionary learning on Grassmann manifolds</b>   
<br />\(\cdot\) <i>M. Harandi, R. Hartley, C. Shen, B. Lovell, C. Sanderson</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2015</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1401.8126" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Harandi2015IJCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Extrinsic+Methods+for+Coding+and+Dictionary+Learning+on+{G}rassmann+Manifolds+Harandi,+Mehrtash+and+Hartley,+Richard+and+Shen,+Chunhua+and+Lovell,+Brian+and+Sanderson,+Conrad" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Extrinsic+Methods+for+Coding+and+Dictionary+Learning+on+{G}rassmann+Manifolds" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/chhshen/Grassmann/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2015CRFPRxxxarXiv.jpg"><b>CRF learning with CNN features for image segmentation</b>   
<br />\(\cdot\) <i>F. Liu, G. Lin, C. Shen</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2015</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1503.08263" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Liu2015CRFPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={CRF}+Learning+with+{CNN}+Features+for+Image+Segmentation+Liu,+Fayao+and+Lin,+Guosheng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={CRF}+Learning+with+{CNN}+Features+for+Image+Segmentation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Hashing2015ShenxxxarXiv.jpg"><b>Hashing on nonlinear manifolds</b>   
<br />\(\cdot\) <i>F. Shen, C. Shen, Q. Shi, A. van den Hengel, Z. Tang, H. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2015</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1412.0826" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Hashing2015Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Hashing+on+Nonlinear+Manifolds+Shen,+Fumin+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin+and+Shen,+Heng+Tao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Hashing+on+Nonlinear+Manifolds" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/chhshen/Hashing-on-Nonlinear-Manifolds" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TIP2014ShortcutxxxarXiv.jpg"><b>A computational model of the short-cut rule for 2D shape decomposition</b>   
<br />\(\cdot\) <i>L. Luo, C. Shen, X. Liu, C. Zhang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2015</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1409.2104" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/TIP2014Shortcut.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Computational+Model+of+the+Short-Cut+Rule+for+{2D}+Shape+Decomposition+Luo,+Lei+and+Shen,+Chunhua+and+Liu,+Xinwang+and+Zhang,+Chunyuan" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Computational+Model+of+the+Short-Cut+Rule+for+{2D}+Shape+Decomposition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/SDP2015LixxxarXiv.jpg"><b>Worst-case linear discriminant analysis as scalable semidefinite feasibility problems</b>   
<br />\(\cdot\) <i>H. Li, C. Shen, A. van den Hengel, Q. Shi</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2015</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1411.7450" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/SDP2015Li.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Worst-Case+Linear+Discriminant+Analysis+as+Scalable+Semidefinite+Feasibility+Problems+Li,+Hui+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Worst-Case+Linear+Discriminant+Analysis+as+Scalable+Semidefinite+Feasibility+Problems" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/chhshen/SDP-WLDA" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1408.5574.pdf"><img class="imgP  right"   src="data/thumbnail/FastHash2015LinxxxarXiv.jpg"></a><b>Supervised hashing using graph cuts and boosted decision trees</b>   
<br />\(\cdot\) <i>G. Lin, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2015</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1408.5574" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2015.2404776" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/FastHash2015Lin.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Supervised+Hashing+Using+Graph+Cuts+and+Boosted+Decision+Trees+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Supervised+Hashing+Using+Graph+Cuts+and+Boosted+Decision+Trees" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://bitbucket.org/chhshen/fasthash/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1504.01492.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15exxxarXiv.jpg"></a><b>Efficient SDP inference for fully-connected CRFs based on low-rank decomposition</b>   
<br />\(\cdot\) <i>P. Wang, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1504.01492" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR15e.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+{SDP}+Inference+for+Fully-connected+{CRFs}+Based+on+Low-rank+Decomposition+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+{SDP}+Inference+for+Fully-connected+{CRFs}+Based+on+Low-rank+Decomposition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR15gxxxPDF.jpg"><b>Learning graph structure for multi-label image classification via clique generation</b>   
<br />\(\cdot\) <i>M. Tan, Q. Shi, A. van den Hengel, C. Shen, J. Gao, F. Hu, Z. Zhang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br />\(\cdot\) <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tan_Learning_Graph_Structure_2015_CVPR_paper.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/CVPR15g.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Graph+Structure+for+Multi-label+Image+Classification+via+Clique+Generation+Tan,+Mingkui+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Gao,+Junbin+and+Hu,+Fuyuan+and+Zhang,+Zhen" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Graph+Structure+for+Multi-label+Image+Classification+via+Clique+Generation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR15cxxxPDF.jpg"><b>Supervised discrete hashing</b>   
<br />\(\cdot\) <i>F. Shen, C. Shen, W. Liu, H. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br />\(\cdot\) <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Shen_Supervised_Discrete_Hashing_2015_CVPR_paper.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/CVPR15c.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Supervised+Discrete+Hashing+Shen,+Fumin+and+Shen,+Chunhua+and+Liu,+Wei+and+Shen,+Heng+Tao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Supervised+Discrete+Hashing" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/bd622/DiscretHashing/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1503.01543.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15fxxxarXiv.jpg"></a><b>Learning to rank in person re-identification with metric ensembles</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1503.01543" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR15f.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+to+rank+in+person+re-identification+with+metric+ensembles+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+to+rank+in+person+re-identification+with+metric+ensembles" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.7466.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15dxxxarXiv.jpg"></a><b>The treasure beneath convolutional layers: cross convolutional layer pooling for image classification</b>   
<br />\(\cdot\) <i>L. Liu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1411.7466" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR15d.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=The+Treasure+beneath+Convolutional+Layers:+Cross+convolutional+layer+Pooling+for+Image+Classification+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=The+Treasure+beneath+Convolutional+Layers:+Cross+convolutional+layer+Pooling+for+Image+Classification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.6387.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15bxxxarXiv.jpg"></a><b>Deep convolutional neural fields for depth estimation from a single image</b>   
<br />\(\cdot\) <i>F. Liu, C. Shen, G. Lin</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1411.6387" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR15b.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Convolutional+Neural+Fields+for+Depth+Estimation+from+a+Single+Image+Liu,+Fayao+and+Shen,+Chunhua+and+Lin,+Guosheng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Convolutional+Neural+Fields+for+Depth+Estimation+from+a+Single+Image" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="http://goo.gl/rAKWrS" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Mid-level deep pattern mining</b>   
<br />\(\cdot\) <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1411.6382" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR15a.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mid-level+Deep+Pattern+Mining+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Mid-level+Deep+Pattern+Mining" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/yaoliUoA/MDPM" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR15hxxxPDF.jpg"><b>Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs</b>   
<br />\(\cdot\) <i>B. Li, C. Shen, Y. Dai, A. van den Hengel, M. He</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br />\(\cdot\) <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Li_Depth_and_Surface_2015_CVPR_paper.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/CVPR15h.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Depth+and+Surface+Normal+Estimation+from+Monocular+Images+Using+Regression+on+Deep+Features+and+Hierarchical+{CRFs}+Li,+Bo+and+Shen,+Chunhua+and+Dai,+Yuchao+and+{van+den+Hengel},+Anton+and+He,+Mingyi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Depth+and+Surface+Normal+Estimation+from+Monocular+Images+Using+Regression+on+Deep+Features+and+Hierarchical+{CRFs}" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV15ZhangxxxPDF.jpg"><b>Hyperspectral compressive sensing using manifold-structured sparsity prior</b>   
<br />\(\cdot\) <i>L. Zhang, W. Wei, Y. Zhang, F. Li, C. Shen, Q. Shi</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;15), 2015</i>.
<br />\(\cdot\) <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zhang_Hyperspectral_Compressive_Sensing_ICCV_2015_paper.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/ICCV15Zhang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Hyperspectral+Compressive+Sensing+Using+Manifold-Structured+Sparsity+Prior+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Li,+Fei+and+Shen,+Chunhua+and+Shi,+Qinfeng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Hyperspectral+Compressive+Sensing+Using+Manifold-Structured+Sparsity+Prior" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/NeurIPS15LinxxxPDF.jpg"><b>Deeply learning the messages in message passing inference</b>   
<br />\(\cdot\) <i>G. Lin, C. Shen, I. Reid, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;15), 2015</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1506.02108" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://papers.NeurIPS.cc/paper/5791-deeply-learning-the-messages-in-message-passing-inference.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/NeurIPS15Lin.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deeply+Learning+the+Messages+in+Message+Passing+Inference+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deeply+Learning+the+Messages+in+Message+Passing+Inference" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR15workshopxxxPDF.jpg"><b>Sequence searching with deep-learnt depth for condition- and viewpoint-invariant route-based place recognition</b>   
<br />\(\cdot\) <i>M. Milford, C. Shen, S. Lowry, N. Suenderhauf, S. Shirazi, G. Lin, F. Liu, E. Pepperell, C. Lerma, B. Upcroft, I. Reid</i>.
<br />\(\cdot\) <i>Proc. 6th International Workshop on Computer Vision in Vehicle Technology, in conjunction with IEEE Conference on Computer Vision and Pattern Recognition (CVVT&rsquo;15), 2015</i>.
<br />\(\cdot\) <a href="http://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W11/papers/Milford_Sequence_Searching_With_2015_CVPR_paper.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/CVPR15workshop.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Sequence+searching+with+Deep-Learnt+Depth+for+Condition-+and+Viewpoint-Invariant+Route-Based+Place+Recognition+Milford,+Michael+and+Shen,+Chunhua+and+Lowry,+Stephanie+and+Suenderhauf,+Niko+and+Shirazi,+Sareh+and+Lin,+Guosheng+and+Liu,+Fayao+and+Pepperell,+Edward+and+Lerma,+Cesar+and+Upcroft,+Ben+and+Reid,+Ian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Sequence+searching+with+Deep-Learnt+Depth+for+Condition-+and+Viewpoint-Invariant+Route-Based+Place+Recognition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Best paper award (Sponsored by NVIDIA).
</p>
</li>
</ol>

</li>
</ol>
<h1>2014</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2014OutlierxxxarXiv.jpg"><b>Fast approximate \(l_\infty\) minimization: Speeding up robust regression</b>   
<br />\(\cdot\) <i>F. Shen, C. Shen, R. Hill, A. van den Hengel, Z. Tang</i>.
<br />\(\cdot\) <i>Computational Statistics and Data Analysis (CSDA), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1304.1250" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Shen2014Outlier.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+approximate+L_\infty+minimization:+{S}peeding+up+robust+regression+Shen,+Fumin+and+Shen,+Chunhua+and+Hill,+Rhys+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+approximate+L_\infty+minimization:+{S}peeding+up+robust+regression" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2014MKLxxxarXiv.jpg"><b>Multiple kernel learning in the primal for multi-modal Alzheimer's disease classification</b>   
<br />\(\cdot\) <i>F. Liu, L. Zhou, C. Shen, J. Yin</i>.
<br />\(\cdot\) <i>IEEE Journal of Biomedical and Health Informatics (JBHI), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1310.0890" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/JBHI.2013.2285378" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Liu2014MKL.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multiple+Kernel+Learning+in+the+Primal+for+Multi-modal+{A}lzheimer's+Disease+Classification+Liu,+Fayao+and+Zhou,+Luping+and+Shen,+Chunhua+and+Yin,+Jianping" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Multiple+Kernel+Learning+in+the+Primal+for+Multi-modal+{A}lzheimer's+Disease+Classification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Online published at IEEE: 10 October 2013.
</p>
</li></ol>
</li>
<li><p><b>Multiple kernel clustering based on centered kernel alignment</b>   
<br />\(\cdot\) <i>Y. Lu, L. Wang, J. Lu, J. Yang, C. Shen</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2014</i>.
<br />\(\cdot\) <a href="data/bibtex/MKL2014.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multiple+kernel+clustering+based+on+centered+kernel+alignment+Lu,+Yanting+and+Wang,+Liantao+and+Lu,+Jianfeng+and+Yang,+Jingyu+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Multiple+kernel+clustering+based+on+centered+kernel+alignment" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Yan2014TIPaxxxarXiv.jpg"><b>Efficient semidefinite spectral clustering via Lagrange duality</b>   
<br />\(\cdot\) <i>Y. Yan, C. Shen, H. Wang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1402.5497" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Yan2014TIPa.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Semidefinite+Spectral+Clustering+via+{L}agrange+Duality+Yan,+Yan+and+Shen,+Chunhua+and+Wang,+Hanzi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+Semidefinite+Spectral+Clustering+via+{L}agrange+Duality" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Paul2014TIPbxxxarXiv.jpg"><b>Large-margin learning of compact binary image encodings</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1402.6383" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Paul2014TIPb.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Large-margin+Learning+of+Compact+Binary+Image+Encodings+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Large-margin+Learning+of+Compact+Binary+Image+Encodings" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Li2014TIPxxxarXiv.jpg"><b>Characterness: An indicator of text in the wild</b>   
<br />\(\cdot\) <i>Y. Li, W. Jia, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1309.6691" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TIP.2014.2302896" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Li2014TIP.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Characterness:+{A}n+Indicator+of+Text+in+the+Wild+Li,+Yao+and+Jia,+Wenjing+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Characterness:+{A}n+Indicator+of+Text+in+the+Wild" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/yaoliUoA/characterness" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Li2013HyperxxxarXiv.jpg"><b>Context-aware hypergraph construction for robust spectral clustering</b>   
<br />\(\cdot\) <i>X. Li, W. Hu, C. Shen, A. Dick, Z. Zhang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Knowledge and Data Engineering (TKDE), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1401.0764" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://doi.ieeecomputersociety.org/10.1109/TKDE.2013.126" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Li2013Hyper.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Context-aware+hypergraph+construction+for+robust+spectral+clustering+Li,+Xi+and+Hu,+Weiming+and+Shen,+Chunhua+and+Dick,+Anthony+and+Zhang,+Zhongfei" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Context-aware+hypergraph+construction+for+robust+spectral+clustering" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Paul2013TMMxxxarXiv.jpg"><b>Asymmetric pruning for learning cascade detectors</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Multimedia (TMM), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1303.6066" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TMM.2014.2308723" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Paul2013TMM.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Asymmetric+pruning+for+learning+cascade+detectors+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Asymmetric+pruning+for+learning+cascade+detectors" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2014MetricxxxarXiv.jpg"><b>Efficient dual approach to distance metric learning</b>   
<br />\(\cdot\) <i>C. Shen, J. Kim, F. Liu, L. Wang, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1302.3219" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Shen2014Metric.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Dual+Approach+to+Distance+Metric+Learning+Shen,+Chunhua+and+Kim,+Junae+and+Liu,+Fayao+and+Wang,+Lei+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+Dual+Approach+to+Distance+Metric+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Paul2013FastboostingxxxPDF.jpg"><b>A scalable stage-wise approach to large-margin multi-class loss based boosting</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1307.5497" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TNNLS.2013.2282369" target=&ldquo;blank&rdquo;>link</a><a href="https://bytebucket.org/chhshen/data/raw/7e2f958b104603e54e9d8376a8e1672363f742a3/papers/Paisitkriangkrai2014TNNLS.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Paul2013Fastboosting.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+scalable+stage-wise+approach+to+large-margin+multi-class+loss+based+boosting+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+scalable+stage-wise+approach+to+large-margin+multi-class+loss+based+boosting" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Paisitkriangkrai2013RandomBoostxxxarXiv.jpg"><b>RandomBoost: Simplified multi-class boosting through randomization</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, Q. Shi, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1302.0963" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TNNLS.2013.2281214" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Paisitkriangkrai2013RandomBoost.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={RandomBoost}:+{S}implified+Multi-class+Boosting+through+Randomization+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={RandomBoost}:+{S}implified+Multi-class+Boosting+through+Randomization" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>A hierarchical word-merging algorithm with class separability measure</b>   
<br />\(\cdot\) <i>L. Wang, L. Zhou, C. Shen, L. Liu, H. Liu</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014</i>.
<br />\(\cdot\) <a href="https://bitbucket.org/chhshen/chhshen.bitbucket.org/src/be12d4ef8deb6207ec97f0fdac6efbe2df151b59/_download/TPAMI14Wang.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Wang2014PAMI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Hierarchical+Word-merging+Algorithm+with+Class+Separability+Measure+Wang,+Lei+and+Zhou,+Luping+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Liu,+Huan" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Hierarchical+Word-merging+Algorithm+with+Class+Separability+Measure" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2014SBoostingxxxarXiv.jpg"><b>StructBoost: Boosting methods for predicting structured output variables</b>   
<br />\(\cdot\) <i>C. Shen, G. Lin, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1302.3283" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2014.2315792" target=&ldquo;blank&rdquo;>link</a><a href="http://goo.gl/goCVLK" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Shen2014SBoosting.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={StructBoost}:+{B}oosting+Methods+for+Predicting+Structured+Output+Variables+Shen,+Chunhua+and+Lin,+Guosheng+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={StructBoost}:+{B}oosting+Methods+for+Predicting+Structured+Output+Variables" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1404.1561.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR14LinxxxarXiv.jpg"></a><b>Fast supervised hashing with decision trees for high-dimensional data</b>   
<br />\(\cdot\) <i>G. Lin, C. Shen, Q. Shi, A. van den Hengel, D. Suter</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;14), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1404.1561" target=&ldquo;blank&rdquo;>arXiv</a><a href="https://bitbucket.org/chhshen/fasthash/src" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/CVPR14Lin.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Supervised+Hashing+with+Decision+Trees+for+High-Dimensional+Data+Lin,+Guosheng+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Suter,+David" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+Supervised+Hashing+with+Decision+Trees+for+High-Dimensional+Data" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://bitbucket.org/chhshen/fasthash/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1407.0786.pdf"><img class="imgP  right"   src="data/thumbnail/ECCV14PaulxxxarXiv.jpg"></a><b>Strengthening the effectiveness of pedestrian detection with spatially pooled features</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;14), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1407.0786" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ECCV14Paul.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Strengthening+the+Effectiveness+of+Pedestrian+Detection+with+Spatially+Pooled+Features+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Strengthening+the+Effectiveness+of+Pedestrian+Detection+with+Spatially+Pooled+Features" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/chhshen/pedestrian-detection" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1407.1151.pdf"><img class="imgP  right"   src="data/thumbnail/ECCV14LinxxxarXiv.jpg"></a><b>Optimizing ranking measures for compact binary code learning</b>   
<br />\(\cdot\) <i>G. Lin, C. Shen, J. Wu</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;14), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1407.1151" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ECCV14Lin.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Optimizing+Ranking+Measures+for+Compact+Binary+Code+Learning+Lin,+Guosheng+and+Shen,+Chunhua+and+Wu,+Jianxin" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Optimizing+Ranking+Measures+for+Compact+Binary+Code+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://bitbucket.org/guosheng/structhash" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Liu2014FisherxxxarXiv.jpg"><b>Encoding high dimensional local features by sparse coding based Fisher vectors</b>   
<br />\(\cdot\) <i>L. Liu, C. Shen, L. Wang, A. van den Hengel, C. Wang</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;14), 2014</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1411.6406" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Liu2014Fisher.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Encoding+High+Dimensional+Local+Features+by+Sparse+Coding+Based+{F}isher+Vectors+Liu,+Lingqiao+and+Shen,+Chunhua+and+Wang,+Lei+and+{van+den+Hengel},+Anton+and+Wang,+Chao" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Encoding+High+Dimensional+Local+Features+by+Sparse+Coding+Based+{F}isher+Vectors" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h1>2013</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1301.2032.pdf"><img class="imgP  right"   src="data/thumbnail/FisherBoost2013IJCVxxxarXiv.jpg"></a><b>Training effective node classifiers for cascade classification</b>   
<br />\(\cdot\) <i>C. Shen, P. Wang, S. Paisitkriangkrai, A. van den Hengel</i>.
<br />\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2013</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1301.2032" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://link.springer.com/article/10.1007%2Fs11263-013-0608-1" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/FisherBoost2013IJCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Training+Effective+Node+Classifiers+for+Cascade+Classification+Shen,+Chunhua+and+Wang,+Peng+and+Paisitkriangkrai,+Sakrapee+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Training+Effective+Node+Classifiers+for+Cascade+Classification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Fully corrective boosting with arbitrary loss and regularization</b>   
<br />\(\cdot\) <i>C. Shen, H. Li, A. van den Hengel</i>.
<br />\(\cdot\) <i>Neural Networks (NN), 2013</i>.
<br />\(\cdot\) <a href="http://hdl.handle.net/2440/78929" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Shen2013NN.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fully+Corrective+Boosting+with+Arbitrary+Loss+and+Regularization+Shen,+Chunhua+and+Li,+Hanxi+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fully+Corrective+Boosting+with+Arbitrary+Loss+and+Regularization" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Approximate least trimmed sum of squares fitting and applications in image analysis</b>   
<br />\(\cdot\) <i>F. Shen, C. Shen, A. van den Hengel, Z. Tang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2013</i>.
<br />\(\cdot\) <a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6408142" target=&ldquo;blank&rdquo;>link</a><a href="http://hdl.handle.net/2440/79428" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/LMS2013TIP.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Approximate+Least+Trimmed+Sum+of+Squares+Fitting+and+Applications+in+Image+Analysis+Shen,+Fumin+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Approximate+Least+Trimmed+Sum+of+Squares+Fitting+and+Applications+in+Image+Analysis" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Visual tracking with spatio-temporal Dempster-Shafer information fusion</b>   
<br />\(\cdot\) <i>X. Li, A. Dick, C. Shen, Z. Zhang, A. van den Hengel, H. Wang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2013</i>.
<br />\(\cdot\) <a href="http://hdl.handle.net/2440/77448" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Xi2013TIP.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+Tracking+with+Spatio-Temporal+{Dempster-Shafer}+Information+Fusion+Li,+Xi+and+Dick,+Anthony+and+Shen,+Chunhua+and+Zhang,+Zhongfei+and+{van+den+Hengel},+Anton+and+Wang,+Hanzi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Visual+Tracking+with+Spatio-Temporal+{Dempster-Shafer}+Information+Fusion" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Xi2013SurveyxxxarXiv.jpg"><b>A survey of appearance models in visual object tracking</b>   
<br />\(\cdot\) <i>X. Li, W. Hu, C. Shen, Z. Zhang, A. Dick, A. van den Hengel</i>.
<br />\(\cdot\) <i>ACM Transactions on Intelligent Systems and Technology (TIST), 2013</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1303.4803" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/Xi2013Survey.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Survey+of+Appearance+Models+in+Visual+Object+Tracking+Li,+Xi+and+Hu,+Weiming+and+Shen,+Chunhua+and+Zhang,+Zhongfei+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Survey+of+Appearance+Models+in+Visual+Object+Tracking" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Shape similarity analysis by self-tuning locally constrained mixed-diffusion</b>   
<br />\(\cdot\) <i>L. Luo, C. Shen, C. Zhang, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Multimedia (TMM), 2013</i>.
<br />\(\cdot\) <a href="http://hdl.handle.net/2440/73304" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/TMM2013Shape.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Shape+Similarity+Analysis+by+Self-Tuning+Locally+Constrained+Mixed-Diffusion+Luo,+Lei+and+Shen,+Chunhua+and+Zhang,+Chunyuan+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Shape+Similarity+Analysis+by+Self-Tuning+Locally+Constrained+Mixed-Diffusion" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TPAMI2013XixxxarXiv.jpg"><b>Incremental learning of 3D-DCT compact representations for robust visual tracking</b>   
<br />\(\cdot\) <i>X. Li, A. Dick, C. Shen, A. van den Hengel, H. Wang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2013</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1207.3389" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2012.166" target=&ldquo;blank&rdquo;>link</a><a href="https://sites.google.com/site/chhshen/publication/tpami12xi.pdf?attredirects=1" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/TPAMI2013Xi.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Incremental+Learning+of+{3D-DCT}+Compact+Representations+for+Robust+Visual+Tracking+Li,+Xi+and+Dick,+Anthony+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Wang,+Hanzi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Incremental+Learning+of+{3D-DCT}+Compact+Representations+for+Robust+Visual+Tracking" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/chhshen/DCT-Tracking/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR13eYaoxxxPDF.jpg"><b>Part-based visual tracking with online latent structural learning</b>   
<br />\(\cdot\) <i>R. Yao, Q. Shi, C. Shen, Y. Zhang, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="http://hdl.handle.net/2440/77413" target=&ldquo;blank&rdquo;>link</a><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yao_Part-Based_Visual_Tracking_2013_CVPR_paper.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/CVPR13eYao.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Part-based+Visual+Tracking+with+Online+Latent+Structural+Learning+Yao,+Rui+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Part-based+Visual+Tracking+with+Online+Latent+Structural+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/chhshen/PartTracking" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR13cWangxxxPDF.jpg"><b>Bilinear programming for human activity recognition with unknown MRF graphs</b>   
<br />\(\cdot\) <i>Z. Wang, Q. Shi, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="http://hdl.handle.net/2440/77411" target=&ldquo;blank&rdquo;>link</a><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_Bilinear_Programming_for_2013_CVPR_paper.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/CVPR13cWang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Bilinear+Programming+for+Human+Activity+Recognition+with+unknown+{MRF}+graphs+Wang,+Zhenhua+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Bilinear+Programming+for+Human+Activity+Recognition+with+unknown+{MRF}+graphs" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1304.0840.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR13dWangxxxarXiv.jpg"></a><b>A fast semidefinite approach to solving binary quadratic problems</b>   
<br />\(\cdot\) <i>P. Wang, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1304.0840" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR13dWang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Fast+Semidefinite+Approach+to+Solving+Binary+Quadratic+Problems+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Fast+Semidefinite+Approach+to+Solving+Binary+Quadratic+Problems" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="./projects/BQP/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p>Oral presentation, 60 out of 1870 submissions.
</p>
</li></ol>
</li>
<li><p><b>Inductive hashing on manifolds</b>   
<br />\(\cdot\) <i>F. Shen, C. Shen, Q. Shi, A. van den Hengel, Z. Tang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1303.7043" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/CVPR13aShen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Inductive+Hashing+on+Manifolds+Shen,+Fumin+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Inductive+Hashing+on+Manifolds" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/chhshen/Hashing-on-Nonlinear-Manifolds" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR13bLixxxPDF.jpg"><b>Learning compact binary codes for visual tracking</b>   
<br />\(\cdot\) <i>X. Li, C. Shen, A. Dick, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="http://hdl.handle.net/2440/77412" target=&ldquo;blank&rdquo;>link</a><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Learning_Compact_Binary_2013_CVPR_paper.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/CVPR13bLi.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Compact+Binary+Codes+for+Visual+Tracking+Li,+Xi+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Compact+Binary+Codes+for+Visual+Tracking" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV2013HarandixxxarXiv.jpg"><b>Dictionary learning and sparse coding on Grassmann manifolds: an extrinsic solution</b>   
<br />\(\cdot\) <i>M. Harandi, C. Sanderson, C. Shen, B. Lovell</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1310.4891" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ICCV2013Harandi.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Dictionary+Learning+and+Sparse+Coding+on+{G}rassmann+Manifolds:+An+Extrinsic+Solution+{Harandi},+Mehrtash+and+{Sanderson},+Conrad+and+Shen,+Chunhua+and+Lovell,+Brian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Dictionary+Learning+and+Sparse+Coding+on+{G}rassmann+Manifolds:+An+Extrinsic+Solution" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/chhshen/Grassmann/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV13PaixxxarXiv.jpg"><b>Efficient pedestrian detection by directly optimizing the partial area under the ROC curve</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1310.0900" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://hdl.handle.net/2440/83158" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/ICCV13Pai.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+pedestrian+detection+by+directly+optimizing+the+partial+area+under+the+{ROC}+curve+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+pedestrian+detection+by+directly+optimizing+the+partial+area+under+the+{ROC}+curve" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV13LinxxxarXiv.jpg"><b>A general two-step approach to learning-based hashing</b>   
<br />\(\cdot\) <i>G. Lin, C. Shen, D. Suter, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1309.1853" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ICCV13Lin.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+General+Two-step+Approach+to+Learning-Based+Hashing+Lin,+Guosheng+and+Shen,+Chunhua+and+Suter,+David+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+General+Two-step+Approach+to+Learning-Based+Hashing" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://bitbucket.org/guosheng/two-step-hashing/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Contextual hypergraph modeling for salient object detection</b>   
<br />\(\cdot\) <i>X. Li, Y. Li, C. Shen, A. Dick, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1310.5767" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ICCV13Li.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Contextual+Hypergraph+Modeling+for+Salient+Object+Detection+Li,+Xi+and+Li,+Yao+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Contextual+Hypergraph+Modeling+for+Salient+Object+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://bitbucket.org/chhshen/saliency-detection" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Extended depth-of-field via focus stacking and graph cuts</b>   
<br />\(\cdot\) <i>C. Zhang, J. Bastian, C. Shen, A. van den Hengel, T. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Image Processing (ICIP&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="data/bibtex/ICIP13cShen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Extended+depth-of-field+via+focus+stacking+and+graph+cuts+Zhang,+Chao+and+Bastian,+John+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shen,+Tingzhi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Extended+depth-of-field+via+focus+stacking+and+graph+cuts" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Approximate constraint generation for efficient structured boosting</b>   
<br />\(\cdot\) <i>G. Lin, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Image Processing (ICIP&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="data/bibtex/ICIP13aShen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Approximate+constraint+generation+for+efficient+structured+boosting+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Approximate+constraint+generation+for+efficient+structured+boosting" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Leveraging surrounding context for scene text detection</b>   
<br />\(\cdot\) <i>Y. Li, C. Shen, W. Jia, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Image Processing (ICIP&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="data/bibtex/ICIP13bShen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Leveraging+surrounding+context+for+scene+text+detection+Li,+Yao+and+Shen,+Chunhua+and+Jia,+Wenjing+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Leveraging+surrounding+context+for+scene+text+detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICML13axxxPDF.jpg"><b>Learning hash functions using column generation</b>   
<br />\(\cdot\) <i>X. Li, G. Lin, C. Shen, A. van den Hengel, A. Dick</i>.
<br />\(\cdot\) <i>Proc. International Conference on Machine Learning (ICML&rsquo;13), 2013</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1303.0339" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://jmlr.csail.mit.edu/proceedings/papers/v28/li13a.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/ICML13a.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Hash+Functions+Using+Column+Generation+Li,+Xi+and+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Dick,+Anthony" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Hash+Functions+Using+Column+Generation" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://bitbucket.org/guosheng/column-generation-hashing/" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li>
</ol>

</li>
</ol>
<h1>2012</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><img class="imgP  right"   src="data/thumbnail/JMLR2012ShenxxxarXiv.jpg"><b>Positive semidefinite metric learning using boosting-like algorithms</b>   
<br />\(\cdot\) <i>C. Shen, J. Kim, L. Wang, A. van den Hengel</i>.
<br />\(\cdot\) <i>Journal of Machine Learning Research (JMLR), 2012</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1104.4704" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://jmlr.csail.mit.edu/papers/v13/shen12a.html" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/JMLR2012Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Positive+Semidefinite+Metric+Learning+Using+Boosting-like+Algorithms+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Positive+Semidefinite+Metric+Learning+Using+Boosting-like+Algorithms" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://code.google.com/archive/p/boosting/downloads" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Fast and robust object detection using asymmetric totally-corrective boosting</b>   
<br />\(\cdot\) <i>P. Wang, C. Shen, N. Barnes, H. Zheng</i>.
<br />\(\cdot\) <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2012</i>.
<br />\(\cdot\) <a href="http://hdl.handle.net/2440/66763" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/AsymBoost2011Wang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+and+Robust+Object+Detection+Using+Asymmetric+Totally-corrective+Boosting+Wang,+Peng+and+Shen,+Chunhua+and+Barnes,+Nick+and+Zheng,+Hong" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+and+Robust+Object+Detection+Using+Asymmetric+Totally-corrective+Boosting" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>UBoost: Boosting with the Universum</b>   
<br />\(\cdot\) <i>C. Shen, P. Wang, F. Shen, H. Wang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2012</i>.
<br />\(\cdot\) <a href="http://hdl.handle.net/2440/67027" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/UBoost2011Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={UBoost}:+{B}oosting+with+the+{U}niversum+Shen,+Chunhua+and+Wang,+Peng+and+Shen,+Fumin+and+Wang,+Hanzi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={UBoost}:+{B}oosting+with+the+{U}niversum" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1311.5947.pdf"><img class="imgP  right"   src="data/thumbnail/ACCV12xxxarXiv.jpg"></a><b>Fast training of effective multi-class boosting using coordinate descent optimization</b>   
<br />\(\cdot\) <i>G. Lin, C. Shen, A. van den Hengel, D. Suter</i>.
<br />\(\cdot\) <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;12), 2012</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1311.5947" target=&ldquo;blank&rdquo;>arXiv</a><a href="data/bibtex/ACCV12.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Training+of+Effective+Multi-class+Boosting+Using+Coordinate+Descent+Optimization+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Suter,+David" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+Training+of+Effective+Multi-class+Boosting+Using+Coordinate+Descent+Optimization" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Sharing features in multi-class boosting via group sparsity</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;12), 2012</i>.
<br />\(\cdot\) <a href="http://hdl.handle.net/2440/69851" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/CVPR12b.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Sharing+Features+in+Multi-class+Boosting+via+Group+Sparsity+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Sharing+Features+in+Multi-class+Boosting+via+Group+Sparsity" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1204.2912.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR12axxxarXiv.jpg"></a><b>Non-sparse linear representations for visual tracking with online reservoir metric learning</b>   
<br />\(\cdot\) <i>X. Li, C. Shen, Q. Shi, A. Dick, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;12), 2012</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1204.2912" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://hdl.handle.net/2440/70244" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/CVPR12a.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Non-sparse+Linear+Representations+for+Visual+Tracking+with+Online+Reservoir+Metric+Learning+Li,+Xi+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Non-sparse+Linear+Representations+for+Visual+Tracking+with+Online+Reservoir+Metric+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Robust tracking with weighted online structured learning</b>   
<br />\(\cdot\) <i>R. Yao, Q. Shi, C. Shen, Y. Zhang, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;12), 2012</i>.
<br />\(\cdot\) <a href="https://sites.google.com/site/chhshen/publication/weighted_tracking_eccv12.pdf?attredirects=1" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/ECCV12.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Robust+Tracking+with+Weighted+Online+Structured+Learning+Yao,+Rui+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Robust+Tracking+with+Weighted+Online+Structured+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICML12xxxarXiv.jpg"><b>Is margin preserved after random projection?</b>   
<br />\(\cdot\) <i>Q. Shi, C. Shen, R. Hill, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. International Conference on Machine Learning (ICML&rsquo;12), 2012</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1206.4651" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://hdl.handle.net/2440/71063" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/ICML12.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Is+margin+preserved+after+random+projection?+Shi,+Qinfeng+and+Shen,+Chunhua+and+Hill,+Rhys+and+van+den+Hengel,+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Is+margin+preserved+after+random+projection?" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>This work provides an analysis of margin distortion under random projections, the conditions under which margins are preserved, and presents bounds on the margin distortion.
</p>
</li>
</ol>

</li>
</ol>
<h2>Other</h2>
<ol reversed>
<li><p><b>Semidefinite programming (book chapter in: encyclopedia of computer vision, springer)</b>   
<br />\(\cdot\) <i>C. Shen, A. van den Hengel</i>.
<br />\(\cdot\)   <i>2012</i>.
<br />\(\cdot\) <a href="data/bibtex/SDP2012.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Semidefinite+programming+(Book+chapter+in:+Encyclopedia+of+Computer+Vision,+Springer)+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Semidefinite+programming+(Book+chapter+in:+Encyclopedia+of+Computer+Vision,+Springer)" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h1>2011</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/0903.3103.pdf"><img class="imgP  right"   src="data/thumbnail/GSLDA2010ShenxxxarXiv.jpg"></a><b>Efficiently learning a detection cascade with sparse eigenvectors</b>   
<br />\(\cdot\) <i>C. Shen, S. Paisitkriangkrai, J. Zhang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2011</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/0903.3103" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TIP.2010.2055880" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/GSLDA2010Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficiently+Learning+a+Detection+Cascade+with+Sparse+Eigenvectors+Shen,+Chunhua+and+Paisitkriangkrai,+Sakrapee+and+Zhang,+Jian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficiently+Learning+a+Detection+Cascade+with+Sparse+Eigenvectors" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Incremental2010ShenxxxarXiv.jpg"><b>Incremental training of a detector using online sparse eigen-decomposition</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2011</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1005.4118" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TIP.2010.2053548" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Incremental2010Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Incremental+Training+of+a+Detector+Using+Online+Sparse+Eigen-decomposition+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Incremental+Training+of+a+Detector+Using+Online+Sparse+Eigen-decomposition" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>Efficiently learning a distance metric for large margin nearest neighbor classification</b>   
<br />\(\cdot\) <i>K. Park, C. Shen, Z. Hao, J. Kim</i>.
<br />\(\cdot\) <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;11), 2011</i>.
<br />\(\cdot\) <a href="data/bibtex/AAAI2011.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficiently+learning+a+distance+metric+for+large+margin+nearest+neighbor+classification+Park,+Kyoungup+and+Shen,+Chunhua+and+Hao,+Zhihui+and+Kim,+Junae" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficiently+learning+a+distance+metric+for+large+margin+nearest+neighbor+classification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Is face recognition really a compressive sensing problem?</b>   
<br />\(\cdot\) <i>Q. Shi, A. Eriksson, A. van den Hengel, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br />\(\cdot\) <a href="http://hdl.handle.net/2440/67036" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Shi2011CVPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Is+face+recognition+really+a+Compressive+Sensing+problem?+Shi,+Qinfeng+and+Eriksson,+Anders+and+van+den+Hengel,+Anton+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Is+face+recognition+really+a+Compressive+Sensing+problem?" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>A scalable dual approach to semidefinite metric learning</b>   
<br />\(\cdot\) <i>C. Shen, J. Kim, L. Wang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br />\(\cdot\) <a href="http://goo.gl/UyVdEc" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Shen2011CVPRb.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Scalable+Dual+Approach+to+Semidefinite+Metric+Learning+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Scalable+Dual+Approach+to+Semidefinite+Metric+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>A direct formulation for totally-corrective multi-class boosting</b>   
<br />\(\cdot\) <i>C. Shen, Z. Hao</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br />\(\cdot\) <a href="http://hdl.handle.net/2440/62919" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Shen2011CVPRa.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+direct+formulation+for+totally-corrective+multi-class+boosting+Shen,+Chunhua+and+Hao,+Zhihui" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+direct+formulation+for+totally-corrective+multi-class+boosting" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>A generalized probabilistic framework for compact codebook creation</b>   
<br />\(\cdot\) <i>L. Liu, L. Wang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br />\(\cdot\) <a href="http://hdl.handle.net/2440/63014" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Liu2011CVPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+generalized+probabilistic+framework+for+compact+codebook+creation+Liu,+Lingqiao+and+Wang,+Lei+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+generalized+probabilistic+framework+for+compact+codebook+creation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Real-time visual tracking using compressive sensing</b>   
<br />\(\cdot\) <i>H. Li, C. Shen, Q. Shi</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br />\(\cdot\) <a href="http://goo.gl/dsjsoM" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Li2011CVPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-time+visual+tracking+Using+compressive+sensing+Li,+Hanxi+and+Shen,+Chunhua+and+Shi,+Qinfeng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Real-time+visual+tracking+Using+compressive+sensing" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Laplacian margin distribution boosting for learning from sparsely labeled data</b>   
<br />\(\cdot\) <i>T. Wang, X. He, C. Shen, N. Barnes</i>.
<br />\(\cdot\) <i>Proc. International Conference on Digital Image Computing: Techniques and Applications (DICTA&rsquo;11), 2011</i>.
<br />\(\cdot\) <a href="data/bibtex/DICTA2011a.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Laplacian+margin+distribution+boosting+for+learning+from+sparsely+labeled+data+Wang,+Tao+and+He,+Xuming+and+Shen,+Chunhua+and+Barnes,+Nick" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Laplacian+margin+distribution+boosting+for+learning+from+sparsely+labeled+data" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>On the optimality of sequential forward feature selection using class separability measure</b>   
<br />\(\cdot\) <i>L. Wang, C. Shen, R. Hartley</i>.
<br />\(\cdot\) <i>Proc. International Conference on Digital Image Computing: Techniques and Applications (DICTA&rsquo;11), 2011</i>.
<br />\(\cdot\) <a href="data/bibtex/DICTA2011b.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=On+The+Optimality+of+Sequential+Forward+Feature+Selection+Using+Class+Separability+Measure+Wang,+Lei+and+Shen,+Chunhua+and+Hartley,+Richard" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=On+The+Optimality+of+Sequential+Forward+Feature+Selection+Using+Class+Separability+Measure" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Graph mode-based contextual kernels for robust SVM tracking</b>   
<br />\(\cdot\) <i>X. Li, A. Dick, H. Wang, C. Shen, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;11), 2011</i>.
<br />\(\cdot\) <a href="http://goo.gl/GzpBVb" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/ICCV2011.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Graph+mode-based+contextual+kernels+for+robust+{SVM}+tracking+Li,+Xi+and+Dick,+Anthony+and+Wang,+Hanzi+and+Shen,+Chunhua+and+van+den+Hengel,+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Graph+mode-based+contextual+kernels+for+robust+{SVM}+tracking" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h1>2010</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><b>Interactive color image segmentation with linear programming</b>   
<br />\(\cdot\) <i>H. Li, C. Shen</i>.
<br />\(\cdot\) <i>Machine Vision and Applications (MVA), 2010</i>.
<br />\(\cdot\) <a href="http://www.springerlink.com/content/b254775776114226" target=&ldquo;blank&rdquo;>link</a><a href="http://sites.google.com/site/chhshen/publication/MVA2010LP.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Li2010Interactive.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Interactive+Color+Image+Segmentation+with+Linear+Programming+Li,+Hongdong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Interactive+Color+Image+Segmentation+with+Linear+Programming" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/0905.2463.pdf"><img class="imgP  right"   src="data/thumbnail/Generalized2010ShenxxxarXiv.jpg"></a><b>Generalized kernel-based visual tracking</b>   
<br />\(\cdot\) <i>C. Shen, J. Kim, H. Wang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2010</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/0905.2463" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TCSVT.2009.2031393" target=&ldquo;blank&rdquo;>link</a><a href="http://sites.google.com/site/chhshen/publication/TCSVT2010.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Generalized2010Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Generalized+Kernel-based+Visual+Tracking+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Hanzi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Generalized+Kernel-based+Visual+Tracking" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://github.com/chhshen/KernelTracking" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
<li><p><b>Feature selection with redundancy-constrained class separability</b>   
<br />\(\cdot\) <i>L. Zhou, L. Wang, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Neural Networks (TNN), 2010</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/TNN.2010.2044189" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Zhou2010FS.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Feature+Selection+With+Redundancy-Constrained+Class+Separability+Zhou,+Luping+and+Wang,+Lei+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Feature+Selection+With+Redundancy-Constrained+Class+Separability" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/MDBoost2010ShenxxxarXiv.jpg"><b>Boosting through optimization of margin distributions</b>   
<br />\(\cdot\) <i>C. Shen, H. Li</i>.
<br />\(\cdot\) <i>IEEE Transactions on Neural Networks (TNN), 2010</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/0904.2037" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TNN.2010.2040484" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/MDBoost2010Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Boosting+through+optimization+of+margin+distributions+Shen,+Chunhua+and+Li,+Hanxi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Boosting+through+optimization+of+margin+distributions" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Scalable2010ShenxxxarXiv.jpg"><b>Scalable large-margin Mahalanobis distance metric learning</b>   
<br />\(\cdot\) <i>C. Shen, J. Kim, L. Wang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Neural Networks (TNN), 2010</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1003.0487" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TNN.2010.2052630" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Scalable2010Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Scalable+Large-Margin+{M}ahalanobis+Distance+Metric+Learning+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Scalable+Large-Margin+{M}ahalanobis+Distance+Metric+Learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Dual2010ShenxxxarXiv.jpg"><b>On the dual formulation of boosting algorithms</b>   
<br />\(\cdot\) <i>C. Shen, H. Li</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2010</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/0901.3590" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2010.47" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Dual2010Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=On+the+Dual+Formulation+of+Boosting+Algorithms+Shen,+Chunhua+and+Li,+Hanxi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=On+the+Dual+Formulation+of+Boosting+Algorithms" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>Pyramid center-symmetric local binary, trinary patterns for effective pedestrian detection</b>   
<br />\(\cdot\) <i>Y. Zheng, C. Shen, R. Hartley, X. Huang</i>.
<br />\(\cdot\) <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;10), 2010</i>.
<br />\(\cdot\) <a href="http://goo.gl/5Cthse" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Zheng2010ACCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Pyramid+Center-symmetric+Local+Binary,+Trinary+Patterns+for+Effective+Pedestrian+Detection+Zheng,+Yongbin+and+Shen,+Chunhua+and+Hartley,+Richard+and+Huang,+Xinsheng" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Pyramid+Center-symmetric+Local+Binary,+Trinary+Patterns+for+Effective+Pedestrian+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Asymmetric totally-corrective boosting for real-time object detection</b>   
<br />\(\cdot\) <i>P. Wang, C. Shen, N. Barnes, H. Zheng, Z. Ren</i>.
<br />\(\cdot\) <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;10), 2010</i>.
<br />\(\cdot\) <a href="data/bibtex/Wang2010ACCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Asymmetric+Totally-corrective+Boosting+for+Real-time+Object+Detection+Wang,+Peng+and+Shen,+Chunhua+and+Barnes,+Nick+and+Zheng,+Hong+and+Ren,+Zhang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Asymmetric+Totally-corrective+Boosting+for+Real-time+Object+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Oral presentation.
</p>
</li></ol>
</li>
<li><p><b>Face detection with effective feature extraction</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br />\(\cdot\) <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;10), 2010</i>.
<br />\(\cdot\) <a href="data/bibtex/Paul2010ACCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Face+Detection+with+Effective+Feature+Extraction+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Face+Detection+with+Effective+Feature+Extraction" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Totally-corrective multi-class boosting</b>   
<br />\(\cdot\) <i>Z. Hao, C. Shen, N. Barnes, B. Wang</i>.
<br />\(\cdot\) <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;10), 2010</i>.
<br />\(\cdot\) <a href="data/bibtex/Hao2010ACCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Totally-corrective+Multi-class+Boosting+Hao,+Zhihui+and+Shen,+Chunhua+and+Barnes,+Nick+and+Wang,+Bo" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Totally-corrective+Multi-class+Boosting" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Rapid face recognition using hashing</b>   
<br />\(\cdot\) <i>Q. Shi, H. Li, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;10), 2010</i>.
<br />\(\cdot\) <a href="http://sites.google.com/site/chhshen/publication/cvpr10.pdf?attredirects=1" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Shi2010CVPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Rapid+face+recognition+using+hashing+Shi,+Qinfeng+and+Li,+Hanxi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Rapid+face+recognition+using+hashing" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Robust face recognition via accurate face alignment and sparse representation</b>   
<br />\(\cdot\) <i>H. Li, P. Wang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. International Conference on on Digital Image Computing: Techniques and Applications (DICTA&rsquo;10), 2010</i>.
<br />\(\cdot\) <a href="data/bibtex/Face2010Li.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Robust+Face+Recognition+via+Accurate+Face+Alignment+and+Sparse+Representation+Li,+Hanxi+and+Wang,+Peng+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Robust+Face+Recognition+via+Accurate+Face+Alignment+and+Sparse+Representation" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2010ECCVxxxarXiv.jpg"><b>LACBoost and FisherBoost: optimally building cascade classifiers</b>   
<br />\(\cdot\) <i>C. Shen, P. Wang, H. Li</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;10), 2010</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/1005.4103" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://dx.doi.org/10.1007/978-3-642-15552-9_44" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Shen2010ECCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={LACBoost}+and+{FisherBoost}:+Optimally+Building+Cascade+Classifiers+Shen,+Chunhua+and+Wang,+Peng+and+Li,+Hanxi" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={LACBoost}+and+{FisherBoost}:+Optimally+Building+Cascade+Classifiers" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Improved human detection and classification in thermal images</b>   
<br />\(\cdot\) <i>W. Wang, J. Zhang, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Image Processing (ICIP&rsquo;10), 2010</i>.
<br />\(\cdot\) <a href="data/bibtex/Human2010.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Improved+Human+Detection+and+Classification+in+Thermal+Images+Wang,+Weihong+and+Zhang,+Jian+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Improved+Human+Detection+and+Classification+in+Thermal+Images" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Training a multi-exit cascade with linear asymmetric classification for efficient object detection</b>   
<br />\(\cdot\) <i>P. Wang, C. Shen, H. Zheng, Z. Ren</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Image Processing (ICIP&rsquo;10), 2010</i>.
<br />\(\cdot\) <a href="data/bibtex/Multiexit2010Wang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Training+a+multi-exit+cascade+with+linear+asymmetric+classification+for+efficient+object+detection+Wang,+Peng+and+Shen,+Chunhua+and+Zheng,+Hong+and+Ren,+Zhang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Training+a+multi-exit+cascade+with+linear+asymmetric+classification+for+efficient+object+detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Hippocampal shape classification using redundancy constrained feature selection</b>   
<br />\(\cdot\) <i>L. Zhou, L. Wang, C. Shen, N. Barnes</i>.
<br />\(\cdot\) <i>Proc. International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI&rsquo;10), 2010</i>.
<br />\(\cdot\) <a href="data/bibtex/Zhou2010MICCAI.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Hippocampal+shape+classification+using+redundancy+constrained+feature+selection+Zhou,+Luping+and+Wang,+Lei+and+Shen,+Chunhua+and+Barnes,+Nick" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Hippocampal+shape+classification+using+redundancy+constrained+feature+selection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Other</h2>
<ol reversed>
<li><p><b>Proceedings of international conference on digital image computing: techniques and applications</b>   
<br />\(\cdot\) <i>J. Zhang, C. Shen, G. Geers, Q. Wu</i>.
<br />\(\cdot\)   <i>Editors, IEEE, 2010</i>.
<br />\(\cdot\) <a href="data/bibtex/DICTA2010.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Proceedings+of+International+Conference+on+Digital+Image+Computing:+Techniques+and+Applications+Zhang,+Jian+and+Shen,+Chunhua+and+Geers,+Glen+and+Wu,+Qiang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Proceedings+of+International+Conference+on+Digital+Image+Computing:+Techniques+and+Applications" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h1>2009</h1>
<h2>Conference</h2>
<ol reversed>
<li><p><b>A variant of the trace quotient formulation for dimensionality reduction</b>   
<br />\(\cdot\) <i>P. Wang, C. Shen, H. Zheng, Z. Ren</i>.
<br />\(\cdot\) <i>Proc. 9th Asian Conference on Computer Vision (ACCV&rsquo;09), 2009</i>.
<br />\(\cdot\) <a href="data/bibtex/Wang2009ACCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Variant+of+the+Trace+Quotient+Formulation+for+Dimensionality+Reduction+Wang,+Peng+and+Shen,+Chunhua+and+Zheng,+Hong+and+Ren,+Zhang" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Variant+of+the+Trace+Quotient+Formulation+for+Dimensionality+Reduction" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>A scalable algorithm for learning a Mahalanobis distance metric</b>   
<br />\(\cdot\) <i>J. Kim, C. Shen, L. Wang</i>.
<br />\(\cdot\) <i>Proc. 9th Asian Conference on Computer Vision (ACCV&rsquo;09), 2009</i>.
<br />\(\cdot\) <a href="data/bibtex/Kim2009ACCV.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Scalable+Algorithm+for+Learning+a+{M}ahalanobis+Distance+Metric+Kim,+Junae+and+Shen,+Chunhua+and+Wang,+Lei" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Scalable+Algorithm+for+Learning+a+{M}ahalanobis+Distance+Metric" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Paisitkriangkrai2009CVPRxxxarXiv.jpg"><b>Efficiently training a better visual detector with sparse eigenvectors</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;09), 2009</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/0903.3103" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://sites.google.com/site/chhshen/publication/CVPR2009GSLDA.pdf?attredirects=1" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Paisitkriangkrai2009CVPR.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficiently+Training+a+Better+Visual+Detector+with+Sparse+Eigenvectors+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficiently+Training+a+Better+Visual+Detector+with+Sparse+Eigenvectors" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>A two-layer night-time vehicle detector</b>   
<br />\(\cdot\) <i>W. Wang, C. Shen, J. Zhang, S. Paisitkriangkrai</i>.
<br />\(\cdot\) <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;09), 2009</i>.
<br />\(\cdot\) <a href="data/bibtex/Wang2009DICTA.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Two-Layer+Night-time+Vehicle+Detector+Wang,+Weihong+and+Shen,+Chunhua+and+Zhang,+Jian+and+Paisitkriangkrai,+Sakrapee" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Two-Layer+Night-time+Vehicle+Detector" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Smooth approximation of \(l_\infty\)-norm for multi-view geometry</b>   
<br />\(\cdot\) <i>Y. Dai, H. Li, M. He, C. Shen</i>.
<br />\(\cdot\) <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;09), 2009</i>.
<br />\(\cdot\) <a href="data/bibtex/Dai2009DICTA.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Smooth+Approximation+of+L_\infty-Norm+for+Multi-view+Geometry+Dai,+Yuchao+and+Li,+Hongdong+and+He,+Mingyi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Smooth+Approximation+of+L_\infty-Norm+for+Multi-view+Geometry" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2009PSDxxxPDF.jpg"><b>Positive semidefinite metric learning with boosting</b>   
<br />\(\cdot\) <i>C. Shen, J. Kim, L. Wang, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;09), 2009</i>.
<br />\(\cdot\) <a href="http://arxiv.org/abs/0910.2279" target=&ldquo;blank&rdquo;>arXiv</a><a href="http://papers.NeurIPS.cc/paper/3658-positive-semidefinite-metric-learning-with-boosting.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Shen2009PSD.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Positive+semidefinite+metric+learning+with+Boosting+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Positive+semidefinite+metric+learning+with+Boosting" target=&ldquo;blank&rdquo;>semantic scholar</a><a href="https://code.google.com/archive/p/boosting/downloads" target=&ldquo;blank&rdquo;>project webpage</a>
</p>
</li>
</ol>
<h1>2008</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><b>Performance evaluation of local features in human classification and detection</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br />\(\cdot\) <i>IET Computer Vision (IETCV), 2008</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1049/iet-cvi:20080026" target=&ldquo;blank&rdquo;>link</a><a href="http://sites.google.com/site/chhshen/publication/Huam2009IET.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Performance2008Paul.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Performance+Evaluation+of+Local+Features+in+Human+Classification+and+Detection+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Performance+Evaluation+of+Local+Features+in+Human+Classification+and+Detection" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Invited submission, special issue of DICTA2007.
</p>
</li></ol>
</li>
<li><p><b>Supervised dimensionality reduction via sequential semidefinite programming</b>   
<br />\(\cdot\) <i>C. Shen, H. Li, M. Brooks</i>.
<br />\(\cdot\) <i>Pattern Recognition (PR), 2008</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1016/j.patcog.2008.06.015" target=&ldquo;blank&rdquo;>link</a><a href="http://sites.google.com/site/chhshen/publication/PR1.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/SDP2008Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Supervised+Dimensionality+Reduction+via+Sequential+Semidefinite+Programming+Shen,+Chunhua+and+Li,+Hongdong+and+Brooks,+Michael+J." target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Supervised+Dimensionality+Reduction+via+Sequential+Semidefinite+Programming" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Fast pedestrian detection using a cascade of boosted covariance features</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br />\(\cdot\) <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2008</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/TCSVT.2008.928213" target=&ldquo;blank&rdquo;>link</a><a href="http://goo.gl/lgpDJB" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Human2008Paul.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Pedestrian+Detection+Using+a+Cascade+of+Boosted+Covariance+Features+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+Pedestrian+Detection+Using+a+Cascade+of+Boosted+Covariance+Features" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>Self-calibrating cameras using semidefinite programming</b>   
<br />\(\cdot\) <i>C. Shen, H. Li, M. Brooks</i>.
<br />\(\cdot\) <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;08), 2008</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/DICTA.2008.46" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Shen2008Self.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Self-Calibrating+Cameras+Using+Semidefinite+Programming+Shen,+Chunhua+and+Li,+Hongdong+and+Brooks,+Michael+J." target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Self-Calibrating+Cameras+Using+Semidefinite+Programming" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Multi-view human motion capture with an improved deformation skin model</b>   
<br />\(\cdot\) <i>Y. Lu, L. Wang, R. Hartley, H. Li, C. Shen</i>.
<br />\(\cdot\) <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;08), 2008</i>.
<br />\(\cdot\) <a href="data/bibtex/Lu2008Human.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-view+Human+Motion+Capture+with+An+Improved+Deformation+Skin+Model+Lu,+Yifan+and+Wang,+Lei+and+Hartley,+Richard+and+Li,+Hongdong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Multi-view+Human+Motion+Capture+with+An+Improved+Deformation+Skin+Model" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Boosting the minimum margin: LPBoost vs. AdaBoost</b>   
<br />\(\cdot\) <i>H. Li, C. Shen</i>.
<br />\(\cdot\) <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;08), 2008</i>.
<br />\(\cdot\) <a href="data/bibtex/Li2008Boosting.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Boosting+the+minimum+margin:+{LPBoost}+vs.+{AdaBoost}+Li,+Hanxi+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Boosting+the+minimum+margin:+{LPBoost}+vs.+{AdaBoost}" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Learning cascaded reduced-set SVMs using linear programming</b>   
<br />\(\cdot\) <i>J. Kim, C. Shen, L. Wang</i>.
<br />\(\cdot\) <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;08), 2008</i>.
<br />\(\cdot\) <a href="data/bibtex/Junae2008SVM.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Cascaded+Reduced-set+{SVM}s+Using+Linear+Programming+Kim,+Junae+and+Shen,+Chunhua+and+Wang,+Lei" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Cascaded+Reduced-set+{SVM}s+Using+Linear+Programming" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>A fast algorithm for creating a compact and discriminative visual codebook</b>   
<br />\(\cdot\) <i>L. Wang, L. Zhou, C. Shen</i>.
<br />\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;08), 2008</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1007/978-3-540-88693-8_53" target=&ldquo;blank&rdquo;>link</a><a href="http://sites.google.com/site/chhshen/publication/ECCV2008Wang.pdf?attredirects=1" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Fast2008Wang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Fast+Algorithm+for+Creating+a+Compact+and+Discriminative+Visual+Codebook+Wang,+Lei+and+Zhou,+Luping+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Fast+Algorithm+for+Creating+a+Compact+and+Discriminative+Visual+Codebook" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Face detection from few training examples</b>   
<br />\(\cdot\) <i>C. Shen, S. Paisitkriangkrai, J. Zhang</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Image Processing (ICIP&rsquo;08), 2008</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/ICIP.2008.4712367" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Face2008Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Face+Detection+From+Few+Training+Examples+Shen,+Chunhua+and+Paisitkriangkrai,+Sakrapee+and+Zhang,+Jian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Face+Detection+From+Few+Training+Examples" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2008PSDxxxPDF.jpg"><b>PSDBoost: matrix-generation linear programming for positive semidefinite matrices learning</b>   
<br />\(\cdot\) <i>C. Shen, A. Welsh, L. Wang</i>.
<br />\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;08), 2008</i>.
<br />\(\cdot\) <a href="http://papers.NeurIPS.cc/paper/3611-psdboost-matrix-generation-linear-programming-for-positive-semidefinite-matrices-learning.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Shen2008PSD.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={PSDB}oost:+Matrix-generation+linear+programming+for+positive+semidefinite+matrices+learning+Shen,+Chunhua+and+Welsh,+Alan+and+Wang,+Lei" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={PSDB}oost:+Matrix-generation+linear+programming+for+positive+semidefinite+matrices+learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Real-time pedestrian detection using a boosted multi-layer classifier</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br />\(\cdot\) <i>Proc. 8th IEEE International Workshop on Visual Surveillance, in conjunction with European Conference on Computer Vision (ECCVW&rsquo;08), 2008</i>.
<br />\(\cdot\) <a href="data/bibtex/Realtime2008Paisitkriangkrai.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-time+Pedestrian+Detection+Using+a+Boosted+Multi-layer+Classifier+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Real-time+Pedestrian+Detection+Using+a+Boosted+Multi-layer+Classifier" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h1>2007</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><b>Fast global kernel density mode seeking: applications to localization and tracking</b>   
<br />\(\cdot\) <i>C. Shen, M. Brooks, A. van den Hengel</i>.
<br />\(\cdot\) <i>IEEE Transactions on Image Processing (TIP), 2007</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/TIP.2007.894233" target=&ldquo;blank&rdquo;>link</a><a href="http://sites.google.com/site/chhshen/publication/TIP2007Shen.pdf" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Fast2007Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+global+kernel+density+mode+seeking:+applications+to+localization+and+tracking+Shen,+Chunhua+and+Brooks,+Michael+J.+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+global+kernel+density+mode+seeking:+applications+to+localization+and+tracking" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Adaptive object tracking based on an effective appearance filter</b>   
<br />\(\cdot\) <i>H. Wang, D. Suter, K. Schindler, C. Shen</i>.
<br />\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2007</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/TPAMI.2007.1112" target=&ldquo;blank&rdquo;>link</a><a href="http://goo.gl/6rQTA1" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Adaptive2007Wang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adaptive+object+tracking+based+on+an+effective+appearance+filter+Wang,+Hanzi+and+Suter,+David+and+Schindler,+Konrad+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Adaptive+object+tracking+based+on+an+effective+appearance+filter" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Featured article of September issue 2007.
</p>
</li>
</ol>

</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>A convex programming approach to the trace quotient problem</b>   
<br />\(\cdot\) <i>C. Shen, H. Li, M. Brooks</i>.
<br />\(\cdot\) <i>Proc. 8th Asian Conference on Computer Vision (ACCV&rsquo;07), 2007</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1007/978-3-540-76390-1_23" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Convex2007Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+convex+programming+approach+to+the+trace+quotient+problem+Shen,+Chunhua+and+Li,+Hongdong+and+Brooks,+Michael+J." target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+convex+programming+approach+to+the+trace+quotient+problem" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Kernel-based tracking from a probabilistic viewpoint</b>   
<br />\(\cdot\) <i>Q. Nguyen, A. Robles-Kelly, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;07), 2007</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/CVPR.2007.383240" target=&ldquo;blank&rdquo;>link</a><a href="http://goo.gl/1QNmaq" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Kernel2007Quang.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Kernel-based+tracking+from+a+probabilistic+viewpoint+Nguyen,+Quang+and+Robles-Kelly,+Antonio+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Kernel-based+tracking+from+a+probabilistic+viewpoint" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Feature extraction using sequential semidefinite programming</b>   
<br />\(\cdot\) <i>C. Shen, H. Li, M. Brooks</i>.
<br />\(\cdot\) <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;07), 2007</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/DICTA.2007.4426829" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Feature2007Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Feature+extraction+using+sequential+semidefinite+programming+Shen,+Chunhua+and+Li,+Hongdong+and+Brooks,+Michael+J." target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Feature+extraction+using+sequential+semidefinite+programming" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>An experimental evaluation of local features for pedestrian classification</b>   
<br />\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br />\(\cdot\) <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;07), 2007</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/DICTA.2007.4426775" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Experimental2007Paul.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+Experimental+Evaluation+of+Local+Features+for+Pedestrian+Classification+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=An+Experimental+Evaluation+of+Local+Features+for+Pedestrian+Classification" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Best Paper Award.
</p>
</li></ol>
</li>
<li><p><b>Color image labelling using linear programming</b>   
<br />\(\cdot\) <i>H. Li, C. Shen, Z. Wen</i>.
<br />\(\cdot\) <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;07), 2007</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/DICTA.2007.4426802" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Color2007Li.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Color+image+labelling+using+linear+programming+Li,+Hongdong+and+Shen,+Chunhua+and+Wen,+Zhiying" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Color+image+labelling+using+linear+programming" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Object-respecting colour image segmentation: an LP approach</b>   
<br />\(\cdot\) <i>H. Li, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Image Processing (ICIP&rsquo;07), 2007</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/ICIP.2007.4379141" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Object2007Li.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Object-respecting+colour+image+segmentation:+An+{LP}+approach+Li,+Hongdong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Object-respecting+colour+image+segmentation:+An+{LP}+approach" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h1>2006</h1>
<h2>Conference</h2>
<ol reversed>
<li><p><b>Classification-based likelihood functions for Bayesian tracking</b>   
<br />\(\cdot\) <i>C. Shen, H. Li, M. Brooks</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Advanced Video and Signal based Surveillance (AVSS&rsquo;06), 2006</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/AVSS.2006.33" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Classification2006Shen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Classification-based+likelihood+functions+for+{B}ayesian+tracking+Shen,+Chunhua+and+Li,+Hongdong+and+Brooks,+Michael+J." target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Classification-based+likelihood+functions+for+{B}ayesian+tracking" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Enhanced kernel-based tracking for monochromatic and thermographic video</b>   
<br />\(\cdot\) <i>Q. Nguyen, A. Robles-Kelly, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Advanced Video and Signal based Surveillance (AVSS&rsquo;06), 2006</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/AVSS.2006.47" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Enhanced2006Nguyen.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Enhanced+kernel-based+tracking+for+monochromatic+and+thermographic+video+Nguyen,+Quang+and+Robles-Kelly,+Antonio+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Enhanced+kernel-based+tracking+for+monochromatic+and+thermographic+video" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>An LMI approach for reliable PTZ camera self-calibration</b>   
<br />\(\cdot\) <i>H. Li, C. Shen</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Advanced Video and Signal based Surveillance (AVSS&rsquo;06), 2006</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/AVSS.2006.21" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/LMI2006Li.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+{LMI}+approach+for+reliable+{PTZ}+camera+self-calibration+Li,+Hongdong+and+Shen,+Chunhua" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=An+{LMI}+approach+for+reliable+{PTZ}+camera+self-calibration" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h1>2005</h1>
<h2>Conference</h2>
<ol reversed>
<li><p><b>Fast global kernel density mode seeking with application to localisation and tracking</b>   
<br />\(\cdot\) <i>C. Shen, M. Brooks, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;05), 2005</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/ICCV.2005.94" target=&ldquo;blank&rdquo;>link</a><a href="http://goo.gl/UHzjWW" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Shen2005Fast.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+global+kernel+density+mode+seeking+with+application+to+localisation+and+tracking+Shen,+Chunhua+and+Brooks,+Michael+J.+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+global+kernel+density+mode+seeking+with+application+to+localisation+and+tracking" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Oral presentation, 45 out of 1200 submissions.
</p>
</li></ol>
</li>
<li><p><b>Visual tracking via efficient kernel discriminant subspace learning</b>   
<br />\(\cdot\) <i>C. Shen, A. van den Hengel, M. Brooks</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Image Processing (ICIP&rsquo;05), 2005</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/ICIP.2005.1530124" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Shen2005Visual.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+tracking+via+efficient+kernel+discriminant+subspace+learning+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Brooks,+Michael+J." target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Visual+tracking+via+efficient+kernel+discriminant+subspace+learning" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Augmented particle filtering for efficient visual tracking</b>   
<br />\(\cdot\) <i>C. Shen, M. Brooks, A. van den Hengel</i>.
<br />\(\cdot\) <i>Proc. IEEE International Conference on Image Processing (ICIP&rsquo;05), 2005</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/ICIP.2005.1530527" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Shen2005Augmented.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Augmented+particle+filtering+for+efficient+visual+tracking+Shen,+Chunhua+and+Brooks,+Michael+J.+and+{van+den+Hengel},+Anton" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Augmented+particle+filtering+for+efficient+visual+tracking" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>Adaptive over-relaxed mean shift</b>   
<br />\(\cdot\) <i>C. Shen, M. Brooks</i>.
<br />\(\cdot\) <i>Proc. 8th International Symposium on Signal Processing and Its Applications (ISSPA&rsquo;05), 2005</i>.
<br />\(\cdot\) <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1581003" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Shen2005Adaptive.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adaptive+over-relaxed+mean+shift+Shen,+Chunhua+and+Brooks,+Michael+J." target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Adaptive+over-relaxed+mean+shift" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Errata: in figure 3 square marker and circle marker should be swapped.
</p>
</li>
</ol>

</li>
</ol>
<h1>2004</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><b>Active control of radiation from a piston set in a rigid sphere</b>   
<br />\(\cdot\) <i>Z. Lin, J. Lu, C. Shen, X. Qiu, B. Xu</i>.
<br />\(\cdot\) <i>Journal of Acoustical Society of America (JASA), 2004</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1121/1.1736654" target=&ldquo;blank&rdquo;>link</a><a href="http://goo.gl/nc4SjU" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Lin2004Active.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Active+control+of+radiation+from+a+piston+set+in+a+rigid+sphere+Lin,+Zhibin+and+Lu,+Jing+and+Shen,+Chunhua+and+Qiu,+Xiaojun+and+Xu,+Boling" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Active+control+of+radiation+from+a+piston+set+in+a+rigid+sphere" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>Enhanced importance sampling: unscented auxiliary particle filtering for visual tracking</b>   
<br />\(\cdot\) <i>C. Shen, A. van den Hengel, A. Dick, M. Brooks</i>.
<br />\(\cdot\) <i>Proc. Australian Joint Conference on Artificial Intelligence (AI&rsquo;04), 2004</i>.
<br />\(\cdot\) <a href="http://digital.library.adelaide.edu.au/dspace/handle/2440/29538" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Shen2004Enhanced.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Enhanced+importance+sampling:+unscented+auxiliary+particle+filtering+for+visual+tracking+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Dick,+Anthony+and+Brooks,+Michael+J." target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Enhanced+importance+sampling:+unscented+auxiliary+particle+filtering+for+visual+tracking" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
<li><p><b>2D articulated tracking with dynamic Bayesian networks</b>   
<br />\(\cdot\) <i>C. Shen, A. van den Hengel, A. Dick, M. Brooks</i>.
<br />\(\cdot\) <i>Proc. International Conference on Computer and Information Technology (CIT&rsquo;04), 2004</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1109/CIT.2004.1357185" target=&ldquo;blank&rdquo;>link</a><a href="data/bibtex/Shen2004Articulated.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={2D}+articulated+tracking+with+dynamic+{B}ayesian+networks+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Dick,+Anthony+and+Brooks,+Michael+J." target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q={2D}+articulated+tracking+with+dynamic+{B}ayesian+networks" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h1>2003</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><b>Lattice form adaptive infinite impulse response filtering algorithm for active noise control</b>   
<br />\(\cdot\) <i>J. Lu, C. Shen, X. Qiu, B. Xu</i>.
<br />\(\cdot\) <i>Journal of Acoustical Society of America (JASA), 2003</i>.
<br />\(\cdot\) <a href="http://dx.doi.org/10.1121/1.1529665" target=&ldquo;blank&rdquo;>link</a><a href="http://sites.google.com/site/chhshen/publication/Lattice2003JASA.pdf?attredirects=1" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Lu2003Lattice.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Lattice+form+adaptive+infinite+impulse+response+filtering+algorithm+for+active+noise+control+Lu,+Jing+and+Shen,+Chunhua+and+Qiu,+Xiaojun+and+Xu,+Boling" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Lattice+form+adaptive+infinite+impulse+response+filtering+algorithm+for+active+noise+control" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>Probabilistic multiple cue integration for particle filter based tracking</b>   
<br />\(\cdot\) <i>C. Shen, A. van den Hengel, A. Dick</i>.
<br />\(\cdot\) <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;03), 2003</i>.
<br />\(\cdot\) <a href="http://sites.google.com/site/chhshen/publication/DICTA2003.pdf?attredirects=1" target=&ldquo;blank&rdquo;>pdf</a><a href="data/bibtex/Shen2003Probabilistic.bib" target=&ldquo;blank&rdquo;>bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Probabilistic+multiple+cue+integration+for+particle+filter+based+tracking+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Dick,+Anthony" target=&ldquo;blank&rdquo;>google scholar</a><a href="https://www.semanticscholar.org/search?q=Probabilistic+multiple+cue+integration+for+particle+filter+based+tracking" target=&ldquo;blank&rdquo;>semantic scholar</a>
</p>
<ol reversed>
<li><p>Nominated for Best Student Paper Award.
</p>
</li>
</ol>

</li>
</ol>
<div id="footer">
<div id="footer-text">
&copy; <b>Chunhua Shen</b>
&bull;
last update: 2023-10-16 09:54:59 ACDT
&bull;
<a href="#" onClick="changewidth(1);return false" title="Expand page width"><b>&larr;&rarr;</b></a>
&bull;
<a href="#" onClick="changewidth(-1);return false" title="Reduce page width"><b>&rarr;&larr;</b></a>
<!-- Javascript -->
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
<script
    src="./script/jquery-1.6.2.min.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery.flot.min.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery-scroll.js"
    type="text/javascript">
</script>
<script
    src="./script/width_change.js"
    type="text/javascript">
</script>
<script
    src="./script/reverse_ol.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery.highlight.js"
    type="text/javascript">
</script>
<!-- Required for the jQuery.LocalScroll Plug-in -->
<script type="text/javascript">
    $(document).ready(function(){
    //
    $.localScroll();
    //
    // Round images
    //
	$(".rounded-img, .rounded-img2").load(function() {
	$(this).wrap(function(){
	return '<span class="' + $(this).attr('class')
                + '" style="background:url(' + $(this).attr('src')
                + ') no-repeat center center; width: '
                + $(this).width() + 'px; height: '
                + $(this).height() + 'px;" />';
		});
		$(this).css("opacity","0");
	});
      //
      //
      //  nav tab animation
        var navDuration = 150; //time in miliseconds
        $('#nav li a').hover(function() {
          $(this).animate({ paddingTop:"50px"  }, navDuration);
        }, function() {
             $(this).animate({ paddingTop:"31px"}, navDuration);
        });
        //
        // plot citation figure using jquery flot, 2012 July, CS
        //
        var flot_options = {
        legend: {
            show: false,
            margin: 10,
            backgroundOpacity: 0.5
                },
        bars:  {
            show: true,
            barWidth: 0.6,
            align: "center"
        },
        yaxis: {
            min: -20,
            tickFormatter: function(val, axis) {
                if (val < 50)
                    return " &nbsp; ";  // some string
                else
                    return val < axis.max ? val.toFixed(0) :   "  &nbsp;  ";
            }
        },
        grid: {
            borderWidth: 0
        }
    };  // end of flot_options
    $.getJSON("./data/cs_cite.json", function(json) {
       //succes - data loaded, now use plot:
       var plotarea = $("#citation_plot_holder");
       var data=[json.data];
       $.plot(plotarea , data, flot_options);
    });
//
// end of jquery flot
//
    changewidth( 0.9 );
//
//
//  highlight ``Shen''
    $("body p").highlight(['C. Shen', 'Chunhua Shen']);
//  highlight selected publication venues
    $("body p").highlight(['CVPR', 'ICCV', 'ECCV', 'ICML', 'NeurIPS',
    'TPAMI', 'IJCV', 'JMLR'],  { element: 'span', className: 'selected_venue' } );
//
    });
</script>
<!-- News ticker -->
<script type="text/javascript">
    function tick(){
        $('#ticker li:first').slideUp( function () { $(this).appendTo($('#ticker')).slideDown(); });
    }
    setInterval(function(){ tick () }, 5000);
</script>
</div>
</div>
</div>
</body>
</html>
