<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<!--
-->
<link rel="stylesheet" href="css/cs.css"                 type="text/css" />
<link rel="stylesheet" href="css/content.css"            type="text/css" />
<!-- font family -->
<link href="https://fonts.cdnfonts.com/css/zilla-slab" rel="stylesheet">
<link href="//netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet" />
<link href="https://fonts.cdnfonts.com/css/comfortaa" rel="stylesheet">
<link href="https://fonts.cdnfonts.com/css/eb-garamond-2?styles=20043,20040" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<link rel="stylesheet" href="css/full_publication.css" type="text/css" />
<title>Chunhua Shen</title>
</head>
<body>
<div id="layout-content">
<div id="menu">
    <div id="menucontainer">
<ul id="nav">
   <li><a href="index.html" target="_self">Home</a></li>
   <li><a href="paper.html" target="_self">Publications</a></li>
   <li><a href="teaching.html" target="_self">Teaching</a></li>
</ul>
</div>
</div>
<div id="toptitle">
<h1>Publications (Full List)</h1>
<div id="subtitle">Categorised <a href="fullpaper2.html">by venue <i class='fa fa-location-arrow' aria-hidden='true'></i></a>,  <a href="fullpaper.html">by year <i class='fa fa-clock-o' aria-hidden='true'></i></a>. <b>348</b>  papers.  </div>
</div>
<p><a href="http://scholar.google.com/citations?hl=en&amp;user=Ljk2BvIAAAAJ&amp;view_op=list_works&amp;pagesize=100">Google scholar (28924 citations)  <i class='ai ai-google-scholar'   aria-hidden='true'></i></a>,
<a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Chunhua">DBLP <i class='ai ai-dblp ai-1x'></i></a>,
<a href="https://tinyurl.com/ww4dlqm">arXiv <i class='ai ai-biorxiv ai-1x'></i></a>.</p>
<p><div id="citation_plot_holder"></div></p>
<h1>2021</h1>
<h2>Journal</h2>
<ol reversed>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1912.09629.pdf"><img class="imgP  right"   src="data/thumbnail/IJCV2021Liuyl_arXiv.jpg"></a><b>Exploring the capacity of an orderless box discretization network for multi-orientation scene text detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Liu, T. He, H. Chen, X. Wang, C. Luo, S. Zhang, C. Shen, L. Jin</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1912.09629">arXiv</a><a href="data/bibtex/IJCV2021Liuyl.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploring+the+Capacity+of+an+Orderless+Box+Discretization+Network+for+Multi-orientation+Scene+Text+Detection+Liu,+Yuliang+and+He,+Tong+and+Chen,+Hao+and+Wang,+Xinyu+and+Luo,+Canjie+and+Zhang,+Shuaitao+and+Shen,+Chunhua+and+Jin,+Lianwen">google scholar</a><a href="https://www.semanticscholar.org/search?q=Exploring+the+Capacity+of+an+Orderless+Box+Discretization+Network+for+Multi-orientation+Scene+Text+Detection">semantic scholar</a><a href="https://git.io/TextDet">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2105.11610.pdf"><img class="imgP  right"   src="data/thumbnail/Bian2021IJCV_arXiv.jpg"></a><b>Unsupervised scale-consistent depth learning from video</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>J. Bian, H. Zhan, N. Wang, Z. Li, L. Zhang, C. Shen, M. Cheng, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2105.11610">arXiv</a><a href="data/bibtex/Bian2021IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Scale-consistent+Depth+Learning+from+Video+Bian,+Jia-Wang+and+Zhan,+Huangying+and+Wang,+Naiyan+and+Li,+Zhichao+and+Zhang,+Le+and+Shen,+Chunhua+and+Cheng,+Ming-Ming+and+Reid,+Ian">google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Scale-consistent+Depth+Learning+from+Video">semantic scholar</a><a href="https://github.com/JiawangBian/SC-SfMLearner-Release">project webpage</a></p>
</li>
<li><p><b>Learning deep part-aware embedding for person retrieval</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Zhao, C. Shen, X. Yu, H. Chen, Y. Gao, S. Xiong</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Zhao2021PR1.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Deep+Part-Aware+Embedding+for+Person+Retrieval+Zhao,+Yang+and+Shen,+Chunhua+and+Yu,+Xiaohan+and+Chen,+Hao+and+Gao,+Yongsheng+and+Xiong,+Shengwu">google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Deep+Part-Aware+Embedding+for+Person+Retrieval">semantic scholar</a></p>
</li>
<li><p><b>An adversarial human pose estimation network injected with graph structure</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>L. Tian, P. Wang, G. Liang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/Tian2021Adversarial.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+Adversarial+Human+Pose+Estimation+Network+Injected+with+Graph+Structure+Tian,+Lei+and+Wang,+Peng+and+Liang,+Guoqiang+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=An+Adversarial+Human+Pose+Estimation+Network+Injected+with+Graph+Structure">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1908.04680.pdf"><img class="imgP  right"   src="data/thumbnail/Zhuang2021Quantization_arXiv.jpg"></a><b>Effective training of convolutional neural networks with low-bitwidth weights and activations</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>B. Zhuang, J. Liu, M. Tan, L. Liu, I. Reid, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.04680">arXiv</a><a href="data/bibtex/Zhuang2021Quantization.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Effective+Training+of+Convolutional+Neural+Networks+with+Low-bitwidth+Weights+and+Activations+Zhuang,+Bohan+and+Liu,+Jing+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Effective+Training+of+Convolutional+Neural+Networks+with+Low-bitwidth+Weights+and+Activations">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2103.04216.pdf"><img class="imgP  right"   src="data/thumbnail/Yin2021PAMIvn_arXiv.jpg"></a><b>Virtual normal: enforcing geometric constraints for accurate and robust depth prediction</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>W. Yin, Y. Liu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2103.04216">arXiv</a><a href="data/bibtex/Yin2021PAMIvn.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Virtual+Normal:+Enforcing+Geometric+Constraints+for+Accurate+and+Robust+Depth+Prediction+Yin,+Wei+and+Liu,+Yifan+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Virtual+Normal:+Enforcing+Geometric+Constraints+for+Accurate+and+Robust+Depth+Prediction">semantic scholar</a><a href="https://git.io/Depth">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2105.00405.pdf"><img class="imgP  right"   src="data/thumbnail/Wang2021PANplus_arXiv.jpg"></a><b>PAN++: towards efficient and accurate end-to-end spotting of arbitrarily-shaped text</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>W. Wang, E. Xie, X. Li, X. Liu, D. Liang, Z. Yang, T. Lu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2105.00405">arXiv</a><a href="data/bibtex/Wang2021PANplus.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={PAN++}:+Towards+Efficient+and+Accurate+End-to-End+Spotting+of+Arbitrarily-Shaped+Text+Wang,+Wenhai+and+Xie,+Enze+and+Li,+Xiang+and+Liu,+Xuebo+and+Liang,+Ding+and+Yang,+Zhibo+and+Lu,+Tong+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q={PAN++}:+Towards+Efficient+and+Accurate+End-to-End+Spotting+of+Arbitrarily-Shaped+Text">semantic scholar</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1906.06013.pdf"><img class="imgP  right"   src="data/thumbnail/Li2021Text_arXiv.jpg"></a><b>Towards end-to-end text spotting in natural scenes</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>P. Wang, H. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1906.06013">arXiv</a><a href="data/bibtex/Li2021Text.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+End-to-End+Text+Spotting+in+Natural+Scenes+Wang,+Peng+and+Li,+Hui+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+End-to-End+Text+Spotting+in+Natural+Scenes">semantic scholar</a></p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>Diverse knowledge distillation for end-to-end person search</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>X. Zhang, X. Wang, J. Bian, C. Shen, M. You</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/ZhangPerson2021AAAI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Diverse+Knowledge+Distillation+for+End-to-end+Person+Search+Zhang,+Xinyu+and+Wang,+Xinlong+and+Bian,+Jia-Wang+and+Shen,+Chunhua+and+You,+Minyu">google scholar</a><a href="https://www.semanticscholar.org/search?q=Diverse+Knowledge+Distillation+for+End-to-end+Person+Search">semantic scholar</a></p>
</li>
<li><p><b>SA-BNN: state-aware binary neural network</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>C. Liu, P. Chen, B. Zhuang, C. Shen, B. Zhang, W. Ding</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/LiuBNN2021AAAI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SA-BNN}:+State-Aware+Binary+Neural+Network+Liu,+Chunlei+and+Chen,+Peng+and+Zhuang,+Bohan+and+Shen,+Chunhua+and+Zhang,+Baochang+and+Ding,+Wenrui">google scholar</a><a href="https://www.semanticscholar.org/search?q={SA-BNN}:+State-Aware+Binary+Neural+Network">semantic scholar</a></p>
</li>
<li><p><b>Deep reasoning network for few-shot semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Y. Zhuge, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <i>Proc. ACM International Conference on Multimedia (ACMMM&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png"  style="vertical-align: 3px" /> <a href="data/bibtex/MM2021B.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Reasoning+Network+for+Few-shot+Semantic+Segmentation+Zhuge,+Yuzhi+and+Shen,+Chunhua">google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Reasoning+Network+for+Few-shot+Semantic+Segmentation">semantic scholar</a></p>
</li>
</ol>
<div id="footer">
<div id="footer-text">
&copy; <b>Chunhua Shen</b>
&bull;
last update: 2021-07-09 16:24:14 ACST
&bull;
<a href="#" onClick="changewidth(1);return false" title="Expand page width"><b>&larr;&rarr;</b></a>
&bull;
<a href="#" onClick="changewidth(-1);return false" title="Reduce page width"><b>&rarr;&larr;</b></a>
<!-- Javascript -->
<script
    src="./script/jquery-1.6.2.min.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery.flot.min.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery-scroll.js"
    type="text/javascript">
</script>
<script
    src="./script/width_change.js"
    type="text/javascript">
</script>
<script
    src="./script/reverse_ol.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery.highlight.js"
    type="text/javascript">
</script>
<!-- Required for the jQuery.LocalScroll Plug-in -->
<script type="text/javascript">
    $(document).ready(function(){
    //
    $.localScroll();
    //
    // Round images
    //
	$(".rounded-img, .rounded-img2").load(function() {
	$(this).wrap(function(){
	return '<span class="' + $(this).attr('class')
                + '" style="background:url(' + $(this).attr('src')
                + ') no-repeat center center; width: '
                + $(this).width() + 'px; height: '
                + $(this).height() + 'px;" />';
		});
		$(this).css("opacity","0");
	});
      //
      //
      //  nav tab animation
        var navDuration = 150; //time in miliseconds
        $('#nav li a').hover(function() {
          $(this).animate({ paddingTop:"50px"  }, navDuration);
        }, function() {
             $(this).animate({ paddingTop:"31px"}, navDuration);
        });
        //
        // plot citation figure using jquery flot, 2012 July, CS
        //
        var flot_options = {
        legend: {
            show: false,
            margin: 10,
            backgroundOpacity: 0.5
                },
        bars:  {
            show: true,
            barWidth: 0.6,
            align: "center"
        },
        yaxis: {
            min: -20,
            tickFormatter: function(val, axis) {
                if (val < 50)
                    return " &nbsp; ";  // some string
                else
                    return val < axis.max ? val.toFixed(0) :   "  &nbsp;  ";
            }
        },
        grid: {
            borderWidth: 0
        }
    };  // end of flot_options
    $.getJSON("./data/cs_cite.json", function(json) {
       //succes - data loaded, now use plot:
       var plotarea = $("#citation_plot_holder");
       var data=[json.data];
       $.plot(plotarea , data, flot_options);
    });
//
// end of jquery flot
//
    changewidth( 0.9 );
//
//
//  highlight ``Shen''
    $("body p").highlight(['C. Shen', 'Chunhua Shen']);
//  highlight selected publication venues
    $("body p").highlight(['CVPR', 'ICCV', 'ECCV', 'ICML', 'NeurIPS',
    'TPAMI', 'IJCV', 'JMLR'],  { element: 'span', className: 'selected_venue' } );
//
    });
</script>
<!-- News ticker -->
<script type="text/javascript">
    function tick(){
        $('#ticker li:first').slideUp( function () { $(this).appendTo($('#ticker')).slideDown(); });
    }
    setInterval(function(){ tick () }, 5000);
</script>
</div>
</div>
</div>
</body>
</html>
