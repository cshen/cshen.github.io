<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en> <head><meta http-equiv=Content-Type content="text/html;charset=utf-8"><link rel=stylesheet href=css/fonts_import.css type=text/css><link rel=stylesheet href=css/cs.css type=text/css><link rel=stylesheet href=css/content.css type=text/css><!-- font family --><link href=//netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css rel=stylesheet><link rel=stylesheet href=https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css><link rel=stylesheet href=css/full_publication.css type=text/css><title>Chunhua Shen</title></head> <body> <div id=layout-content> <div id=menu> <div id=menucontainer> <ul id=nav> <li><a href=index.html target=_self>关于 index</a></li> <li><a href=paper.html target=_self>论文 publications</a></li> <li><a href=teaching.html target=_self>教学 teaching</a></li> </ul> </div> </div> <div id=toptitle> <h1>Selected Publications</h1> <div id=subtitle>Categorised <a href=paper.html target=“blank”>by year <i class="fa fa-clock-o" aria-hidden=true></i></a>, <a href=paper2.html target=“blank”>by venue <i class="fa fa-location-arrow" aria-hidden=true></i></a>. <a href=fullpaper.html target=“blank”>Full publication list <i class="fa fa-code" aria-hidden=true></i></a>. <a href=cshen_papers.pdf target=“blank”>Full list in PDF <i class="fa fa-file-pdf-o" aria-hidden=true></i></a>. </div> </div> <p><a href="https://scholar.google.com/citations?hl=en&user=Ljk2BvIAAAAJ&view_op=list_works&pagesize=100" target=“blank”>Google scholar (43018 citations) <i class="ai ai-google-scholar" aria-hidden=true></i></a>, <a href=https://dblp.org/pid/56/1673.html target=“blank”>DBLP <i class="ai ai-dblp ai-1x"></i></a>, <a href=https://arxiv.org/a/shen_c_1.html target=“blank”>arXiv <i class="ai ai-biorxiv ai-1x"></i></a>. </p> <h2>Journal: 52</h2> <h3>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI): 33</h3> <h3>International Journal of Computer Vision (IJCV): 17</h3> <h3>Journal of Machine Learning Research (JMLR): 1</h3> <h3>ACM Transactions on Graphics (TOG): 1</h3> <p><br> </p> <ol reversed> <li><p><b>Structured binary neural networks for image recognition</b> <br>\(\cdot\) <i>B. Zhuang, C. Shen, M. Tan, P. Chen, L. Liu, I. Reid</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2022</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1909.09934 target=“blank”>arXiv</a><a href=data/bibtex/Zhuang2022IJCV.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Structured+Binary+Neural+Networks+for+Image+Recognition+Zhuang,+Bohan+and+Shen,+Chunhua+and+Tan,+Mingkui+and+Chen,+Peng+and+Liu,+Lingqiao+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+Binary+Neural+Networks+for+Image+Recognition" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2105.11610.pdf><img class="imgP  right" src="data/thumbnail/Bian2021IJCV</u>arXiv.jpg"></a><b>Unsupervised scale-consistent depth learning from video</b> <br>\(\cdot\) <i>J. Bian, H. Zhan, N. Wang, Z. Li, L. Zhang, C. Shen, M. Cheng, I. Reid</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2105.11610 target=“blank”>arXiv</a><a href=data/bibtex/Bian2021IJCV.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Unsupervised+Scale-consistent+Depth+Learning+from+Video+Bian,+Jia-Wang+and+Zhan,+Huangying+and+Wang,+Naiyan+and+Li,+Zhichao+and+Zhang,+Le+and+Shen,+Chunhua+and+Cheng,+Ming-Ming+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Scale-consistent+Depth+Learning+from+Video" target=“blank”>semantic scholar</a><a href=https://github.com/JiawangBian/SC-SfMLearner-Release target=“blank”>project webpage</a> </p> </li> <li><p><b>Joint classification and regression for visual tracking with fully convolutional Siamese networks</b> <br>\(\cdot\) <i>Y. Cui, D. Guo, Y. Shao, Z. Wang, C. Shen, L. Zhang, S. Chen</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>. <br>\(\cdot\) <a href=data/bibtex/Cui2021Joint.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Joint+classification+and+regression+for+visual+tracking+with+fully+convolutional+{S}iamese+networks+Cui,+Ying+and+Guo,+Dongyan+and+Shao,+Yanyan+and+Wang,+Zhenhua+and+Shen,+Chunhua+and+Zhang,+Liyan+and+Chen,+Shengyong" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Joint+classification+and+regression+for+visual+tracking+with+fully+convolutional+{S}iamese+networks" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/IJCV2021Liuyl_arXiv.jpg><b>Exploring the capacity of an orderless box discretization network for multi-orientation scene text detection</b> <br>\(\cdot\) <i>Y. Liu, T. He, H. Chen, X. Wang, C. Luo, S. Zhang, C. Shen, L. Jin</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1912.09629 target=“blank”>arXiv</a><a href=data/bibtex/IJCV2021Liuyl.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Exploring+the+Capacity+of+an+Orderless+Box+Discretization+Network+for+Multi-orientation+Scene+Text+Detection+Liu,+Yuliang+and+He,+Tong+and+Chen,+Hao+and+Wang,+Xinyu+and+Luo,+Canjie+and+Zhang,+Shuaitao+and+Shen,+Chunhua+and+Jin,+Lianwen" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Exploring+the+Capacity+of+an+Orderless+Box+Discretization+Network+for+Multi-orientation+Scene+Text+Detection" target=“blank”>semantic scholar</a><a href=https://git.io/TextDet target=“blank”>project webpage</a> </p> </li> <li><p><b>NAS-FCOS: efficient search for object detection architectures</b> <br>\(\cdot\) <i>N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, Y. Zhang</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>. <br>\(\cdot\) <a href=data/bibtex/Wang2021IJCV_NAS.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={NAS-FCOS}:+Efficient+Search+for+Object+Detection+Architectures+Wang,+Ning+and+Gao,+Yang+and+Chen,+Hao+and+Wang,+Peng+and+Tian,+Zhi+and+Shen,+Chunhua+and+Zhang,+Yanning" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={NAS-FCOS}:+Efficient+Search+for+Object+Detection+Architectures" target=“blank”>semantic scholar</a><a href=https://github.com/Lausannen/NAS-FCOS target=“blank”>project webpage</a> </p> </li> <li><p><b>A dual-attention-guided network for ghost-free high dynamic range imaging</b> <br>\(\cdot\) <i>Q. Yan, D. Gong, Q. Shi, A. van den Hengel, C. Shen, I. Reid, Y. Zhang</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>. <br>\(\cdot\) <a href=data/bibtex/Yan2021Ghostfree.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=A+Dual-Attention-guided+network+for+ghost-free+high+dynamic+range+imaging+Yan,+Qingsen+and+Gong,+Dong+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Reid,+Ian+and+Zhang,+Yanning" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Dual-Attention-guided+network+for+ghost-free+high+dynamic+range+imaging" target=“blank”>semantic scholar</a><a href=https://github.com/qingsenyangit/AHDRNet target=“blank”>project webpage</a> </p> </li> <li><p><b>BiSeNet v2: bilateral network with guided aggregation for real-time semantic segmentation</b> <br>\(\cdot\) <i>C. Yu, C. Gao, J. Wang, G. Yu, C. Shen, N. Sang</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2004.02147 target=“blank”>arXiv</a><a href=data/bibtex/Yu2021BiSegV2.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={BiSeNet}+v2:+Bilateral+Network+with+Guided+Aggregation+for+Real-time+Semantic+Segmentation+Yu,+Changqian+and+Gao,+Changxin+and+Wang,+Jingbo+and+Yu,+Gang+and+Shen,+Chunhua+and+Sang,+Nong" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={BiSeNet}+v2:+Bilateral+Network+with+Guided+Aggregation+for+Real-time+Semantic+Segmentation" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Memory-efficient hierarchical neural architecture search for image restoration</b> <br>\(\cdot\) <i>H. Zhang, Y. Li, H. Chen, C. Gong, Z. Bai, C. Shen</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2012.13212 target=“blank”>arXiv</a><a href=data/bibtex/Haokui2021NAS.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Memory-Efficient+Hierarchical+Neural+Architecture+Search+for+Image+Restoration+Zhang,+Haokui+and+Li,+Ying+and+Chen,+Hao+and+Gong,+Chengrong+and+Bai,+Zongwen+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Memory-Efficient+Hierarchical+Neural+Architecture+Search+for+Image+Restoration" target=“blank”>semantic scholar</a><a href=https://github.com/hkzhang91/HiNAS target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Luo2020IJCV_arXiv.jpg><b>Separating content from style using adversarial learning for recognizing text in the wild</b> <br>\(\cdot\) <i>C. Luo, Q. Lin, Y. Liu, L. Jin, C. Shen</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2001.04189 target=“blank”>arXiv</a><a href=data/bibtex/Luo2020IJCV.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Separating+Content+from+Style+Using+Adversarial+Learning+for+Recognizing+Text+in+the+Wild+Luo,+Canjie+and+Lin,+Qingxiang+and+Liu,+Yuliang+and+Jin,+Lianwen+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Separating+Content+from+Style+Using+Adversarial+Learning+for+Recognizing+Text+in+the+Wild" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1806.01576.pdf><img class="imgP  right" src="data/thumbnail/Adaptive2019Zhang</u>arXiv.jpg"></a><b>Adaptive importance learning for improving lightweight image super-resolution network</b> <br>\(\cdot\) <i>L. Zhang, P. Wang, C. Shen, L. Liu, W. Wei, Y. Zhang, A. van den Hengel</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1806.01576 target=“blank”>arXiv</a><a href=data/bibtex/Adaptive2019Zhang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Adaptive+Importance+Learning+for+Improving+Lightweight+Image+Super-resolution+Network+Zhang,+Lei+and+Wang,+Peng+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Wei,+Wei+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Adaptive+Importance+Learning+for+Improving+Lightweight+Image+Super-resolution+Network" target=“blank”>semantic scholar</a><a href=https://tinyurl.com/Super-resolution-Network target=“blank”>project webpage</a> </p> </li> <li><p><b>Cluster sparsity field: an internal hyperspectral imagery prior for reconstruction</b> <br>\(\cdot\) <i>L. Zhang, W. Wei, Y. Zhang, C. Shen, A. van den Hengel, Q. Shi</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2018</i>. <br>\(\cdot\) <a href=https://www.researchgate.net/publication/323914969_Cluster_Sparsity_Field_An_Internal_Hyperspectral_Imagery_Prior_for_Reconstruction target=“blank”>pdf</a><a href=data/bibtex/Zhang2018IJCV.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Cluster+Sparsity+Field:+An+Internal+Hyperspectral+Imagery+Prior+for+Reconstruction+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Cluster+Sparsity+Field:+An+Internal+Hyperspectral+Imagery+Prior+for+Reconstruction" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/IJCV2017Lin_arXiv.jpg><b>Structured learning of binary codes with column generation for optimizing ranking measures</b> <br>\(\cdot\) <i>G. Lin, F. Liu, C. Shen, J. Wu, H. Shen</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1602.06654 target=“blank”>arXiv</a><a href=data/bibtex/IJCV2017Lin.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Structured+Learning+of+Binary+Codes+with+Column+Generation+for+Optimizing+Ranking+Measures+Lin,+Guosheng+and+Liu,+Fayao+and+Shen,+Chunhua+and+Wu,+Jianxin+and+Shen,+Heng+Tao" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+Learning+of+Binary+Codes+with+Column+Generation+for+Optimizing+Ranking+Measures" target=“blank”>semantic scholar</a><a href=https://bitbucket.org/guosheng/structhash target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Yao2016IJCV_arXiv.jpg><b>Mining mid-level visual patterns with deep CNN activations</b> <br>\(\cdot\) <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1506.06343 target=“blank”>arXiv</a><a href=http://rdcu.be/j1mA target=“blank”>link</a><a href=data/bibtex/Yao2016IJCV.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Mining+Mid-level+Visual+Patterns+with+Deep+{CNN}+Activations+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Mining+Mid-level+Visual+Patterns+with+Deep+{CNN}+Activations" target=“blank”>semantic scholar</a><a href=https://github.com/yaoliUoA/MDPM target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1404.5009.pdf><img class="imgP  right" src="data/thumbnail/BnB2015Wang</u>arXiv.jpg"></a><b>Efficient semidefinite branch-and-cut for MAP-MRF inference</b> <br>\(\cdot\) <i>P. Wang, C. Shen, A. van den Hengel, P. Torr</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1404.5009 target=“blank”>arXiv</a><a href=http://doi.org/10.1007/s11263-015-0865-2 target=“blank”>link</a><a href=data/bibtex/BnB2015Wang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Semidefinite+Branch-and-Cut+for+{MAP-MRF}+Inference+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Torr,+Philip" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+Semidefinite+Branch-and-Cut+for+{MAP-MRF}+Inference" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Zhang2015IJCV_arXiv.jpg><b>Unsupervised feature learning for dense correspondences across scenes</b> <br>\(\cdot\) <i>C. Zhang, C. Shen, T. Shen</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1501.00642 target=“blank”>arXiv</a><a href=data/bibtex/Zhang2015IJCV.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Unsupervised+Feature+Learning+for+Dense+Correspondences+across+Scenes+Zhang,+Chao+and+Shen,+Chunhua+and+Shen,+Tingzhi" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Feature+Learning+for+Dense+Correspondences+across+Scenes" target=“blank”>semantic scholar</a><a href=https://bitbucket.org/chhshen/ufl target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1401.8126.pdf><img class="imgP  right" src="data/thumbnail/Harandi2015IJCV</u>arXiv.jpg"></a><b>Extrinsic methods for coding and dictionary learning on Grassmann manifolds</b> <br>\(\cdot\) <i>M. Harandi, R. Hartley, C. Shen, B. Lovell, C. Sanderson</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2015</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1401.8126 target=“blank”>arXiv</a><a href=data/bibtex/Harandi2015IJCV.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Extrinsic+Methods+for+Coding+and+Dictionary+Learning+on+{G}rassmann+Manifolds+Harandi,+Mehrtash+and+Hartley,+Richard+and+Shen,+Chunhua+and+Lovell,+Brian+and+Sanderson,+Conrad" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Extrinsic+Methods+for+Coding+and+Dictionary+Learning+on+{G}rassmann+Manifolds" target=“blank”>semantic scholar</a><a href=https://github.com/chhshen/Grassmann/ target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1301.2032.pdf><img class="imgP  right" src="data/thumbnail/FisherBoost2013IJCV</u>arXiv.jpg"></a><b>Training effective node classifiers for cascade classification</b> <br>\(\cdot\) <i>C. Shen, P. Wang, S. Paisitkriangkrai, A. van den Hengel</i>. <br>\(\cdot\) <i>International Journal of Computer Vision (IJCV), 2013</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1301.2032 target=“blank”>arXiv</a><a href=http://link.springer.com/article/10.1007%2Fs11263-013-0608-1 target=“blank”>link</a><a href=data/bibtex/FisherBoost2013IJCV.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Training+Effective+Node+Classifiers+for+Cascade+Classification+Shen,+Chunhua+and+Wang,+Peng+and+Paisitkriangkrai,+Sakrapee+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Training+Effective+Node+Classifiers+for+Cascade+Classification" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/JMLR2012Shen_arXiv.jpg><b>Positive semidefinite metric learning using boosting-like algorithms</b> <br>\(\cdot\) <i>C. Shen, J. Kim, L. Wang, A. van den Hengel</i>. <br>\(\cdot\) <i>Journal of Machine Learning Research (JMLR), 2012</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1104.4704 target=“blank”>arXiv</a><a href=http://jmlr.csail.mit.edu/papers/v13/shen12a.html target=“blank”>link</a><a href=data/bibtex/JMLR2012Shen.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Positive+Semidefinite+Metric+Learning+Using+Boosting-like+Algorithms+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Positive+Semidefinite+Metric+Learning+Using+Boosting-like+Algorithms" target=“blank”>semantic scholar</a><a href=https://code.google.com/archive/p/boosting/downloads target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Liu2020TOG_arXiv.jpg><b>Real-time image smoothing via iterative least squares</b> <br>\(\cdot\) <i>W. Liu, P. Zhang, X. Huang, J. Yang, C. Shen, I. Reid</i>. <br>\(\cdot\) <i>ACM Transactions on Graphics (TOG), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2003.07504 target=“blank”>arXiv</a><a href=https://doi.org/10.1145/3388887 target=“blank”>link</a><a href=data/bibtex/Liu2020TOG.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Real-time+image+smoothing+via+iterative+least+squares+Liu,+Wei+and+Zhang,+Pingping+and+Huang,+Xiaolin+and+Yang,+Jie+and+Shen,+Chunhua+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Real-time+image+smoothing+via+iterative+least+squares" target=“blank”>semantic scholar</a><a href=https://github.com/wliusjtu/Real-time-Image-Smoothing-via-Iterative-Least-Squares target=“blank”>project webpage</a> </p> </li> <li><p><b>Instance and panoptic segmentation using conditional convolutions</b> <br>\(\cdot\) <i>Z. Tian, B. Zhang, H. Chen, C. Shen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2102.03026 target=“blank”>arXiv</a><a href=data/bibtex/CondInst2022Tian.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Instance+and+Panoptic+Segmentation+Using+Conditional+Convolutions+Tian,+Zhi+and+Zhang,+Bowen+and+Chen,+Hao+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Instance+and+Panoptic+Segmentation+Using+Conditional+Convolutions" target=“blank”>semantic scholar</a><a href=https://github.com/aim-uofa/AdelaiDet/ target=“blank”>project webpage</a> </p> </li> <li><p><b>Auto-rectify network for unsupervised indoor depth estimation</b> <br>\(\cdot\) <i>J. Bian, H. Zhan, N. Wang, T. Chin, C. Shen, I. Reid</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>. <br>\(\cdot\) <a href=data/bibtex/Autorectify2021Bian.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Auto-Rectify+Network+for+Unsupervised+Indoor+Depth+Estimation+Bian,+Jia-Wang+and+Zhan,+Huangying+and+Wang,+Naiyan+and+Chin,+Tat-Jun+and+Shen,+Chunhua+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Auto-Rectify+Network+for+Unsupervised+Indoor+Depth+Estimation" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>ABCNet v2: adaptive bezier-curve network for real-time end-to-end text spotting</b> <br>\(\cdot\) <i>Y. Liu, C. Shen, L. Jin, T. He, P. Chen, C. Liu, H. Chen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2105.03620 target=“blank”>arXiv</a><a href=data/bibtex/Liu2021ABCNetv2.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={ABCNet}+v2:+Adaptive+Bezier-Curve+Network+for+Real-time+End-to-end+Text+Spotting+Liu,+Yuliang+and+Shen,+Chunhua+and+Jin,+Lianwen+and+He,+Tong+and+Chen,+Peng+and+Liu,+Chongyu+and+Chen,+Hao" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={ABCNet}+v2:+Adaptive+Bezier-Curve+Network+for+Real-time+End-to-end+Text+Spotting" target=“blank”>semantic scholar</a><a href=https://git.io/AdelaiDet target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Li2021Text_arXiv.jpg><b>Towards end-to-end text spotting in natural scenes</b> <br>\(\cdot\) <i>P. Wang, H. Li, C. Shen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1906.06013 target=“blank”>arXiv</a><a href=data/bibtex/Li2021Text.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Towards+End-to-End+Text+Spotting+in+Natural+Scenes+Wang,+Peng+and+Li,+Hui+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+End-to-End+Text+Spotting+in+Natural+Scenes" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Wang2021PANplus_arXiv.jpg><b>PAN++: towards efficient and accurate end-to-end spotting of arbitrarily-shaped text</b> <br>\(\cdot\) <i>W. Wang, E. Xie, X. Li, X. Liu, D. Liang, Z. Yang, T. Lu, C. Shen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2105.00405 target=“blank”>arXiv</a><a href=data/bibtex/Wang2021PANplus.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={PAN++}:+Towards+Efficient+and+Accurate+End-to-End+Spotting+of+Arbitrarily-Shaped+Text+Wang,+Wenhai+and+Xie,+Enze+and+Li,+Xiang+and+Liu,+Xuebo+and+Liang,+Ding+and+Yang,+Zhibo+and+Lu,+Tong+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={PAN++}:+Towards+Efficient+and+Accurate+End-to-End+Spotting+of+Arbitrarily-Shaped+Text" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>SOLO: a simple framework for instance segmentation</b> <br>\(\cdot\) <i>X. Wang, R. Zhang, C. Shen, T. Kong, L. Li</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2106.15947 target=“blank”>arXiv</a><a href=data/bibtex/WXL2021SOLO.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={SOLO}:+A+Simple+Framework+for+Instance+Segmentation+Wang,+Xinlong+and+Zhang,+Rufeng+and+Shen,+Chunhua+and+Kong,+Tao+and+Li,+Lei" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={SOLO}:+A+Simple+Framework+for+Instance+Segmentation" target=“blank”>semantic scholar</a><a href=https://git.io/AdelaiDet target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Yin2021PAMIvn_arXiv.jpg><b>Virtual normal: enforcing geometric constraints for accurate and robust depth prediction</b> <br>\(\cdot\) <i>W. Yin, Y. Liu, C. Shen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2103.04216 target=“blank”>arXiv</a><a href=data/bibtex/Yin2021PAMIvn.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Virtual+Normal:+Enforcing+Geometric+Constraints+for+Accurate+and+Robust+Depth+Prediction+Yin,+Wei+and+Liu,+Yifan+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Virtual+Normal:+Enforcing+Geometric+Constraints+for+Accurate+and+Robust+Depth+Prediction" target=“blank”>semantic scholar</a><a href=https://git.io/Depth target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Zhuang2021Quantization_arXiv.jpg><b>Effective training of convolutional neural networks with low-bitwidth weights and activations</b> <br>\(\cdot\) <i>B. Zhuang, J. Liu, M. Tan, L. Liu, I. Reid, C. Shen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1908.04680 target=“blank”>arXiv</a><a href=data/bibtex/Zhuang2021Quantization.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Effective+Training+of+Convolutional+Neural+Networks+with+Low-bitwidth+Weights+and+Activations+Zhuang,+Bohan+and+Liu,+Jing+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Effective+Training+of+Convolutional+Neural+Networks+with+Low-bitwidth+Weights+and+Activations" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2008.00942.pdf><img class="imgP  right" src="data/thumbnail/Cao2020GAN</u>arXiv.jpg"></a><b>Improving generative adversarial networks with local coordinate coding</b> <br>\(\cdot\) <i>J. Cao, Y. Guo, Q. Wu, C. Shen, J. Huang, M. Tan</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2008.00942 target=“blank”>arXiv</a><a href=data/bibtex/Cao2020GAN.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Improving+Generative+Adversarial+Networks+with+Local+Coordinate+Coding+Cao,+Jiezhang+and+Guo,+Yong+and+Wu,+Qingyao+and+Shen,+Chunhua+and+Huang,+Junzhou+and+Tan,+Mingkui" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Improving+Generative+Adversarial+Networks+with+Local+Coordinate+Coding" target=“blank”>semantic scholar</a><a href=https://github.com/SCUTjinchengli/LCCGAN-v2 target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1711.00253.pdf><img class="imgP  right" src="data/thumbnail/Chen2019PAMI</u>arXiv.jpg"></a><b>Adversarial learning of structure-aware fully convolutional networks for landmark localization</b> <br>\(\cdot\) <i>Y. Chen, C. Shen, H. Chen, X. Wei, L. Liu, J. Yang</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1711.00253 target=“blank”>arXiv</a><a href=https://doi.org/10.1109/TPAMI.2019.2901875 target=“blank”>link</a><a href=data/bibtex/Chen2019PAMI.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Adversarial+Learning+of+Structure-Aware+Fully+Convolutional+Networks+for+Landmark+Localization+Chen,+Yu+and+Shen,+Chunhua+and+Chen,+Hao+and+Wei,+Xiu-Shen+and+Liu,+Lingqiao+and+Yang,+Jian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Adversarial+Learning+of+Structure-Aware+Fully+Convolutional+Networks+for+Landmark+Localization" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Liu2020PAMI_arXiv.jpg><b>Structured knowledge distillation for dense prediction</b> <br>\(\cdot\) <i>Y. Liu, C. Shun, J. Wang, C. Shen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1903.04197 target=“blank”>arXiv</a><a href=https://ieeexplore.ieee.org/document/9115859 target=“blank”>link</a><a href=data/bibtex/Liu2020PAMI.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Structured+Knowledge+Distillation+for+Dense+Prediction+Liu,+Yifan+and+Shun,+Changyong+and+Wang,+Jingdong+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+Knowledge+Distillation+for+Dense+Prediction" target=“blank”>semantic scholar</a><a href=https://github.com/irfanICMLL/structure_knowledge_distillation target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Lu2020PAMIIndexNet_arXiv.jpg><b>Index networks</b> <br>\(\cdot\) <i>H. Lu, Y. Dai, C. Shen, S. Xu</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1908.09895 target=“blank”>arXiv</a><a href=https://doi.org/10.1109/TPAMI.2020.3004474 target=“blank”>link</a><a href=data/bibtex/Lu2020PAMIIndexNet.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Index+Networks+Lu,+Hao+and+Dai,+Yutong+and+Shen,+Chunhua+and+Xu,+Songcen" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Index+Networks" target=“blank”>semantic scholar</a><a href=https://git.io/IndexNet target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Zhang2020OrderlessReID_arXiv.jpg><b>Ordered or orderless: a revisit for video based person re-identification</b> <br>\(\cdot\) <i>L. Zhang, Z. Shi, J. Zhou, M. Cheng, Y. Liu, J. Bian, Z. Zeng, C. Shen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1912.11236 target=“blank”>arXiv</a><a href=https://doi.org/10.1109/TPAMI.2020.2976969 target=“blank”>link</a><a href=data/bibtex/Zhang2020OrderlessReID.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Ordered+or+Orderless:+A+Revisit+for+Video+based+Person+Re-Identification+Zhang,+Le+and+Shi,+Zenglin+and+Zhou,+Joey+Tianyi+and+Cheng,+Ming-Ming+and+Liu,+Yun+and+Bian,+Jia-Wang+and+Zeng,+Zeng+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Ordered+or+Orderless:+A+Revisit+for+Video+based+Person+Re-Identification" target=“blank”>semantic scholar</a><a href=https://github.com/ZhangLeUestc/VideoReid-TPAMI2020 target=“blank”>project webpage</a> </p> </li> <li><p><b>Plenty is plague: fine-grained learning for visual question answering</b> <br>\(\cdot\) <i>Y. Zhou, R. Ji, J. Su, X. Sun, D. Meng, Y. Gao, C. Shen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>. <br>\(\cdot\) <a href=https://doi.org/10.1109/TPAMI.2019.2956699 target=“blank”>link</a><a href=data/bibtex/Zhou2020TPAMIZhou.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Plenty+Is+Plague:+Fine-Grained+Learning+for+Visual+Question+Answering+Zhou,+Yiyi+and+Ji,+Rongrong+and+Su,+Jinsong+and+Sun,+Xiaoshuai+and+Meng,+Deyu+and+Gao,+Yue+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Plenty+Is+Plague:+Fine-Grained+Learning+for+Visual+Question+Answering" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>RefineNet: multi-path refinement networks for dense prediction</b> <br>\(\cdot\) <i>G. Lin, F. Liu, A. Milan, C. Shen, I. Reid</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2019</i>. <br>\(\cdot\) <a href=https://doi.org/10.1109/TPAMI.2019.2893630 target=“blank”>link</a><a href=data/bibtex/Fayao2019PAMI.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={RefineNet}:+Multi-Path+Refinement+Networks+for+Dense+Prediction+Lin,+Guosheng+and+Liu,+Fayao+and+Milan,+Anton+and+Shen,+Chunhua+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={RefineNet}:+Multi-Path+Refinement+Networks+for+Dense+Prediction" target=“blank”>semantic scholar</a><a href=https://github.com/guosheng/refinenet target=“blank”>project webpage</a> </p> <ol reversed> <li><p>Pytorch code is <a href=https://github.com/DrSleep/refinenet-pytorch target=“blank”>here</a>. </p> </li></ol> </li> <li><p><b>Ordinal constraint binary coding for approximate nearest neighbor search</b> <br>\(\cdot\) <i>H. Liu, R. Ji, J. Wang, C. Shen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018</i>. <br>\(\cdot\) <a href=https://www.researchgate.net/publication/324053386_Ordinal_Constraint_Binary_Coding_for_Approximate_Nearest_Neighbor_Search target=“blank”>pdf</a><a href=data/bibtex/HLiu2018TPAMI.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Ordinal+Constraint+Binary+Coding+for+Approximate+Nearest+Neighbor+Search+Liu,+Hong+and+Ji,+Rongrong+and+Wang,+Jingdong+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Ordinal+Constraint+Binary+Coding+for+Approximate+Nearest+Neighbor+Search" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Wang2017FVQA_arXiv.jpg><b>FVQA: fact-based visual question answering</b> <br>\(\cdot\) <i>P. Wang, Q. Wu, C. Shen, A. Dick, A. van den Hengel</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1606.05433 target=“blank”>arXiv</a><a href=data/bibtex/Wang2017FVQA.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={FVQA}:+Fact-based+Visual+Question+Answering+Wang,+Peng+and+Wu,+Qi+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={FVQA}:+Fact-based+Visual+Question+Answering" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Lin2017Semantic_arXiv.jpg><b>Exploring context with deep structured models for semantic segmentation</b> <br>\(\cdot\) <i>G. Lin, C. Shen, A. van den Hengel, I. Reid</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1603.03183 target=“blank”>arXiv</a><a href=data/bibtex/Lin2017Semantic.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Exploring+Context+with+Deep+Structured+models+for+Semantic+Segmentation+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Exploring+Context+with+Deep+Structured+models+for+Semantic+Segmentation" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1510.00921.pdf><img class="imgP  right" src="data/thumbnail/Cross2017Liu</u>arXiv.jpg"></a><b>Cross-convolutional-layer pooling for image recognition</b> <br>\(\cdot\) <i>L. Liu, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1510.00921 target=“blank”>arXiv</a><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7779086" target=“blank”>link</a><a href=data/bibtex/Cross2017Liu.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Cross-convolutional-layer+Pooling+for+Image+Recognition+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Cross-convolutional-layer+Pooling+for+Image+Recognition" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/TPAMI2017Liu_arXiv.jpg><b>Compositional model based Fisher vector coding for image classification</b> <br>\(\cdot\) <i>L. Liu, P. Wang, C. Shen, L. Wang, A. van den Hengel, C. Wang, H. Shen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1601.04143 target=“blank”>arXiv</a><a href=data/bibtex/TPAMI2017Liu.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Compositional+Model+based+{F}isher+Vector+Coding+for+Image+Classification+Liu,+Lingqiao+and+Wang,+Peng+and+Shen,+Chunhua+and+Wang,+Lei+and+{van+den+Hengel},+Anton+and+Wang,+Chao+and+Shen,+Heng+Tao" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Compositional+Model+based+{F}isher+Vector+Coding+for+Image+Classification" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Wu2017External_arXiv.jpg><b>Image captioning and visual question answering based on attributes and external knowledge</b> <br>\(\cdot\) <i>Q. Wu, C. Shen, P. Wang, A. Dick, A. van den Hengel</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1603.02814 target=“blank”>arXiv</a><a href=data/bibtex/Wu2017External.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Image+Captioning+and+Visual+Question+Answering+Based+on+Attributes+and+External+Knowledge+Wu,+Qi+and+Shen,+Chunhua+and+Wang,+Peng+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Image+Captioning+and+Visual+Question+Answering+Based+on+Attributes+and+External+Knowledge" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Xi2015TPAMI_arXiv.jpg><b>Online metric-weighted linear representations for robust visual tracking</b> <br>\(\cdot\) <i>X. Li, C. Shen, A. Dick, Z. Zhang, Y. Zhuang</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1507.05737 target=“blank”>arXiv</a><a href=data/bibtex/Xi2015TPAMI.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Online+Metric-Weighted+Linear+Representations+for+Robust+Visual+Tracking+Li,+Xi+and+Shen,+Chunhua+and+Dick,+Anthony+and+Zhang,+Zhongfei+and+Zhuang,+Yueting" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Online+Metric-Weighted+Linear+Representations+for+Robust+Visual+Tracking" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Depth2015Liu_arXiv.jpg><b>Learning depth from single monocular images using deep convolutional neural fields</b> <br>\(\cdot\) <i>F. Liu, C. Shen, G. Lin, I. Reid</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1502.07411 target=“blank”>arXiv</a><a href=http://dx.doi.org/10.1109/TPAMI.2015.2505283 target=“blank”>link</a><a href=data/bibtex/Depth2015Liu.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Learning+Depth+from+Single+Monocular+Images+Using+Deep+Convolutional+Neural+Fields+Liu,+Fayao+and+Shen,+Chunhua+and+Lin,+Guosheng+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Depth+from+Single+Monocular+Images+Using+Deep+Convolutional+Neural+Fields" target=“blank”>semantic scholar</a><a href=http://goo.gl/rAKWrS target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Liu2015TPAMI_arXiv.jpg><b>A generalized probabilistic framework for compact codebook creation</b> <br>\(\cdot\) <i>L. Liu, L. Wang, C. Shen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1401.7713 target=“blank”>arXiv</a><a href=http://doi.org/10.1109/TPAMI.2015.2441069 target=“blank”>link</a><a href=data/bibtex/Liu2015TPAMI.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=A+Generalized+Probabilistic+Framework+for+Compact+Codebook+Creation+Liu,+Lingqiao+and+Wang,+Lei+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Generalized+Probabilistic+Framework+for+Compact+Codebook+Creation" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Paisitkriangkrai2015TPAMI_arXiv.jpg><b>Pedestrian detection with spatially pooled features and structured ensemble learning</b> <br>\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1409.5209 target=“blank”>arXiv</a><a href=http://doi.org/10.1109/TPAMI.2015.2474388 target=“blank”>link</a><a href=data/bibtex/Paisitkriangkrai2015TPAMI.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Pedestrian+Detection+with+Spatially+Pooled+Features+and+Structured+Ensemble+Learning+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Pedestrian+Detection+with+Spatially+Pooled+Features+and+Structured+Ensemble+Learning" target=“blank”>semantic scholar</a><a href=https://github.com/chhshen/pedestrian-detection target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1411.7564.pdf><img class="imgP  right" src="data/thumbnail/BQP2015Wang</u>arXiv.jpg"></a><b>Large-scale binary quadratic optimization using semidefinite relaxation and applications</b> <br>\(\cdot\) <i>P. Wang, C. Shen, A. van den Hengel, P. Torr</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1411.7564 target=“blank”>arXiv</a><a href=http://dx.doi.org/10.1109/TPAMI.2016.2541146 target=“blank”>link</a><a href=data/bibtex/BQP2015Wang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Large-scale+Binary+Quadratic+Optimization+Using+Semidefinite+Relaxation+and+Applications+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Torr,+Philip+H.+S." target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Large-scale+Binary+Quadratic+Optimization+Using+Semidefinite+Relaxation+and+Applications" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1408.5574.pdf><img class="imgP  right" src="data/thumbnail/FastHash2015Lin</u>arXiv.jpg"></a><b>Supervised hashing using graph cuts and boosted decision trees</b> <br>\(\cdot\) <i>G. Lin, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2015</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1408.5574 target=“blank”>arXiv</a><a href=http://dx.doi.org/10.1109/TPAMI.2015.2404776 target=“blank”>link</a><a href=data/bibtex/FastHash2015Lin.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Supervised+Hashing+Using+Graph+Cuts+and+Boosted+Decision+Trees+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Supervised+Hashing+Using+Graph+Cuts+and+Boosted+Decision+Trees" target=“blank”>semantic scholar</a><a href=https://bitbucket.org/chhshen/fasthash/ target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Shen2014SBoosting_PDF.jpg><b>StructBoost: Boosting methods for predicting structured output variables</b> <br>\(\cdot\) <i>C. Shen, G. Lin, A. van den Hengel</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1302.3283 target=“blank”>arXiv</a><a href=http://dx.doi.org/10.1109/TPAMI.2014.2315792 target=“blank”>link</a><a href=http://goo.gl/goCVLK target=“blank”>pdf</a><a href=data/bibtex/Shen2014SBoosting.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={StructBoost}:+{B}oosting+Methods+for+Predicting+Structured+Output+Variables+Shen,+Chunhua+and+Lin,+Guosheng+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={StructBoost}:+{B}oosting+Methods+for+Predicting+Structured+Output+Variables" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>A hierarchical word-merging algorithm with class separability measure</b> <br>\(\cdot\) <i>L. Wang, L. Zhou, C. Shen, L. Liu, H. Liu</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014</i>. <br>\(\cdot\) <a href=https://bitbucket.org/chhshen/chhshen.bitbucket.org/src/be12d4ef8deb6207ec97f0fdac6efbe2df151b59/_download/TPAMI14Wang.pdf target=“blank”>pdf</a><a href=data/bibtex/Wang2014PAMI.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=A+Hierarchical+Word-merging+Algorithm+with+Class+Separability+Measure+Wang,+Lei+and+Zhou,+Luping+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Liu,+Huan" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Hierarchical+Word-merging+Algorithm+with+Class+Separability+Measure" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/TPAMI2013Xi_PDF.jpg><b>Incremental learning of 3D-DCT compact representations for robust visual tracking</b> <br>\(\cdot\) <i>X. Li, A. Dick, C. Shen, A. van den Hengel, H. Wang</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2013</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1207.3389 target=“blank”>arXiv</a><a href=http://dx.doi.org/10.1109/TPAMI.2012.166 target=“blank”>link</a><a href="https://sites.google.com/site/chhshen/publication/tpami12xi.pdf?attredirects=1" target=“blank”>pdf</a><a href=data/bibtex/TPAMI2013Xi.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Incremental+Learning+of+{3D-DCT}+Compact+Representations+for+Robust+Visual+Tracking+Li,+Xi+and+Dick,+Anthony+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Wang,+Hanzi" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Incremental+Learning+of+{3D-DCT}+Compact+Representations+for+Robust+Visual+Tracking" target=“blank”>semantic scholar</a><a href=https://github.com/chhshen/DCT-Tracking/ target=“blank”>project webpage</a> </p> </li> <li><p><b>UBoost: Boosting with the Universum</b> <br>\(\cdot\) <i>C. Shen, P. Wang, F. Shen, H. Wang</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2012</i>. <br>\(\cdot\) <a href=http://hdl.handle.net/2440/67027 target=“blank”>pdf</a><a href=data/bibtex/UBoost2011Shen.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={UBoost}:+{B}oosting+with+the+{U}niversum+Shen,+Chunhua+and+Wang,+Peng+and+Shen,+Fumin+and+Wang,+Hanzi" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={UBoost}:+{B}oosting+with+the+{U}niversum" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Dual2010Shen_arXiv.jpg><b>On the dual formulation of boosting algorithms</b> <br>\(\cdot\) <i>C. Shen, H. Li</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2010</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/0901.3590 target=“blank”>arXiv</a><a href=http://dx.doi.org/10.1109/TPAMI.2010.47 target=“blank”>link</a><a href=data/bibtex/Dual2010Shen.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=On+the+Dual+Formulation+of+Boosting+Algorithms+Shen,+Chunhua+and+Li,+Hanxi" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=On+the+Dual+Formulation+of+Boosting+Algorithms" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Adaptive object tracking based on an effective appearance filter</b> <br>\(\cdot\) <i>H. Wang, D. Suter, K. Schindler, C. Shen</i>. <br>\(\cdot\) <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2007</i>. <br>\(\cdot\) <a href=http://dx.doi.org/10.1109/TPAMI.2007.1112 target=“blank”>link</a><a href=http://goo.gl/6rQTA1 target=“blank”>pdf</a><a href=data/bibtex/Adaptive2007Wang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Adaptive+object+tracking+based+on+an+effective+appearance+filter+Wang,+Hanzi+and+Suter,+David+and+Schindler,+Konrad+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Adaptive+object+tracking+based+on+an+effective+appearance+filter" target=“blank”>semantic scholar</a> </p> <ol reversed> <li><p>Featured article of September issue 2007. </p> </li> </ol> </li> </ol> <h2>Conference: 146</h2> <h3>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR): 88</h3> <h3>Proc. IEEE International Conference on Computer Vision (ICCV): 25</h3> <h3>Proc. European Conference on Computer Vision (ECCV): 20</h3> <h3>Proc. International Conference on Machine Learning (ICML): 3</h3> <h3>Proc. Advances in Neural Information Processing Systems (NeurIPS): 10</h3> <p><br> </p> <ol reversed> <li><p><b>Boosting robustness of image matting with context assembling and strong data augmentation</b> <br>\(\cdot\) <i>Y. Dai, B. Price, H. Zhang, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;22), 2022</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2201.06889 target=“blank”>arXiv</a><a href=data/bibtex/Dai2022Matting.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Boosting+Robustness+of+Image+Matting+with+Context+Assembling+and+Strong+Data+Augmentation+Dai,+Yutong+and+Price,+Brian+and+Zhang,+He+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Boosting+Robustness+of+Image+Matting+with+Context+Assembling+and+Strong+Data+Augmentation" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Catching both gray and black swans: open-set supervised anomaly detection</b> <br>\(\cdot\) <i>C. Ding, G. Pan, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;22), 2022</i>. <br>\(\cdot\) <a href=data/bibtex/Ding2022Catching.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Catching+Both+Gray+and+Black+Swans:+Open-set+Supervised+Anomaly+Detection+Ding,+Choubo+and+Pan,+Guansong+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Catching+Both+Gray+and+Black+Swans:+Open-set+Supervised+Anomaly+Detection" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>RigidFlow: self-supervised scene flow learning on point clouds by local rigidity prior</b> <br>\(\cdot\) <i>R. Li, C. Zhang, G. Lin, Z. Wang, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;22), 2022</i>. <br>\(\cdot\) <a href=data/bibtex/Li2022Rigid.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={RigidFlow}:+Self-Supervised+Scene+Flow+Learning+on+Point+Clouds+by+Local+Rigidity+Prior+Li,+Ruibo+and+Zhang,+Chi+and+Lin,+Guosheng+and+Wang,+Zhe+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={RigidFlow}:+Self-Supervised+Scene+Flow+Learning+on+Point+Clouds+by+Local+Rigidity+Prior" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Retrieval augmented classification for long-tail visual recognition</b> <br>\(\cdot\) <i>A. Long, W. Yin, T. Ajanthan, V. Nguyen, P. Purkait, R. Garg, A. Blair, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;22), 2022</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2202.11233 target=“blank”>arXiv</a><a href=data/bibtex/Long2022RAC.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Retrieval+Augmented+Classification+for+Long-Tail+Visual+Recognition+Long,+Alexander+and+Yin,+Wei+and+Ajanthan,+Thalaiyasingam+and+Nguyen,+Vu+and+Purkait,+Pulak+and+Garg,+Ravi+and+Blair,+Alan+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Retrieval+Augmented+Classification+for+Long-Tail+Visual+Recognition" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>FreeSOLO: learning to segment objects without annotations</b> <br>\(\cdot\) <i>X. Wang, Z. Yu, S. De Mello, J. Kautz, A. Anandkumar, C. Shen, J. Alvarez</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;22), 2022</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2202.12181 target=“blank”>arXiv</a><a href=data/bibtex/Wang2022Solo.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={FreeSOLO}:+Learning+to+Segment+Objects+without+Annotations+Wang,+Xinlong+and+Yu,+Zhiding+and+{De+Mello},+Shalini+and+Kautz,+Jan+and+Anandkumar,+Anima+and+Shen,+Chunhua+and+Alvarez,+Jose" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={FreeSOLO}:+Learning+to+Segment+Objects+without+Annotations" target=“blank”>semantic scholar</a><a href=https://git.io/AdelaiDet target=“blank”>project webpage</a> </p> </li> <li><p><b>TopFormer: token pyramid transformer for mobile semantic segmentation</b> <br>\(\cdot\) <i>W. Zhang, Z. Huang, G. Yu, T. Chen, G. Luo, X. Wang, W. Liu, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;22), 2022</i>. <br>\(\cdot\) <a href=data/bibtex/Topformer2022.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={TopFormer}:+Token+Pyramid+Transformer+for+Mobile+Semantic+Segmentation+Zhang,+Wenqiang+and+Huang,+Zilong+and+Yu,+Gang+and+Chen,+Tao+and+Luo,+Guozhong+and+Wang,+Xinggang+and+Liu,+Wenyu+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={TopFormer}:+Token+Pyramid+Transformer+for+Mobile+Semantic+Segmentation" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2007.06919.pdf><img class="imgP  right" src="data/thumbnail/Chen2021CVPR3</u>arXiv.jpg"></a><b>AQD: towards accurate quantized object detection</b> <br>\(\cdot\) <i>P. Chen, J. Liu, B. Zhuang, M. Tan, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2007.06919 target=“blank”>arXiv</a><a href=data/bibtex/Chen2021CVPR3.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={AQD}:+Towards+Accurate+Quantized+Object+Detection+Chen,+Peng+and+Liu,+Jing+and+Zhuang,+Bohan+and+Tan,+Mingkui+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={AQD}:+Towards+Accurate+Quantized+Object+Detection" target=“blank”>semantic scholar</a> </p> <ol reversed> <li><p>Oral presentation. </p> </li></ol> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2011.14288.pdf><img class="imgP  right" src="data/thumbnail/Dai2021CVPR8</u>arXiv.jpg"></a><b>Learning affinity-aware upsampling for deep image matting</b> <br>\(\cdot\) <i>Y. Dai, H. Lu, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2011.14288 target=“blank”>arXiv</a><a href=data/bibtex/Dai2021CVPR8.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Learning+Affinity-Aware+Upsampling+for+Deep+Image+Matting+Dai,+Yutong+and+Lu,+Hao+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Affinity-Aware+Upsampling+for+Deep+Image+Matting" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2011.11204.pdf><img class="imgP  right" src="data/thumbnail/Guo2021CVPR14</u>arXiv.jpg"></a><b>Graph attention tracking</b> <br>\(\cdot\) <i>D. Guo, Y. Shao, Y. Cui, Z. Wang, L. Zhang, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2011.11204 target=“blank”>arXiv</a><a href=data/bibtex/Guo2021CVPR14.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Graph+Attention+Tracking+Guo,+Dongyan+and+Shao,+Yanyan+and+Cui,+Ying+and+Wang,+Zhenhua+and+Zhang,+Liyan+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Graph+Attention+Tracking" target=“blank”>semantic scholar</a><a href=https://git.io/SiamGAT target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/He2021CVPR7_arXiv.jpg><b>DyCo3D: robust instance segmentation of 3d point clouds through dynamic convolution</b> <br>\(\cdot\) <i>T. He, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2011.13328 target=“blank”>arXiv</a><a href=data/bibtex/He2021CVPR7.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={DyCo3D}:+Robust+Instance+Segmentation+of+3D+Point+Clouds+through+Dynamic+Convolution+He,+Tong+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={DyCo3D}:+Robust+Instance+Segmentation+of+3D+Point+Clouds+through+Dynamic+Convolution" target=“blank”>semantic scholar</a><a href=https://git.io/DyCo3D target=“blank”>project webpage</a> </p> </li> <li><p><b>HCRF-Flow: scene flow from point clouds with continuous high-order CRFs and position-aware flow embedding</b> <br>\(\cdot\) <i>R. Li, G. Lin, T. He, F. Liu, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=data/bibtex/Li2021CVPR12.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={HCRF-Flow}:+Scene+Flow+from+Point+Clouds+with+Continuous+High-order+{CRFs}+and+Position-aware+Flow+Embedding+Li,+Ruibo+and+Lin,+Guosheng+and+He,+Tong+and+Liu,+Fayao+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={HCRF-Flow}:+Scene+Flow+from+Point+Clouds+with+Continuous+High-order+{CRFs}+and+Position-aware+Flow+Embedding" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Generic perceptual loss for modelling structured output dependencies</b> <br>\(\cdot\) <i>Y. Liu, W. Yin, Y. Chen, H. Chen, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=data/bibtex/Liu2021CVPR5.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Generic+Perceptual+Loss+for+Modelling+Structured+Output+Dependencies+Liu,+Yifan+and+Yin,+Wei+and+Chen,+Yu+and+Chen,+Hao+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Generic+Perceptual+Loss+for+Modelling+Structured+Output+Dependencies" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>FCPose: fully convolutional multi-person pose estimation with dynamic instance-aware convolutions</b> <br>\(\cdot\) <i>W. Mao, Z. Tian, X. Wang, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=data/bibtex/Mao2021CVPR4.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={FCPose}:+Fully+Convolutional+Multi-Person+Pose+Estimation+with+Dynamic+Instance-Aware+Convolutions+Mao,+Weian+and+Tian,+Zhi+and+Wang,+Xinlong+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={FCPose}:+Fully+Convolutional+Multi-Person+Pose+Estimation+with+Dynamic+Instance-Aware+Convolutions" target=“blank”>semantic scholar</a><a href=https://git.io/AdelaiDet target=“blank”>project webpage</a> </p> </li> <li><p><b>Feature decomposition and reconstruction learning for effective facial expression recognition</b> <br>\(\cdot\) <i>D. Ruan, Y. Yan, S. Lai, Z. Chai, C. Shen, H. Wang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=data/bibtex/Ruan2021CVPR9.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Feature+Decomposition+and+Reconstruction+Learning+for+Effective+Facial+Expression+Recognition+Ruan,+Delian+and+Yan,+Yan+and+Lai,+Shenqi+and+Chai,+Zhenhua+and+Shen,+Chunhua+and+Wang,+Hanzi" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Feature+Decomposition+and+Reconstruction+Learning+for+Effective+Facial+Expression+Recognition" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Learning spatial-semantic relationship for facial attribute recognition with limited labeled data</b> <br>\(\cdot\) <i>Y. Shu, Y. Yan, S. Chen, J. Xue, C. Shen, H. Wang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=data/bibtex/Shu2021CVPR10.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Learning+Spatial-Semantic+Relationship+for+Facial+Attribute+Recognition+with+Limited+Labeled+Data+Shu,+Ying+and+Yan,+Yan+and+Chen,+Si+and+Xue,+Jing-Hao+and+Shen,+Chunhua+and+Wang,+Hanzi" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Spatial-Semantic+Relationship+for+Facial+Attribute+Recognition+with+Limited+Labeled+Data" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Tian2021CVPR2_arXiv.jpg><b>BoxInst: high-performance instance segmentation with box annotations</b> <br>\(\cdot\) <i>Z. Tian, C. Shen, X. Wang, H. Chen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2012.02310 target=“blank”>arXiv</a><a href=data/bibtex/Tian2021CVPR2.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={BoxInst}:+High-Performance+Instance+Segmentation+with+Box+Annotations+Tian,+Zhi+and+Shen,+Chunhua+and+Wang,+Xinlong+and+Chen,+Hao" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={BoxInst}:+High-Performance+Instance+Segmentation+with+Box+Annotations" target=“blank”>semantic scholar</a><a href=https://git.io/AdelaiDet target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Wang2021CVPR13_arXiv.jpg><b>Dense contrastive learning for self-supervised visual pre-training</b> <br>\(\cdot\) <i>X. Wang, R. Zhang, C. Shen, T. Kong, L. Li</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2011.09157 target=“blank”>arXiv</a><a href=data/bibtex/Wang2021CVPR13.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Dense+Contrastive+Learning+for+Self-Supervised+Visual+Pre-Training+Wang,+Xinlong+and+Zhang,+Rufeng+and+Shen,+Chunhua+and+Kong,+Tao+and+Li,+Lei" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Dense+Contrastive+Learning+for+Self-Supervised+Visual+Pre-Training" target=“blank”>semantic scholar</a><a href=https://git.io/AdelaiDet target=“blank”>project webpage</a> </p> <ol reversed> <li><p>Oral presentation. </p> </li></ol> </li> <li><p><img class="imgP  right" src=data/thumbnail/Wang2021CVPR11_arXiv.jpg><b>End-to-end video instance segmentation with Transformers</b> <br>\(\cdot\) <i>Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, H. Xia</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2011.14503 target=“blank”>arXiv</a><a href=data/bibtex/Wang2021CVPR11.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=End-to-End+Video+Instance+Segmentation+with+{T}ransformers+Wang,+Yuqing+and+Xu,+Zhaoliang+and+Wang,+Xinlong+and+Shen,+Chunhua+and+Cheng,+Baoshan+and+Shen,+Hao+and+Xia,+Huaxia" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=End-to-End+Video+Instance+Segmentation+with+{T}ransformers" target=“blank”>semantic scholar</a> </p> <ol reversed> <li><p>Oral presentation. </p> </li></ol> </li> <li><p><img class="imgP  right" src=data/thumbnail/Yin2021CVPR6_arXiv.jpg><b>Learning to recover 3D scene shape from a single image</b> <br>\(\cdot\) <i>W. Yin, J. Zhang, O. Wang, S. Niklaus, L. Mai, S. Chen, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2012.09365 target=“blank”>arXiv</a><a href=data/bibtex/Yin2021CVPR6.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Learning+to+Recover+{3D}+Scene+Shape+from+a+Single+Image+Yin,+Wei+and+Zhang,+Jianming+and+Wang,+Oliver+and+Niklaus,+Simon+and+Mai,+Long+and+Chen,+Simon+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+to+Recover+{3D}+Scene+Shape+from+a+Single+Image" target=“blank”>semantic scholar</a><a href=https://git.io/Depth target=“blank”>project webpage</a> </p> <ol reversed> <li><p>Listed as one of the Best Paper Candidates, 32 out of about 6000 submissions. </p> </li></ol> </li> <li><p><img class="imgP  right" src=data/thumbnail/Zhang2021CVPR1_arXiv.jpg><b>DoDNet: learning to segment multi-organ and tumors from multiple partially labeled datasets</b> <br>\(\cdot\) <i>J. Zhang, Y. Xie, Y. Xia, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2011.10217 target=“blank”>arXiv</a><a href=data/bibtex/Zhang2021CVPR1.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={DoDNet}:+Learning+to+segment+multi-organ+and+tumors+from+multiple+partially+labeled+datasets+Zhang,+Jianpeng+and+Xie,+Yutong+and+Xia,+Yong+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={DoDNet}:+Learning+to+segment+multi-organ+and+tumors+from+multiple+partially+labeled+datasets" target=“blank”>semantic scholar</a><a href=https://github.com/aim-uofa/partially-labelled target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2001.00309.pdf><img class="imgP  right" src="data/thumbnail/CVPR2020BlendMask7</u>arXiv.jpg"></a><b>BlendMask: top-down meets bottom-up for instance segmentation</b> <br>\(\cdot\) <i>H. Chen, K. Sun, Z. Tian, C. Shen, Y. Huang, Y. Yan</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2001.00309 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2020BlendMask7.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={BlendMask}:+Top-Down+Meets+Bottom-Up+for+Instance+Segmentation+Chen,+Hao+and+Sun,+Kunyang+and+Tian,+Zhi+and+Shen,+Chunhua+and+Huang,+Yongming+and+Yan,+Youliang" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={BlendMask}:+Top-Down+Meets+Bottom-Up+for+Instance+Segmentation" target=“blank”>semantic scholar</a><a href=https://github.com/aim-uofa/AdelaiDet/ target=“blank”>project webpage</a> </p> <ol reversed> <li><p>Oral presentation. </p> </li></ol> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2002.10200.pdf><img class="imgP  right" src="data/thumbnail/CVPR2020Liu6</u>arXiv.jpg"></a><b>ABCNet: arbitrarily-shaped scene text spotting with adaptive Bezier-curve network in real time</b> <br>\(\cdot\) <i>Y. Liu, H. Chen, C. Shen, T. He, L. Jin, L. Wang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2002.10200 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2020Liu6.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={ABCNet}:+Arbitrarily-Shaped+Scene+Text+Spotting+with+Adaptive+{B}ezier-Curve+Network+in+Real+Time+Liu,+Yuliang+and+Chen,+Hao+and+Shen,+Chunhua+and+He,+Tong+and+Jin,+Lianwen+and+Wang,+Liangwei" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={ABCNet}:+Arbitrarily-Shaped+Scene+Text+Spotting+with+Adaptive+{B}ezier-Curve+Network+in+Real+Time" target=“blank”>semantic scholar</a><a href=https://github.com/aim-uofa/AdelaiDet/ target=“blank”>project webpage</a> </p> <ol reversed> <li><p>Oral presentation. </p> </li></ol> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2003.06780.pdf><img class="imgP  right" src="data/thumbnail/CVPR2020Self12</u>arXiv.jpg"></a><b>Self-trained deep ordinal regression for end-to-end video anomaly detection</b> <br>\(\cdot\) <i>G. Pang, C. Yan, C. Shen, A. van den Hengel, X. Bai</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2003.06780 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2020Self12.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Self-trained+Deep+Ordinal+Regression+for+End-to-End+Video+Anomaly+Detection+Pang,+Guansong+and+Yan,+Cheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Bai,+Xiao" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Self-trained+Deep+Ordinal+Regression+for+End-to-End+Video+Anomaly+Detection" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1904.10151.pdf><img class="imgP  right" src="data/thumbnail/CVPR2020REVERIE1</u>arXiv.jpg"></a><b>REVERIE: remote embodied visual referring expression in real indoor environments</b> <br>\(\cdot\) <i>Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Wang, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1904.10151 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2020REVERIE1.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={REVERIE}:+Remote+Embodied+Visual+Referring+Expression+in+Real+Indoor+Environments+Qi,+Yuankai+and+Wu,+Qi+and+Anderson,+Peter+and+Wang,+Xin+and+Wang,+William+Yang+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={REVERIE}:+Remote+Embodied+Visual+Referring+Expression+in+Real+Indoor+Environments" target=“blank”>semantic scholar</a> </p> <ol reversed> <li><p>Oral presentation. </p> </li></ol> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1906.04423.pdf><img class="imgP  right" src="data/thumbnail/CVPR2020NASFCOS10</u>arXiv.jpg"></a><b>NAS-FCOS: fast neural architecture search for object detection</b> <br>\(\cdot\) <i>N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, Y. Zhang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1906.04423 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2020NASFCOS10.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={NAS-FCOS}:+Fast+Neural+Architecture+Search+for+Object+Detection+Wang,+Ning+and+Gao,+Yang+and+Chen,+Hao+and+Wang,+Peng+and+Tian,+Zhi+and+Shen,+Chunhua+and+Zhang,+Yanning" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={NAS-FCOS}:+Fast+Neural+Architecture+Search+for+Object+Detection" target=“blank”>semantic scholar</a><a href=https://github.com/Lausannen/NAS-FCOS target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2002.10215.pdf><img class="imgP  right" src="data/thumbnail/CVPR2020Wang4</u>arXiv.jpg"></a><b>On the general value of evidence, and bilingual scene-text visual question answering</b> <br>\(\cdot\) <i>X. Wang, Y. Liu, C. Shen, C. Ng, C. Luo, L. Jin, C. Chan, A. van den Hengel, L. Wang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2002.10215 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2020Wang4.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=On+the+General+Value+of+Evidence,+and+Bilingual+Scene-Text+Visual+Question+Answering+Wang,+Xinyu+and+Liu,+Yuliang+and+Shen,+Chunhua+and+Ng,+Chun+Chet+and+Luo,+Canjie+and+Jin,+Lianwen+and+Chan,+Chee+Seng+and+{van+den+Hengel},+Anton+and+Wang,+Liangwei" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=On+the+General+Value+of+Evidence,+and+Bilingual+Scene-Text+Visual+Question+Answering" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1909.13226.pdf><img class="imgP  right" src="data/thumbnail/CVPR2020Xie8</u>arXiv.jpg"></a><b>PolarMask: single shot instance segmentation with polar representation</b> <br>\(\cdot\) <i>E. Xie, P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen, P. Luo</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1909.13226 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2020Xie8.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={PolarMask}:+Single+Shot+Instance+Segmentation+with+Polar+Representation+Xie,+Enze+and+Sun,+Peize+and+Song,+Xiaoge+and+Wang,+Wenhai+and+Liu,+Xuebo+and+Liang,+Ding+and+Shen,+Chunhua+and+Luo,+Ping" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={PolarMask}:+Single+Shot+Instance+Segmentation+with+Polar+Representation" target=“blank”>semantic scholar</a><a href=https://github.com/xieenze/PolarMask target=“blank”>project webpage</a> </p> <ol reversed> <li><p>Oral presentation. </p> </li></ol> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2004.01547.pdf><img class="imgP  right" src="data/thumbnail/CVPR2020Yu2</u>arXiv.jpg"></a><b>Context prior for scene segmentation</b> <br>\(\cdot\) <i>C. Yu, J. Wang, C. Gao, G. Yu, C. Shen, N. Sang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2004.01547 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2020Yu2.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Context+Prior+for+Scene+Segmentation+Yu,+Changqian+and+Wang,+Jingbo+and+Gao,+Changxin+and+Yu,+Gang+and+Shen,+Chunhua+and+Sang,+Nong" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Context+Prior+for+Scene+Segmentation" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2003.06777.pdf><img class="imgP  right" src="data/thumbnail/CVPR2020EMD9</u>arXiv.jpg"></a><b>DeepEMD: few-shot image classification with differentiable earth mover's distance and structured classifiers</b> <br>\(\cdot\) <i>C. Zhang, Y. Cai, G. Lin, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2003.06777 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2020EMD9.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={DeepEMD}:+Few-Shot+Image+Classification+with+Differentiable+Earth+Mover's+Distance+and+Structured+Classifiers+Zhang,+Chi+and+Cai,+Yujun+and+Lin,+Guosheng+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={DeepEMD}:+Few-Shot+Image+Classification+with+Differentiable+Earth+Mover's+Distance+and+Structured+Classifiers" target=“blank”>semantic scholar</a><a href=https://github.com/icoz69/DeepEMD target=“blank”>project webpage</a> </p> <ol reversed> <li><p>Oral presentation. </p> </li></ol> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1909.08228.pdf><img class="imgP  right" src="data/thumbnail/CVPR2020NAS11</u>arXiv.jpg"></a><b>Memory-efficient hierarchical neural architecture search for image denoising</b> <br>\(\cdot\) <i>H. Zhang, Y. Li, H. Chen, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1909.08228 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2020NAS11.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Memory-Efficient+Hierarchical+Neural+Architecture+Search+for+Image+Denoising+Zhang,+Haokui+and+Li,+Ying+and+Chen,+Hao+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Memory-Efficient+Hierarchical+Neural+Architecture+Search+for+Image+Denoising" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2003.11712.pdf><img class="imgP  right" src="data/thumbnail/CVPR2020Zhang3</u>arXiv.jpg"></a><b>Mask encoding for single shot instance segmentation</b> <br>\(\cdot\) <i>R. Zhang, Z. Tian, C. Shen, M. You, Y. Yan</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2003.11712 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2020Zhang3.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Mask+Encoding+for+Single+Shot+Instance+Segmentation+Zhang,+Rufeng+and+Tian,+Zhi+and+Shen,+Chunhua+and+You,+Mingyu+and+Yan,+Youliang" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Mask+Encoding+for+Single+Shot+Instance+Segmentation" target=“blank”>semantic scholar</a><a href=https://github.com/aim-uofa/AdelaiDet/ target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1903.11236.pdf><img class="imgP  right" src="data/thumbnail/CVPR2020Zhuang5</u>arXiv.jpg"></a><b>Training quantized neural networks with a full-precision auxiliary module</b> <br>\(\cdot\) <i>B. Zhuang, L. Liu, M. Tan, C. Shen, I. Reid</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1903.11236 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2020Zhuang5.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Training+Quantized+Neural+Networks+with+a+Full-precision+Auxiliary+Module+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Tan,+Mingkui+and+Shen,+Chunhua+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Training+Quantized+Neural+Networks+with+a+Full-precision+Auxiliary+Module" target=“blank”>semantic scholar</a> </p> <ol reversed> <li><p>Oral presentation. </p> </li></ol> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1903.04688.pdf><img class="imgP  right" src="data/thumbnail/CVPR19He8</u>arXiv.jpg"></a><b>Knowledge adaptation for efficient semantic segmentation</b> <br>\(\cdot\) <i>T. He, C. Shen, Z. Tian, D. Gong, C. Sun, Y. Yan</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1903.04688 target=“blank”>arXiv</a><a href=data/bibtex/CVPR19He8.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Knowledge+Adaptation+for+Efficient+Semantic+Segmentation+He,+Tong+and+Shen,+Chunhua+and+Tian,+Zhi+and+Gong,+Dong+and+Sun,+Changming+and+Yan,+Youliang" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Knowledge+Adaptation+for+Efficient+Semantic+Segmentation" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1811.11903.pdf><img class="imgP  right" src="data/thumbnail/CVPR19HuiLi2</u>arXiv.jpg"></a><b>Visual question answering as reading comprehension</b> <br>\(\cdot\) <i>H. Li, P. Wang, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1811.11903 target=“blank”>arXiv</a><a href=data/bibtex/CVPR19HuiLi2.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Visual+Question+Answering+as+Reading+Comprehension+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Visual+Question+Answering+as+Reading+Comprehension" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1810.10804.pdf><img class="imgP  right" src="data/thumbnail/CVPR19Nekrasov1</u>arXiv.jpg"></a><b>Fast neural architecture search of compact semantic segmentation models via auxiliary cells</b> <br>\(\cdot\) <i>V. Nekrasov, H. Chen, C. Shen, I. Reid</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1810.10804 target=“blank”>arXiv</a><a href=data/bibtex/CVPR19Nekrasov1.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Fast+Neural+Architecture+Search+of+Compact+Semantic+Segmentation+Models+via+Auxiliary+Cells+Nekrasov,+Vladimir+and+Chen,+Hao+and+Shen,+Chunhua+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+Neural+Architecture+Search+of+Compact+Semantic+Segmentation+Models+via+Auxiliary+Cells" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1903.02120.pdf><img class="imgP  right" src="data/thumbnail/CVPR19Tian7</u>arXiv.jpg"></a><b>Decoders matter for semantic segmentation: data-dependent decoding enables flexible feature aggregation</b> <br>\(\cdot\) <i>Z. Tian, T. He, C. Shen, Y. Yan</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1903.02120 target=“blank”>arXiv</a><a href=data/bibtex/CVPR19Tian7.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Decoders+Matter+for+Semantic+Segmentation:+Data-Dependent+Decoding+Enables+Flexible+Feature+Aggregation+Tian,+Zhi+and+He,+Tong+and+Shen,+Chunhua+and+Yan,+Youliang" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Decoders+Matter+for+Semantic+Segmentation:+Data-Dependent+Decoding+Enables+Flexible+Feature+Aggregation" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1812.04794.pdf><img class="imgP  right" src="data/thumbnail/CVPR19PengWang0</u>arXiv.jpg"></a><b>Neighbourhood watch: referring expression comprehension via language-guided graph attention networks</b> <br>\(\cdot\) <i>P. Wang, Q. Wu, J. Cao, C. Shen, L. Gao, A. vanden Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1812.04794 target=“blank”>arXiv</a><a href=data/bibtex/CVPR19PengWang0.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Neighbourhood+Watch:+Referring+Expression+Comprehension+via+Language-guided+Graph+Attention+Networks+Wang,+Peng+and+Wu,+Qi+and+Cao,+Jiewei+and+Shen,+Chunhua+and+Gao,+Lianli+and+{vanden+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Neighbourhood+Watch:+Referring+Expression+Comprehension+via+Language-guided+Graph+Attention+Networks" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1902.09852.pdf><img class="imgP  right" src="data/thumbnail/CVPR19Wang4</u>arXiv.jpg"></a><b>Associatively segmenting instances and semantics in point clouds</b> <br>\(\cdot\) <i>X. Wang, S. Liu, X. Shen, C. Shen, J. Jia</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1902.09852 target=“blank”>arXiv</a><a href=data/bibtex/CVPR19Wang4.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Associatively+Segmenting+Instances+and+Semantics+in+Point+Clouds+Wang,+Xinlong+and+Liu,+Shu+and+Shen,+Xiaoyong+and+Shen,+Chunhua+and+Jia,+Jiaya" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Associatively+Segmenting+Instances+and+Semantics+in+Point+Clouds" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1904.10293.pdf><img class="imgP  right" src="data/thumbnail/CVPR19Yan9</u>arXiv.jpg"></a><b>Attention-guided network for ghost-free high dynamic range imaging</b> <br>\(\cdot\) <i>Q. Yan, D. Gong, Q. Shi, A. van den Hengel, C. Shen, I. Reid, Y. Zhang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1904.10293 target=“blank”>arXiv</a><a href=data/bibtex/CVPR19Yan9.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Attention-guided+Network+for+Ghost-free+High+Dynamic+Range+Imaging+Yan,+Qingsen+and+Gong,+Dong+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Reid,+Ian+and+Zhang,+Yanning" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Attention-guided+Network+for+Ghost-free+High+Dynamic+Range+Imaging" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1903.02351.pdf><img class="imgP  right" src="data/thumbnail/CVPR19Zhang5</u>arXiv.jpg"></a><b>CANet: class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</b> <br>\(\cdot\) <i>C. Zhang, G. Lin, F. Liu, R. Yao, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1903.02351 target=“blank”>arXiv</a><a href=data/bibtex/CVPR19Zhang5.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={CANet}:+Class-Agnostic+Segmentation+Networks+with+Iterative+Refinement+and+Attentive+Few-Shot+Learning+Zhang,+Chi+and+Lin,+Guosheng+and+Liu,+Fayao+and+Yao,+Rui+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={CANet}:+Class-Agnostic+Segmentation+Networks+with+Iterative+Refinement+and+Attentive+Few-Shot+Learning" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/CVPR19Zhang6_PDF.jpg><b>Mind your neighbours: image annotation with metadata neighbourhood graph co-attention networks</b> <br>\(\cdot\) <i>J. Zhang, Q. Wu, J. Zhang, C. Shen, J. Lu</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Mind_Your_Neighbours_Image_Annotation_With_Metadata_Neighbourhood_Graph_Co-Attention_CVPR_2019_paper.pdf target=“blank”>link</a><a href=data/bibtex/CVPR19Zhang6.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Mind+Your+Neighbours:+Image+Annotation+with+Metadata+Neighbourhood+Graph+Co-Attention+Networks+Zhang,+Junjie+and+Wu,+Qi+and+Zhang,+Jian+and+Shen,+Chunhua+and+Lu,+Jianfeng" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Mind+Your+Neighbours:+Image+Annotation+with+Metadata+Neighbourhood+Graph+Co-Attention+Networks" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1811.10413.pdf><img class="imgP  right" src="data/thumbnail/CVPR19Zhuang3</u>arXiv.jpg"></a><b>Structured binary neural networks for accurate image classification and semantic segmentation</b> <br>\(\cdot\) <i>B. Zhuang, C. Shen, M. Tan, L. Liu, I. Reid</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1811.10413 target=“blank”>arXiv</a><a href=data/bibtex/CVPR19Zhuang3.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Structured+Binary+Neural+Networks+for+Accurate+Image+Classification+and+Semantic+Segmentation+Zhuang,+Bohan+and+Shen,+Chunhua+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Structured+Binary+Neural+Networks+for+Accurate+Image+Classification+and+Semantic+Segmentation" target=“blank”>semantic scholar</a><a href=https://bitbucket.org/jingruixiaozhuang/group-net-image-classification/ target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1711.10703.pdf><img class="imgP  right" src="data/thumbnail/Chen2018CVPR</u>arXiv.jpg"></a><b>FSRNet: end-to-end learning face super-resolution with facial priors</b> <br>\(\cdot\) <i>Y. Chen, Y. Tai, X. Liu, C. Shen, J. Yang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1711.10703 target=“blank”>arXiv</a><a href=data/bibtex/Chen2018CVPR.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={FSRNet}:+End-to-End+Learning+Face+Super-Resolution+with+Facial+Priors+Chen,+Yu+and+Tai,+Ying+and+Liu,+Xiaoming+and+Shen,+Chunhua+and+Yang,+Jian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={FSRNet}:+End-to-End+Learning+Face+Super-Resolution+with+Facial+Priors" target=“blank”>semantic scholar</a><a href=https://github.com/tyshiwo/FSRNet target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/He2018CVPR_arXiv.jpg><b>An end-to-end textspotter with explicit alignment and attention</b> <br>\(\cdot\) <i>T. He, Z. Tian, W. Huang, C. Shen, Y. Qiao, C. Sun</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1803.03474 target=“blank”>arXiv</a><a href=data/bibtex/He2018CVPR.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=An+end-to-end+TextSpotter+with+Explicit+Alignment+and+Attention+He,+Tong+and+Tian,+Zhi+and+Huang,+Weilin+and+Shen,+Chunhua+and+Qiao,+Yu+and+Sun,+Changming" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=An+end-to-end+TextSpotter+with+Explicit+Alignment+and+Attention" target=“blank”>semantic scholar</a><a href=https://github.com/tonghe90/textspotter target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src="data/thumbnail/Ma2018CVPR<u>a</u>arXiv.jpg"><b>Visual question answering with memory-augmented networks</b> <br>\(\cdot\) <i>C. Ma, C. Shen, A. Dick, Q. Wu, P. Wang, A. van den Hengel, I. Reid</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1707.04968 target=“blank”>arXiv</a><a href=data/bibtex/Ma2018CVPR_a.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Visual+Question+Answering+with+Memory-Augmented+Networks+Ma,+Chao+and+Shen,+Chunhua+and+Dick,+Anthony+and+Wu,+Qi+and+Wang,+Peng+and+{van+den+Hengel},+Anton+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Visual+Question+Answering+with+Memory-Augmented+Networks" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Bootstrapping the performance of webly supervised semantic segmentation</b> <br>\(\cdot\) <i>T. Shen, G. Lin, C. Shen, I. Reid</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=data/bibtex/TongShen2018CVPR.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Bootstrapping+the+Performance+of+Webly+Supervised+Semantic+Segmentation+Shen,+Tong+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Bootstrapping+the+Performance+of+Webly+Supervised+Semantic+Segmentation" target=“blank”>semantic scholar</a><a href=https://github.com/ascust/BDWSS target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Song2018CVPR_arXiv.jpg><b>VITAL: visual tracking via adversarial learning</b> <br>\(\cdot\) <i>Y. Song, C. Ma, X. Wu, L. Gong, L. Bao, W. Zuo, C. Shen, R. Lau, M. Yang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1804.04273 target=“blank”>arXiv</a><a href=data/bibtex/Song2018CVPR.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={VITAL}:+VIsual+Tracking+via+Adversarial+Learning+Song,+Yibing+and+Ma,+Chao+and+Wu,+Xiaohe+and+Gong,+Lijun+and+Bao,+Linchao+and+Zuo,+Wangmeng+and+Shen,+Chunhua+and+Lau,+Rynson+and+Yang,+Ming-Hsuan" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={VITAL}:+VIsual+Tracking+via+Adversarial+Learning" target=“blank”>semantic scholar</a><a href=https://ybsong00.github.io/cvpr18_tracking/index target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Wang2018CVPR_arXiv.jpg><b>Repulsion loss: detecting pedestrians in a crowd</b> <br>\(\cdot\) <i>X. Wang, T. Xiao, Y. Jiang, S. Shao, J. Sun, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1711.07752 target=“blank”>arXiv</a><a href=data/bibtex/Wang2018CVPR.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Repulsion+Loss:+Detecting+Pedestrians+in+a+Crowd+Wang,+Xinlong+and+Xiao,+Tete+and+Jiang,+Yuning+and+Shao,+Shuai+and+Sun,+Jian+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Repulsion+Loss:+Detecting+Pedestrians+in+a+Crowd" target=“blank”>semantic scholar</a> </p> <ol reversed> <li><p>Others have implemented our paper: <a href=https://github.com/bailvwangzi/repulsion_loss_ssd target=“blank”>Repulsion loss in SSD</a> and <a href=https://github.com/rainofmine/Repulsion_Loss target=“blank”>Repulsion loss in RetinaNet</a>. </p> </li></ol> </li> <li><p><img class="imgP  right" src=data/thumbnail/QWu2018CVPR_arXiv.jpg><b>Are you talking to me? reasoned visual dialog generation through adversarial learning</b> <br>\(\cdot\) <i>Q. Wu, P. Wang, C. Shen, I. Reid, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1711.07613 target=“blank”>arXiv</a><a href=data/bibtex/QWu2018CVPR.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Are+You+Talking+to+Me?+Reasoned+Visual+Dialog+Generation+through+Adversarial+Learning+Wu,+Qi+and+Wang,+Peng+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Are+You+Talking+to+Me?+Reasoned+Visual+Dialog+Generation+through+Adversarial+Learning" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Monocular relative depth perception with web stereo data supervision</b> <br>\(\cdot\) <i>K. Xian, C. Shen, Z. Cao, H. Lu, Y. Xiao, R. Li, Z. Luo</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=data/bibtex/Xian2018CVPR.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Monocular+Relative+Depth+Perception+with+Web+Stereo+Data+Supervision+Xian,+Ke+and+Shen,+Chunhua+and+Cao,+Zhiguo+and+Lu,+Hao+and+Xiao,+Yang+and+Li,+Ruibo+and+Luo,+Zhenbo" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Monocular+Relative+Depth+Perception+with+Web+Stereo+Data+Supervision" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src="data/thumbnail/Zhuang2018CVPR<u>b</u>arXiv.jpg"><b>Towards effective low-bitwidth convolutional neural networks</b> <br>\(\cdot\) <i>B. Zhuang, C. Shen, M. Tan, L. Liu, I. Reid</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1711.00205 target=“blank”>arXiv</a><a href=data/bibtex/Zhuang2018CVPR_b.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Towards+Effective+Low-bitwidth+Convolutional+Neural+Networks+Zhuang,+Bohan+and+Shen,+Chunhua+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+Effective+Low-bitwidth+Convolutional+Neural+Networks" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src="data/thumbnail/Zhuang2018CVPR<u>a</u>arXiv.jpg"><b>Parallel attention: a unified framework for visual object discovery through dialogs and queries</b> <br>\(\cdot\) <i>B. Zhuang, Q. Wu, C. Shen, I. Reid, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1711.06370 target=“blank”>arXiv</a><a href=data/bibtex/Zhuang2018CVPR_a.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Parallel+Attention:+A+Unified+Framework+for+Visual+Object+Discovery+through+Dialogs+and+Queries+Zhuang,+Bohan+and+Wu,+Qi+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Parallel+Attention:+A+Unified+Framework+for+Visual+Object+Discovery+through+Dialogs+and+Queries" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1612.02583.pdf><img class="imgP  right" src="data/thumbnail/CVPR2017Gong</u>arXiv.jpg"></a><b>From motion blur to motion flow: a deep learning solution for removing heterogeneous motion blur</b> <br>\(\cdot\) <i>D. Gong, J. Yang, L. Liu, Y. Zhang, I. Reid, C. Shen, A. van den Hengel, Q. Shi</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1612.02583 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2017Gong.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=From+Motion+Blur+to+Motion+Flow:+a+Deep+Learning+Solution+for+Removing+Heterogeneous+Motion+Blur+Gong,+Dong+and+Yang,+Jie+and+Liu,+Lingqiao+and+Zhang,+Yanning+and+Reid,+Ian+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=From+Motion+Blur+to+Motion+Flow:+a+Deep+Learning+Solution+for+Removing+Heterogeneous+Motion+Blur" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1611.09967.pdf><img class="imgP  right" src="data/thumbnail/CVPR2017YaoLi</u>arXiv.jpg"></a><b>Sequential person recognition in photo albums with a recurrent network</b> <br>\(\cdot\) <i>Y. Li, G. Lin, B. Zhuang, L. Liu, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1611.09967 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2017YaoLi.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Sequential+Person+Recognition+in+Photo+Albums+with+a+Recurrent+Network+Li,+Yao+and+Lin,+Guosheng+and+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Sequential+Person+Recognition+in+Photo+Albums+with+a+Recurrent+Network" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1611.06612.pdf><img class="imgP  right" src="data/thumbnail/CVPR2017Lin</u>arXiv.jpg"></a><b>RefineNet: multi-path refinement networks for high-resolution semantic segmentation</b> <br>\(\cdot\) <i>G. Lin, A. Milan, C. Shen, I. Reid</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1611.06612 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2017Lin.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={RefineNet}:+Multi-Path+Refinement+Networks+for+High-Resolution+Semantic+Segmentation+Lin,+Guosheng+and+Milan,+Anton+and+Shen,+Chunhua+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={RefineNet}:+Multi-Path+Refinement+Networks+for+High-Resolution+Semantic+Segmentation" target=“blank”>semantic scholar</a><a href=https://github.com/guosheng/refinenet target=“blank”>project webpage</a> </p> <ol reversed> <li><p><a href=https://github.com/DrSleep/light-weight-refinenet target=“blank”>Light-weight RefineNet with Pytorch code</a>. </p> </li></ol> </li> <li><p><img class="imgP  right" src=data/thumbnail/CVPR2017WangAttend_PDF.jpg><b>Multi-attention network for one shot learning</b> <br>\(\cdot\) <i>P. Wang, L. Liu, C. Shen, Z. Huang, A. van den Hengel, H. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>. <br>\(\cdot\) <a href=http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Multi-Attention_Network_for_CVPR_2017_paper.pdf target=“blank”>pdf</a><a href=data/bibtex/CVPR2017WangAttend.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Multi-attention+Network+for+One+Shot+Learning+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Huang,+Zi+and+{van+den+Hengel},+Anton+and+Shen,+Heng+Tao" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Multi-attention+Network+for+One+Shot+Learning" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1612.05386.pdf><img class="imgP  right" src="data/thumbnail/CVPR2017WangVQA</u>arXiv.jpg"></a><b>The VQA-machine: learning how to use existing vision algorithms to answer new questions</b> <br>\(\cdot\) <i>P. Wang, Q. Wu, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1612.05386 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2017WangVQA.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=The+{VQA}-Machine:+Learning+How+to+Use+Existing+Vision+Algorithms+to+Answer+New+Questions+Wang,+Peng+and+Wu,+Qi+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=The+{VQA}-Machine:+Learning+How+to+Use+Existing+Vision+Algorithms+to+Answer+New+Questions" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1611.09960.pdf><img class="imgP  right" src="data/thumbnail/CVPR2017Zhuang</u>arXiv.jpg"></a><b>Attend in groups: a weakly-supervised deep learning framework for learning from web data</b> <br>\(\cdot\) <i>B. Zhuang, L. Liu, Y. Li, C. Shen, I. Reid</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1611.09960 target=“blank”>arXiv</a><a href=data/bibtex/CVPR2017Zhuang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Attend+in+groups:+a+weakly-supervised+deep+learning+framework+for+learning+from+web+data+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Li,+Yao+and+Shen,+Chunhua+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Attend+in+groups:+a+weakly-supervised+deep+learning+framework+for+learning+from+web+data" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1504.01013.pdf><img class="imgP  right" src="data/thumbnail/CVPR16labelling</u>arXiv.jpg"></a><b>Efficient piecewise training of deep structured models for semantic segmentation</b> <br>\(\cdot\) <i>G. Lin, C. Shen, A. van dan Hengel, I. Reid</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1504.01013 target=“blank”>arXiv</a><a href=data/bibtex/CVPR16labelling.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Efficient+piecewise+training+of+deep+structured+models+for+semantic+segmentation+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+dan+Hengel},+Anton+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+piecewise+training+of+deep+structured+models+for+semantic+segmentation" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1604.01146.pdf><img class="imgP  right" src="data/thumbnail/CVPR16Zeroshot</u>arXiv.jpg"></a><b>Less is more: zero-shot learning from online textual documents with noise suppression</b> <br>\(\cdot\) <i>R. Qiao, L. Liu, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1604.01146 target=“blank”>arXiv</a><a href=data/bibtex/CVPR16Zeroshot.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Less+is+More:+Zero-shot+Learning+from+Online+Textual+Documents+with+Noise+Suppression+Qiao,+Ruizhi+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Less+is+More:+Zero-shot+Learning+from+Online+Textual+Documents+with+Noise+Suppression" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1602.04422.pdf><img class="imgP  right" src="data/thumbnail/CVPR16Irregular</u>arXiv.jpg"></a><b>What's wrong with that object? identifying irregular object from images by modelling the detection score distribution</b> <br>\(\cdot\) <i>P. Wang, L. Liu, C. Shen, Z. Huang, A. van den Hengel, H. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1602.04422 target=“blank”>arXiv</a><a href=data/bibtex/CVPR16Irregular.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=What's+Wrong+with+that+Object?+Identifying+Irregular+Object+From+Images+by+Modelling+the+Detection+Score+Distribution+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Huang,+Zi+and+{van+den+Hengel},+Anton+and+Shen,+Heng+Tao" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=What's+Wrong+with+that+Object?+Identifying+Irregular+Object+From+Images+by+Modelling+the+Detection+Score+Distribution" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1506.01144.pdf><img class="imgP  right" src="data/thumbnail/CVPR16What</u>arXiv.jpg"></a><b>What value do explicit high level concepts have in vision to language problems</b> <br>\(\cdot\) <i>Q. Wu, C. Shen, L. Liu, A. Dick, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1506.01144 target=“blank”>arXiv</a><a href=data/bibtex/CVPR16What.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=What+Value+Do+Explicit+High+Level+Concepts+Have+in+Vision+to+Language+Problems+Wu,+Qi+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=What+Value+Do+Explicit+High+Level+Concepts+Have+in+Vision+to+Language+Problems" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1511.06973.pdf><img class="imgP  right" src="data/thumbnail/CVPR16AMA</u>arXiv.jpg"></a><b>Ask me anything: free-form visual question answering based on knowledge from external sources</b> <br>\(\cdot\) <i>Q. Wu, P. Wang, C. Shen, A. Dick, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1511.06973 target=“blank”>arXiv</a><a href=data/bibtex/CVPR16AMA.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Ask+Me+Anything:+Free-form+Visual+Question+Answering+Based+on+Knowledge+from+External+Sources+Wu,+Qi+and+Wang,+Peng+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Ask+Me+Anything:+Free-form+Visual+Question+Answering+Based+on+Knowledge+from+External+Sources" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1603.02844.pdf><img class="imgP  right" src="data/thumbnail/CVPR16Binary</u>arXiv.jpg"></a><b>Fast training of triplet-based deep binary embedding networks</b> <br>\(\cdot\) <i>B. Zhuang, G. Lin, C. Shen, I. Reid</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1603.02844 target=“blank”>arXiv</a><a href=data/bibtex/CVPR16Binary.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Fast+Training+of+Triplet-based+Deep+Binary+Embedding+Networks+Zhuang,+Bohan+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+Training+of+Triplet-based+Deep+Binary+Embedding+Networks" target=“blank”>semantic scholar</a><a href=https://bitbucket.org/jingruixiaozhuang/fast-training-of-triplet-based-deep-binary-embedding-networks target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/CVPR15h_PDF.jpg><b>Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs</b> <br>\(\cdot\) <i>B. Li, C. Shen, Y. Dai, A. van den Hengel, M. He</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>. <br>\(\cdot\) <a href=http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Li_Depth_and_Surface_2015_CVPR_paper.pdf target=“blank”>pdf</a><a href=data/bibtex/CVPR15h.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Depth+and+Surface+Normal+Estimation+from+Monocular+Images+Using+Regression+on+Deep+Features+and+Hierarchical+{CRFs}+Li,+Bo+and+Shen,+Chunhua+and+Dai,+Yuchao+and+{van+den+Hengel},+Anton+and+He,+Mingyi" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Depth+and+Surface+Normal+Estimation+from+Monocular+Images+Using+Regression+on+Deep+Features+and+Hierarchical+{CRFs}" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1411.6382.pdf><img class="imgP  right" src="data/thumbnail/CVPR15a</u>arXiv.jpg"></a><b>Mid-level deep pattern mining</b> <br>\(\cdot\) <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1411.6382 target=“blank”>arXiv</a><a href=data/bibtex/CVPR15a.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Mid-level+Deep+Pattern+Mining+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Mid-level+Deep+Pattern+Mining" target=“blank”>semantic scholar</a><a href=https://github.com/yaoliUoA/MDPM target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1411.6387.pdf><img class="imgP  right" src="data/thumbnail/CVPR15b</u>arXiv.jpg"></a><b>Deep convolutional neural fields for depth estimation from a single image</b> <br>\(\cdot\) <i>F. Liu, C. Shen, G. Lin</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1411.6387 target=“blank”>arXiv</a><a href=data/bibtex/CVPR15b.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Deep+Convolutional+Neural+Fields+for+Depth+Estimation+from+a+Single+Image+Liu,+Fayao+and+Shen,+Chunhua+and+Lin,+Guosheng" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deep+Convolutional+Neural+Fields+for+Depth+Estimation+from+a+Single+Image" target=“blank”>semantic scholar</a><a href=http://goo.gl/rAKWrS target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1411.7466.pdf><img class="imgP  right" src="data/thumbnail/CVPR15d</u>arXiv.jpg"></a><b>The treasure beneath convolutional layers: cross convolutional layer pooling for image classification</b> <br>\(\cdot\) <i>L. Liu, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1411.7466 target=“blank”>arXiv</a><a href=data/bibtex/CVPR15d.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=The+Treasure+beneath+Convolutional+Layers:+Cross+convolutional+layer+Pooling+for+Image+Classification+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=The+Treasure+beneath+Convolutional+Layers:+Cross+convolutional+layer+Pooling+for+Image+Classification" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1503.01543.pdf><img class="imgP  right" src="data/thumbnail/CVPR15f</u>arXiv.jpg"></a><b>Learning to rank in person re-identification with metric ensembles</b> <br>\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1503.01543 target=“blank”>arXiv</a><a href=data/bibtex/CVPR15f.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Learning+to+rank+in+person+re-identification+with+metric+ensembles+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+to+rank+in+person+re-identification+with+metric+ensembles" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/CVPR15c_PDF.jpg><b>Supervised discrete hashing</b> <br>\(\cdot\) <i>F. Shen, C. Shen, W. Liu, H. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>. <br>\(\cdot\) <a href=http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Shen_Supervised_Discrete_Hashing_2015_CVPR_paper.pdf target=“blank”>pdf</a><a href=data/bibtex/CVPR15c.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Supervised+Discrete+Hashing+Shen,+Fumin+and+Shen,+Chunhua+and+Liu,+Wei+and+Shen,+Heng+Tao" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Supervised+Discrete+Hashing" target=“blank”>semantic scholar</a><a href=https://github.com/bd622/DiscretHashing/ target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/CVPR15g_PDF.jpg><b>Learning graph structure for multi-label image classification via clique generation</b> <br>\(\cdot\) <i>M. Tan, Q. Shi, A. van den Hengel, C. Shen, J. Gao, F. Hu, Z. Zhang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>. <br>\(\cdot\) <a href=http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tan_Learning_Graph_Structure_2015_CVPR_paper.pdf target=“blank”>pdf</a><a href=data/bibtex/CVPR15g.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Learning+Graph+Structure+for+Multi-label+Image+Classification+via+Clique+Generation+Tan,+Mingkui+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Gao,+Junbin+and+Hu,+Fuyuan+and+Zhang,+Zhen" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Graph+Structure+for+Multi-label+Image+Classification+via+Clique+Generation" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1504.01492.pdf><img class="imgP  right" src="data/thumbnail/CVPR15e</u>arXiv.jpg"></a><b>Efficient SDP inference for fully-connected CRFs based on low-rank decomposition</b> <br>\(\cdot\) <i>P. Wang, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1504.01492 target=“blank”>arXiv</a><a href=data/bibtex/CVPR15e.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Efficient+{SDP}+Inference+for+Fully-connected+{CRFs}+Based+on+Low-rank+Decomposition+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+{SDP}+Inference+for+Fully-connected+{CRFs}+Based+on+Low-rank+Decomposition" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1404.1561.pdf><img class="imgP  right" src="data/thumbnail/CVPR14Lin</u>arXiv.jpg"></a><b>Fast supervised hashing with decision trees for high-dimensional data</b> <br>\(\cdot\) <i>G. Lin, C. Shen, Q. Shi, A. van den Hengel, D. Suter</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;14), 2014</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1404.1561 target=“blank”>arXiv</a><a href=https://bitbucket.org/chhshen/fasthash/src target=“blank”>link</a><a href=data/bibtex/CVPR14Lin.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Fast+Supervised+Hashing+with+Decision+Trees+for+High-Dimensional+Data+Lin,+Guosheng+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Suter,+David" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+Supervised+Hashing+with+Decision+Trees+for+High-Dimensional+Data" target=“blank”>semantic scholar</a><a href=https://bitbucket.org/chhshen/fasthash/ target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/CVPR13bLi_PDF.jpg><b>Learning compact binary codes for visual tracking</b> <br>\(\cdot\) <i>X. Li, C. Shen, A. Dick, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>. <br>\(\cdot\) <a href=http://hdl.handle.net/2440/77412 target=“blank”>link</a><a href=http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Learning_Compact_Binary_2013_CVPR_paper.pdf target=“blank”>pdf</a><a href=data/bibtex/CVPR13bLi.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Learning+Compact+Binary+Codes+for+Visual+Tracking+Li,+Xi+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Compact+Binary+Codes+for+Visual+Tracking" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1303.7043.pdf><img class="imgP  right" src="data/thumbnail/CVPR13aShen</u>arXiv.jpg"></a><b>Inductive hashing on manifolds</b> <br>\(\cdot\) <i>F. Shen, C. Shen, Q. Shi, A. van den Hengel, Z. Tang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1303.7043 target=“blank”>arXiv</a><a href=data/bibtex/CVPR13aShen.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Inductive+Hashing+on+Manifolds+Shen,+Fumin+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Inductive+Hashing+on+Manifolds" target=“blank”>semantic scholar</a><a href=https://github.com/chhshen/Hashing-on-Nonlinear-Manifolds target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1304.0840.pdf><img class="imgP  right" src="data/thumbnail/CVPR13dWang</u>arXiv.jpg"></a><b>A fast semidefinite approach to solving binary quadratic problems</b> <br>\(\cdot\) <i>P. Wang, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1304.0840 target=“blank”>arXiv</a><a href=data/bibtex/CVPR13dWang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=A+Fast+Semidefinite+Approach+to+Solving+Binary+Quadratic+Problems+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Fast+Semidefinite+Approach+to+Solving+Binary+Quadratic+Problems" target=“blank”>semantic scholar</a><a href=./projects/BQP/ target=“blank”>project webpage</a> </p> <ol reversed> <li><p>Oral presentation, 60 out of 1870 submissions. </p> </li></ol> </li> <li><p><img class="imgP  right" src=data/thumbnail/CVPR13cWang_PDF.jpg><b>Bilinear programming for human activity recognition with unknown MRF graphs</b> <br>\(\cdot\) <i>Z. Wang, Q. Shi, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>. <br>\(\cdot\) <a href=http://hdl.handle.net/2440/77411 target=“blank”>link</a><a href=http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_Bilinear_Programming_for_2013_CVPR_paper.pdf target=“blank”>pdf</a><a href=data/bibtex/CVPR13cWang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Bilinear+Programming+for+Human+Activity+Recognition+with+unknown+{MRF}+graphs+Wang,+Zhenhua+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Bilinear+Programming+for+Human+Activity+Recognition+with+unknown+{MRF}+graphs" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/CVPR13eYao_PDF.jpg><b>Part-based visual tracking with online latent structural learning</b> <br>\(\cdot\) <i>R. Yao, Q. Shi, C. Shen, Y. Zhang, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>. <br>\(\cdot\) <a href=http://hdl.handle.net/2440/77413 target=“blank”>link</a><a href=http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yao_Part-Based_Visual_Tracking_2013_CVPR_paper.pdf target=“blank”>pdf</a><a href=data/bibtex/CVPR13eYao.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Part-based+Visual+Tracking+with+Online+Latent+Structural+Learning+Yao,+Rui+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Part-based+Visual+Tracking+with+Online+Latent+Structural+Learning" target=“blank”>semantic scholar</a><a href=https://github.com/chhshen/PartTracking target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1204.2912.pdf><img class="imgP  right" src="data/thumbnail/CVPR12a</u>arXiv.jpg"></a><b>Non-sparse linear representations for visual tracking with online reservoir metric learning</b> <br>\(\cdot\) <i>X. Li, C. Shen, Q. Shi, A. Dick, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;12), 2012</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1204.2912 target=“blank”>arXiv</a><a href=http://hdl.handle.net/2440/70244 target=“blank”>pdf</a><a href=data/bibtex/CVPR12a.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Non-sparse+Linear+Representations+for+Visual+Tracking+with+Online+Reservoir+Metric+Learning+Li,+Xi+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Non-sparse+Linear+Representations+for+Visual+Tracking+with+Online+Reservoir+Metric+Learning" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Sharing features in multi-class boosting via group sparsity</b> <br>\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;12), 2012</i>. <br>\(\cdot\) <a href=http://hdl.handle.net/2440/69851 target=“blank”>pdf</a><a href=data/bibtex/CVPR12b.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Sharing+Features+in+Multi-class+Boosting+via+Group+Sparsity+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Sharing+Features+in+Multi-class+Boosting+via+Group+Sparsity" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Real-time visual tracking using compressive sensing</b> <br>\(\cdot\) <i>H. Li, C. Shen, Q. Shi</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>. <br>\(\cdot\) <a href=http://goo.gl/dsjsoM target=“blank”>pdf</a><a href=data/bibtex/Li2011CVPR.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Real-time+visual+tracking+Using+compressive+sensing+Li,+Hanxi+and+Shen,+Chunhua+and+Shi,+Qinfeng" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Real-time+visual+tracking+Using+compressive+sensing" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>A generalized probabilistic framework for compact codebook creation</b> <br>\(\cdot\) <i>L. Liu, L. Wang, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>. <br>\(\cdot\) <a href=http://hdl.handle.net/2440/63014 target=“blank”>pdf</a><a href=data/bibtex/Liu2011CVPR.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=A+generalized+probabilistic+framework+for+compact+codebook+creation+Liu,+Lingqiao+and+Wang,+Lei+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+generalized+probabilistic+framework+for+compact+codebook+creation" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>A direct formulation for totally-corrective multi-class boosting</b> <br>\(\cdot\) <i>C. Shen, Z. Hao</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>. <br>\(\cdot\) <a href=http://hdl.handle.net/2440/62919 target=“blank”>pdf</a><a href=data/bibtex/Shen2011CVPRa.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=A+direct+formulation+for+totally-corrective+multi-class+boosting+Shen,+Chunhua+and+Hao,+Zhihui" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+direct+formulation+for+totally-corrective+multi-class+boosting" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Shen2011CVPRb_PDF.jpg><b>A scalable dual approach to semidefinite metric learning</b> <br>\(\cdot\) <i>C. Shen, J. Kim, L. Wang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>. <br>\(\cdot\) <a href=http://goo.gl/UyVdEc target=“blank”>pdf</a><a href=data/bibtex/Shen2011CVPRb.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=A+Scalable+Dual+Approach+to+Semidefinite+Metric+Learning+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Scalable+Dual+Approach+to+Semidefinite+Metric+Learning" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Is face recognition really a compressive sensing problem?</b> <br>\(\cdot\) <i>Q. Shi, A. Eriksson, A. van den Hengel, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>. <br>\(\cdot\) <a href=http://hdl.handle.net/2440/67036 target=“blank”>pdf</a><a href=data/bibtex/Shi2011CVPR.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Is+face+recognition+really+a+Compressive+Sensing+problem?+Shi,+Qinfeng+and+Eriksson,+Anders+and+van+den+Hengel,+Anton+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Is+face+recognition+really+a+Compressive+Sensing+problem?" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Shi2010CVPR_PDF.jpg><b>Rapid face recognition using hashing</b> <br>\(\cdot\) <i>Q. Shi, H. Li, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;10), 2010</i>. <br>\(\cdot\) <a href="http://sites.google.com/site/chhshen/publication/cvpr10.pdf?attredirects=1" target=“blank”>pdf</a><a href=data/bibtex/Shi2010CVPR.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Rapid+face+recognition+using+hashing+Shi,+Qinfeng+and+Li,+Hanxi+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Rapid+face+recognition+using+hashing" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Paisitkriangkrai2009CVPR_PDF.jpg><b>Efficiently training a better visual detector with sparse eigenvectors</b> <br>\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;09), 2009</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/0903.3103 target=“blank”>arXiv</a><a href="http://sites.google.com/site/chhshen/publication/CVPR2009GSLDA.pdf?attredirects=1" target=“blank”>link</a><a href=data/bibtex/Paisitkriangkrai2009CVPR.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Efficiently+Training+a+Better+Visual+Detector+with+Sparse+Eigenvectors+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficiently+Training+a+Better+Visual+Detector+with+Sparse+Eigenvectors" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Kernel2007Quang_PDF.jpg><b>Kernel-based tracking from a probabilistic viewpoint</b> <br>\(\cdot\) <i>Q. Nguyen, A. Robles-Kelly, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;07), 2007</i>. <br>\(\cdot\) <a href=http://dx.doi.org/10.1109/CVPR.2007.383240 target=“blank”>link</a><a href=http://goo.gl/1QNmaq target=“blank”>pdf</a><a href=data/bibtex/Kernel2007Quang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Kernel-based+tracking+from+a+probabilistic+viewpoint+Nguyen,+Quang+and+Robles-Kelly,+Antonio+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Kernel-based+tracking+from+a+probabilistic+viewpoint" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Learning and memorizing representative prototypes for 3D point cloud semantic and instance segmentation</b> <br>\(\cdot\) <i>T. He, D. Gong, Z. Tian, C. Shen</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=data/bibtex/He2020PC1.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Learning+and+Memorizing+Representative+Prototypes+for+{3D}+Point+Cloud+Semantic+and+Instance+Segmentation+He,+Tong+and+Gong,+Dong+and+Tian,+Zhi+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+and+Memorizing+Representative+Prototypes+for+{3D}+Point+Cloud+Semantic+and+Instance+Segmentation" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Instance-aware embedding for point cloud instance segmentation</b> <br>\(\cdot\) <i>T. He, Y. Liu, C. Shen, X. Wang, C. Sun</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=data/bibtex/He2020InstanceAware.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Instance-Aware+Embedding+for+Point+Cloud+Instance+Segmentation+He,+Tong+and+Liu,+Yifan+and+Shen,+Chunhua+and+Wang,+Xinlong+and+Sun,+Changming" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Instance-Aware+Embedding+for+Point+Cloud+Instance+Segmentation" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Liu2020WeightingRL_arXiv.jpg><b>Weighing counts: sequential crowd counting by reinforcement learning</b> <br>\(\cdot\) <i>L. Liu, H. Lu, H. Zou, H. Xiong, Z. Cao, C. Shen</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2007.08260 target=“blank”>arXiv</a><a href=data/bibtex/Liu2020WeightingRL.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Weighing+Counts:+Sequential+Crowd+Counting+by+Reinforcement+Learning+Liu,+Liang+and+Lu,+Hao+and+Zou,+Hongwei+and+Xiong,+Haipeng+and+Cao,+Zhiguo+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Weighing+Counts:+Sequential+Crowd+Counting+by+Reinforcement+Learning" target=“blank”>semantic scholar</a><a href=https://github.com/poppinace/libranet target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Liu2020EfficientSemantic_arXiv.jpg><b>Efficient semantic video segmentation with per-frame inference</b> <br>\(\cdot\) <i>Y. Liu, C. Shen, C. Yu, J. Wang</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2002.11433 target=“blank”>arXiv</a><a href=data/bibtex/Liu2020EfficientSemantic.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Semantic+Video+Segmentation+with+Per-frame+Inference+Liu,+Yifan+and+Shen,+Chunhua+and+Yu,+Changqian+and+Wang,+Jingdong" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+Semantic+Video+Segmentation+with+Per-frame+Inference" target=“blank”>semantic scholar</a><a href=https://tinyurl.com/segment-video target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Tian2020CondInst_arXiv.jpg><b>Conditional convolutions for instance segmentation</b> <br>\(\cdot\) <i>Z. Tian, C. Shen, H. Chen</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2003.05664 target=“blank”>arXiv</a><a href=data/bibtex/Tian2020CondInst.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Conditional+Convolutions+for+Instance+Segmentation+Tian,+Zhi+and+Shen,+Chunhua+and+Chen,+Hao" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Conditional+Convolutions+for+Instance+Segmentation" target=“blank”>semantic scholar</a><a href=https://github.com/aim-uofa/adet target=“blank”>project webpage</a> </p> <ol reversed> <li><p>Oral presentation. </p> </li></ol> </li> <li><p><b>Soft expert reward learning for vision-and-language navigation</b> <br>\(\cdot\) <i>H. Wang, Q. Wu, C. Shen</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=data/bibtex/Wang2020Soft.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Soft+Expert+Reward+Learning+for+Vision-and-Language+Navigation+Wang,+Hu+and+Wu,+Qi+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Soft+Expert+Reward+Learning+for+Vision-and-Language+Navigation" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>AE TextSpotter: learning visual and linguistic representation for ambiguous text spotting</b> <br>\(\cdot\) <i>W. Wang, X. Liu, X. Ji, E. Xie, D. Liang, Z. Yang, T. Lu, C. Shen, P. Luo</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=data/bibtex/Wang2020AET.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={AE+TextSpotter}:+Learning+Visual+and+Linguistic+Representation+for+Ambiguous+Text+Spotting+Wang,+Wenhai+and+Liu,+Xuebo+and+Ji,+Xiaozhong+and+Xie,+Enze+and+Liang,+Ding+and+Yang,+ZhiBo+and+Lu,+Tong+and+Shen,+Chunhua+and+Luo,+Ping" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={AE+TextSpotter}:+Learning+Visual+and+Linguistic+Representation+for+Ambiguous+Text+Spotting" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Wang2020SuperRes_arXiv.jpg><b>Scene text image super-resolution in the wild</b> <br>\(\cdot\) <i>W. Wang, E. Xie, X. Liu, W. Wang, D. Liang, C. Shen, X. Bai</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2005.03341 target=“blank”>arXiv</a><a href=data/bibtex/Wang2020SuperRes.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Scene+Text+Image+Super-Resolution+in+the+Wild+Wang,+Wenjia+and+Xie,+Enze+and+Liu,+Xuebo+and+Wang,+Wenhai+and+Liang,+Ding+and+Shen,+Chunhua+and+Bai,+Xiang" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Scene+Text+Image+Super-Resolution+in+the+Wild" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Wang2020SOLO_arXiv.jpg><b>SOLO: segmenting objects by locations</b> <br>\(\cdot\) <i>X. Wang, T. Kong, C. Shen, Y. Jiang, L. Li</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1912.04488 target=“blank”>arXiv</a><a href=data/bibtex/Wang2020SOLO.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={SOLO}:+Segmenting+Objects+by+Locations+Wang,+Xinlong+and+Kong,+Tao+and+Shen,+Chunhua+and+Jiang,+Yuning+and+Li,+Lei" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={SOLO}:+Segmenting+Objects+by+Locations" target=“blank”>semantic scholar</a><a href=https://github.com/aim-uofa/adet target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Xie2020Segtransp_arXiv.jpg><b>Segmenting transparent objects in the wild</b> <br>\(\cdot\) <i>E. Xie, W. Wang, W. Wang, M. Ding, C. Shen, P. Luo</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2003.13948 target=“blank”>arXiv</a><a href=data/bibtex/Xie2020Segtransp.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Segmenting+Transparent+Objects+in+the+Wild+Xie,+Enze+and+Wang,+Wenjia+and+Wang,+Wenhai+and+Ding,+Mingyu+and+Shen,+Chunhua+and+Luo,+Ping" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Segmenting+Transparent+Objects+in+the+Wild" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Yu2020RepGraphNet_arXiv.jpg><b>Representative graph neural network</b> <br>\(\cdot\) <i>C. Yu, Y. Liu, C. Gao, C. Shen, N. Sang</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2008.05202 target=“blank”>arXiv</a><a href=data/bibtex/Yu2020RepGraphNet.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Representative+Graph+Neural+Network+Yu,+Changqian+and+Liu,+Yifan+and+Gao,+Changxin+and+Shen,+Chunhua+and+Sang,+Nong" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Representative+Graph+Neural+Network" target=“blank”>semantic scholar</a><a href=https://github.com/ycszen/RepGraph target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1807.10097.pdf><img class="imgP  right" src="data/thumbnail/Deng2018ECCV</u>arXiv.jpg"></a><b>Learning to predict crisp boundaries</b> <br>\(\cdot\) <i>R. Deng, C. Shen, S. Liu, H. Wang, X. Liu</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1807.10097 target=“blank”>arXiv</a><a href=data/bibtex/Deng2018ECCV.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Learning+to+Predict+Crisp+Boundaries+Deng,+Ruoxi+and+Shen,+Chunhua+and+Liu,+Shengjun+and+Wang,+Huibing+and+Liu,+Xinru" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+to+Predict+Crisp+Boundaries" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Zhang2018ECCV_arXiv.jpg><b>Goal-oriented visual question generation via intermediate rewards</b> <br>\(\cdot\) <i>J. Zhang, Q. Wu, C. Shen, J. Zhang, J. Lu, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1711.07614 target=“blank”>arXiv</a><a href=data/bibtex/Zhang2018ECCV.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Goal-Oriented+Visual+Question+Generation+via+Intermediate+Rewards+Zhang,+Junjie+and+Wu,+Qi+and+Shen,+Chunhua+and+Zhang,+Jian+and+Lu,+Jianfeng+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Goal-Oriented+Visual+Question+Generation+via+Intermediate+Rewards" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ECCV16Li_arXiv.jpg><b>Image co-localization by mimicking a good detector's confidence score distribution</b> <br>\(\cdot\) <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;16), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1603.04619 target=“blank”>arXiv</a><a href=data/bibtex/ECCV16Li.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Image+Co-localization+by+Mimicking+a+Good+Detector's+Confidence+Score+Distribution+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Image+Co-localization+by+Mimicking+a+Good+Detector's+Confidence+Score+Distribution" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Cluster sparsity field for hyperspectral imagery denoising</b> <br>\(\cdot\) <i>L. Zhang, W. Wei, Y. Zhang, C. Shen, A. van den Hengel, Q. Shi</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;16), 2016</i>. <br>\(\cdot\) <a href=data/bibtex/ECCV16hyperspectral.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Cluster+Sparsity+Field+for+Hyperspectral+Imagery+Denoising+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Cluster+Sparsity+Field+for+Hyperspectral+Imagery+Denoising" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1407.1151.pdf><img class="imgP  right" src="data/thumbnail/ECCV14Lin</u>arXiv.jpg"></a><b>Optimizing ranking measures for compact binary code learning</b> <br>\(\cdot\) <i>G. Lin, C. Shen, J. Wu</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;14), 2014</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1407.1151 target=“blank”>arXiv</a><a href=data/bibtex/ECCV14Lin.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Optimizing+Ranking+Measures+for+Compact+Binary+Code+Learning+Lin,+Guosheng+and+Shen,+Chunhua+and+Wu,+Jianxin" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Optimizing+Ranking+Measures+for+Compact+Binary+Code+Learning" target=“blank”>semantic scholar</a><a href=https://bitbucket.org/guosheng/structhash target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1407.0786.pdf><img class="imgP  right" src="data/thumbnail/ECCV14Paul</u>arXiv.jpg"></a><b>Strengthening the effectiveness of pedestrian detection with spatially pooled features</b> <br>\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;14), 2014</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1407.0786 target=“blank”>arXiv</a><a href=data/bibtex/ECCV14Paul.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Strengthening+the+Effectiveness+of+Pedestrian+Detection+with+Spatially+Pooled+Features+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Strengthening+the+Effectiveness+of+Pedestrian+Detection+with+Spatially+Pooled+Features" target=“blank”>semantic scholar</a><a href=https://github.com/chhshen/pedestrian-detection target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ECCV12_PDF.jpg><b>Robust tracking with weighted online structured learning</b> <br>\(\cdot\) <i>R. Yao, Q. Shi, C. Shen, Y. Zhang, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;12), 2012</i>. <br>\(\cdot\) <a href="https://sites.google.com/site/chhshen/publication/weighted_tracking_eccv12.pdf?attredirects=1" target=“blank”>pdf</a><a href=data/bibtex/ECCV12.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Robust+Tracking+with+Weighted+Online+Structured+Learning+Yao,+Rui+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Robust+Tracking+with+Weighted+Online+Structured+Learning" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Shen2010ECCV_arXiv.jpg><b>LACBoost and FisherBoost: optimally building cascade classifiers</b> <br>\(\cdot\) <i>C. Shen, P. Wang, H. Li</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;10), 2010</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1005.4103 target=“blank”>arXiv</a><a href=http://dx.doi.org/10.1007/978-3-642-15552-9_44 target=“blank”>link</a><a href=data/bibtex/Shen2010ECCV.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={LACBoost}+and+{FisherBoost}:+Optimally+Building+Cascade+Classifiers+Shen,+Chunhua+and+Wang,+Peng+and+Li,+Hanxi" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={LACBoost}+and+{FisherBoost}:+Optimally+Building+Cascade+Classifiers" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Fast2008Wang_PDF.jpg><b>A fast algorithm for creating a compact and discriminative visual codebook</b> <br>\(\cdot\) <i>L. Wang, L. Zhou, C. Shen</i>. <br>\(\cdot\) <i>Proc. European Conference on Computer Vision (ECCV&rsquo;08), 2008</i>. <br>\(\cdot\) <a href=http://dx.doi.org/10.1007/978-3-540-88693-8_53 target=“blank”>link</a><a href="http://sites.google.com/site/chhshen/publication/ECCV2008Wang.pdf?attredirects=1" target=“blank”>pdf</a><a href=data/bibtex/Fast2008Wang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=A+Fast+Algorithm+for+Creating+a+Compact+and+Discriminative+Visual+Codebook+Wang,+Lei+and+Zhou,+Luping+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Fast+Algorithm+for+Creating+a+Compact+and+Discriminative+Visual+Codebook" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/2008.05101.pdf><img class="imgP  right" src="data/thumbnail/Chen2021ICCV</u>arXiv.jpg"></a><b>FATNN: fast and accurate ternary neural networks</b> <br>\(\cdot\) <i>P. Chen, B. Zhuang, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2008.05101 target=“blank”>arXiv</a><a href=data/bibtex/Chen2021ICCV.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={FATNN}:+Fast+and+Accurate+Ternary+Neural+Networks+Chen,+Peng+and+Zhuang,+Bohan+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={FATNN}:+Fast+and+Accurate+Ternary+Neural+Networks" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Channel-wise knowledge distillation for dense prediction</b> <br>\(\cdot\) <i>C. Shu, Y. Liu, J. Gao, L. Xu, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2011.13256 target=“blank”>arXiv</a><a href=data/bibtex/Shu2021ICCVKD.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Channel-wise+Knowledge+Distillation+for+Dense+Prediction+Shu,+Changyong+and+Liu,+Yifan+and+Gao,+Jianfei+and+Xu,+Lin+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Channel-wise+Knowledge+Distillation+for+Dense+Prediction" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Occluded person re-identification with single-scale global representations</b> <br>\(\cdot\) <i>C. Yan, G. Pang, J. Jiao, X. Bai, X. Feng, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=data/bibtex/Yan2021ICCVOccl.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Occluded+Person+Re-Identification+with+Single-scale+Global+Representations+Yan,+Cheng+and+Pang,+Guansong+and+Jiao,+Jile+and+Bai,+Xiao+and+Feng,+Xuetao+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Occluded+Person+Re-Identification+with+Single-scale+Global+Representations" target=“blank”>semantic scholar</a> </p> <ol reversed> <li><p>Oral presentation. </p> </li></ol> </li> <li><p><b>BV-Person: a large-scale dataset for bird-view person re-identification</b> <br>\(\cdot\) <i>C. Yan, G. Pang, L. Wang, J. Jiao, X. Feng, C. Shen, J. Li</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=data/bibtex/Yan2021ICCVBVPerson.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={BV-Person}:+A+Large-scale+Dataset+for+Bird-view+Person+Re-identification+Yan,+Cheng+and+Pang,+Guansong+and+Wang,+Lei+and+Jiao,+Jile+and+Feng,+Xuetao+and+Shen,+Chunhua+and+Li,+Jingjing" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={BV-Person}:+A+Large-scale+Dataset+for+Bird-view+Person+Re-identification" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>A simple baseline for semi-supervised semantic segmentation with strong data augmentation</b> <br>\(\cdot\) <i>J. Yuan, Y. Liu, C. Shen, Z. Wang, H. Li</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2104.07256 target=“blank”>arXiv</a><a href=data/bibtex/Yuan2021ICCVSimple.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=A+Simple+Baseline+for+Semi-supervised+Semantic+Segmentation+with+Strong+Data+Augmentation+Yuan,+Jianlong+and+Liu,+Yifan+and+Shen,+Chunhua+and+Wang,+Zhibin+and+Li,+Hao" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+Simple+Baseline+for+Semi-supervised+Semantic+Segmentation+with+Strong+Data+Augmentation" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>Meta navigator: search for a good adaptation policy for few-shot learning</b> <br>\(\cdot\) <i>C. Zhang, H. Ding, G. Lin, R. Li, C. Wang, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2109.05749 target=“blank”>arXiv</a><a href=data/bibtex/Chizhang2021ICCVMeta.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Meta+Navigator:+Search+for+a+Good+Adaptation+Policy+for+Few-shot+Learning+Zhang,+Chi+and+Ding,+Henghui+and+Lin,+Guosheng+and+Li,+Ruibo+and+Wang,+Changhu+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Meta+Navigator:+Search+for+a+Good+Adaptation+Policy+for+Few-shot+Learning" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Matting2019Lu_arXiv.jpg><b>Indices matter: learning to index for deep image matting</b> <br>\(\cdot\) <i>H. Lu, Y. Dai, C. Shen, S. Xu</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1908.00672 target=“blank”>arXiv</a><a href=data/bibtex/Matting2019Lu.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Indices+Matter:+Learning+to+Index+for+Deep+Image+Matting+Lu,+Hao+and+Dai,+Yutong+and+Shen,+Chunhua+and+Xu,+Songcen" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Indices+Matter:+Learning+to+Index+for+Deep+Image+Matting" target=“blank”>semantic scholar</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1904.01355.pdf><img class="imgP  right" src="data/thumbnail/FCOS2019Tian</u>arXiv.jpg"></a><b>FCOS: fully convolutional one-stage object detection</b> <br>\(\cdot\) <i>Z. Tian, C. Shen, H. Chen, T. He</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1904.01355 target=“blank”>arXiv</a><a href=data/bibtex/FCOS2019Tian.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={FCOS}:+Fully+Convolutional+One-Stage+Object+Detection+Tian,+Zhi+and+Shen,+Chunhua+and+Chen,+Hao+and+He,+Tong" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={FCOS}:+Fully+Convolutional+One-Stage+Object+Detection" target=“blank”>semantic scholar</a><a href=https://tinyurl.com/FCOSv1 target=“blank”>project webpage</a> </p> </li> <li><p><b>Efficient and accurate arbitrary-shaped text detection with pixel aggregation network</b> <br>\(\cdot\) <i>W. Wang, E. Xie, X. Song, Y. Zang, W. Wang, T. Lu, G. Yu, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=data/bibtex/TextDet2019Wang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Efficient+and+Accurate+Arbitrary-Shaped+Text+Detection+with+Pixel+Aggregation+Network+Wang,+Wenhai+and+Xie,+Enze+and+Song,+Xiaoge+and+Zang,+Yuhang+and+Wang,+Wenjia+and+Lu,+Tong+and+Yu,+Gang+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+and+Accurate+Arbitrary-Shaped+Text+Detection+with+Pixel+Aggregation+Network" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/OpenSet2019Xiong_arXiv.jpg><b>From open set to closed set: counting objects by spatial divide-and-conquer</b> <br>\(\cdot\) <i>H. Xiong, H. Lu, C. Liu, L. Liu, Z. Cao, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1908.06473 target=“blank”>arXiv</a><a href=data/bibtex/OpenSet2019Xiong.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=From+Open+Set+to+Closed+Set:+Counting+Objects+by+Spatial+Divide-and-Conquer+Xiong,+Haipeng+and+Lu,+Hao+and+Liu,+Chengxin+and+Liu,+Liang+and+Cao,+Zhiguo+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=From+Open+Set+to+Closed+Set:+Counting+Objects+by+Spatial+Divide-and-Conquer" target=“blank”>semantic scholar</a><a href=https://github.com/xhp-hust-2018-2011/S-DCNet target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/VNL2019Yin_arXiv.jpg><b>Enforcing geometric constraints of virtual normal for depth prediction</b> <br>\(\cdot\) <i>W. Yin, Y. Liu, C. Shen, Y. Yan</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1907.12209 target=“blank”>arXiv</a><a href=data/bibtex/VNL2019Yin.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Enforcing+geometric+constraints+of+virtual+normal+for+depth+prediction+Yin,+Wei+and+Liu,+Yifan+and+Shen,+Chunhua+and+Yan,+Youliang" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Enforcing+geometric+constraints+of+virtual+normal+for+depth+prediction" target=“blank”>semantic scholar</a><a href=https://github.com/YvanYin/VNL_Monocular_Depth_Prediction target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Temporal2019Zhang_arXiv.jpg><b>Exploiting temporal consistency for real-time video depth estimation</b> <br>\(\cdot\) <i>H. Zhang, C. Shen, Y. Li, Y. Cao, Y. Liu, Y. Yan</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1908.03706 target=“blank”>arXiv</a><a href=data/bibtex/Temporal2019Zhang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Exploiting+temporal+consistency+for+real-time+video+depth+estimation+Zhang,+Haokui+and+Shen,+Chunhua+and+Li,+Ying+and+Cao,+Yuanzhouhan+and+Liu,+Yu+and+Yan,+Youliang" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Exploiting+temporal+consistency+for+real-time+video+depth+estimation" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/PersonReID2019Zhang_arXiv.jpg><b>Self-training with progressive augmentation for unsupervised cross-domain person re-identification</b> <br>\(\cdot\) <i>X. Zhang, J. Cao, C. Shen, M. You</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1907.13315 target=“blank”>arXiv</a><a href=data/bibtex/PersonReID2019Zhang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Self-Training+with+Progressive+Augmentation+for+Unsupervised+Cross-Domain+Person+Re-Identification+Zhang,+Xinyu+and+Cao,+Jiewei+and+Shen,+Chunhua+and+You,+Mingyu" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Self-Training+with+Progressive+Augmentation+for+Unsupervised+Cross-Domain+Person+Re-Identification" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ICCV2017Chen_arXiv.jpg><b>Adversarial PoseNet: a structure-aware convolutional network for human pose estimation</b> <br>\(\cdot\) <i>Y. Chen, C. Shen, X. Wei, L. Liu, J. Yang</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1705.00389 target=“blank”>arXiv</a><a href=data/bibtex/ICCV2017Chen.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Adversarial+{PoseNet}:+A+Structure-aware+Convolutional+Network+for+Human+Pose+Estimation+Chen,+Yu+and+Shen,+Chunhua+and+Wei,+Xiu-Shen+and+Liu,+Lingqiao+and+Yang,+Jian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Adversarial+{PoseNet}:+A+Structure-aware+Convolutional+Network+for+Human+Pose+Estimation" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ICCV2017HuiLi_arXiv.jpg><b>Towards end-to-end text spotting with convolutional recurrent neural networks</b> <br>\(\cdot\) <i>H. Li, P. Wang, C. Shen</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1707.03985 target=“blank”>arXiv</a><a href=data/bibtex/ICCV2017HuiLi.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Towards+End-to-end+Text+Spotting+with+Convolutional+Recurrent+Neural+Networks+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+End-to-end+Text+Spotting+with+Convolutional+Recurrent+Neural+Networks" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ICCV2017WeiLiu_arXiv.jpg><b>Semi-global weighted least squares in image filtering</b> <br>\(\cdot\) <i>W. Liu, X. Chen, C. Shen, Z. Liu, J. Yang</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1705.01674 target=“blank”>arXiv</a><a href=data/bibtex/ICCV2017WeiLiu.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Semi-Global+Weighted+Least+Squares+in+Image+Filtering+Liu,+Wei+and+Chen,+Xiaogang+and+Shen,+Chuanhua+and+Liu,+Zhi+and+Yang,+Jie" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Semi-Global+Weighted+Least+Squares+in+Image+Filtering" target=“blank”>semantic scholar</a> </p> </li> <li><p><b>When unsupervised domain adaptation meets tensor representations</b> <br>\(\cdot\) <i>H. Lu, L. Zhang, Z. Cao, W. Wei, K. Xian, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>. <br>\(\cdot\) <a href=data/bibtex/ICCV2017Haolu.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=When+Unsupervised+Domain+Adaptation+Meets+Tensor+Representations+Lu,+Hao+and+Zhang,+Lei+and+Cao,+Zhiguo+and+Wei,+Wei+and+Xian,+Ke+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=When+Unsupervised+Domain+Adaptation+Meets+Tensor+Representations" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ICCV2017Zhuang_arXiv.jpg><b>Towards context-aware interaction recognition</b> <br>\(\cdot\) <i>B. Zhuang, L. Liu, C. Shen, I. Reid</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1703.06246 target=“blank”>arXiv</a><a href=data/bibtex/ICCV2017Zhuang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Towards+Context-aware+Interaction+Recognition+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Towards+Context-aware+Interaction+Recognition" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ICCV15Zhang_PDF.jpg><b>Hyperspectral compressive sensing using manifold-structured sparsity prior</b> <br>\(\cdot\) <i>L. Zhang, W. Wei, Y. Zhang, F. Li, C. Shen, Q. Shi</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;15), 2015</i>. <br>\(\cdot\) <a href=http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zhang_Hyperspectral_Compressive_Sensing_ICCV_2015_paper.pdf target=“blank”>pdf</a><a href=data/bibtex/ICCV15Zhang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Hyperspectral+Compressive+Sensing+Using+Manifold-Structured+Sparsity+Prior+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Li,+Fei+and+Shen,+Chunhua+and+Shi,+Qinfeng" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Hyperspectral+Compressive+Sensing+Using+Manifold-Structured+Sparsity+Prior" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ICCV13Li_arXiv.jpg><b>Contextual hypergraph modeling for salient object detection</b> <br>\(\cdot\) <i>X. Li, Y. Li, C. Shen, A. Dick, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1310.5767 target=“blank”>arXiv</a><a href=data/bibtex/ICCV13Li.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Contextual+Hypergraph+Modeling+for+Salient+Object+Detection+Li,+Xi+and+Li,+Yao+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Contextual+Hypergraph+Modeling+for+Salient+Object+Detection" target=“blank”>semantic scholar</a><a href=https://bitbucket.org/chhshen/saliency-detection target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ICCV13Lin_arXiv.jpg><b>A general two-step approach to learning-based hashing</b> <br>\(\cdot\) <i>G. Lin, C. Shen, D. Suter, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1309.1853 target=“blank”>arXiv</a><a href=data/bibtex/ICCV13Lin.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=A+General+Two-step+Approach+to+Learning-Based+Hashing+Lin,+Guosheng+and+Shen,+Chunhua+and+Suter,+David+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=A+General+Two-step+Approach+to+Learning-Based+Hashing" target=“blank”>semantic scholar</a><a href=https://bitbucket.org/guosheng/two-step-hashing/ target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ICCV13Pai_arXiv.jpg><b>Efficient pedestrian detection by directly optimizing the partial area under the ROC curve</b> <br>\(\cdot\) <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1310.0900 target=“blank”>arXiv</a><a href=http://hdl.handle.net/2440/83158 target=“blank”>pdf</a><a href=data/bibtex/ICCV13Pai.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Efficient+pedestrian+detection+by+directly+optimizing+the+partial+area+under+the+{ROC}+curve+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Efficient+pedestrian+detection+by+directly+optimizing+the+partial+area+under+the+{ROC}+curve" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ICCV2013Harandi_arXiv.jpg><b>Dictionary learning and sparse coding on Grassmann manifolds: an extrinsic solution</b> <br>\(\cdot\) <i>M. Harandi, C. Sanderson, C. Shen, B. Lovell</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1310.4891 target=“blank”>arXiv</a><a href=data/bibtex/ICCV2013Harandi.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Dictionary+Learning+and+Sparse+Coding+on+{G}rassmann+Manifolds:+An+Extrinsic+Solution+{Harandi},+Mehrtash+and+{Sanderson},+Conrad+and+Shen,+Chunhua+and+Lovell,+Brian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Dictionary+Learning+and+Sparse+Coding+on+{G}rassmann+Manifolds:+An+Extrinsic+Solution" target=“blank”>semantic scholar</a><a href=https://github.com/chhshen/Grassmann/ target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ICCV2011_PDF.jpg><b>Graph mode-based contextual kernels for robust SVM tracking</b> <br>\(\cdot\) <i>X. Li, A. Dick, H. Wang, C. Shen, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;11), 2011</i>. <br>\(\cdot\) <a href=http://goo.gl/GzpBVb target=“blank”>pdf</a><a href=data/bibtex/ICCV2011.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Graph+mode-based+contextual+kernels+for+robust+{SVM}+tracking+Li,+Xi+and+Dick,+Anthony+and+Wang,+Hanzi+and+Shen,+Chunhua+and+van+den+Hengel,+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Graph+mode-based+contextual+kernels+for+robust+{SVM}+tracking" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Shen2005Fast_PDF.jpg><b>Fast global kernel density mode seeking with application to localisation and tracking</b> <br>\(\cdot\) <i>C. Shen, M. Brooks, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;05), 2005</i>. <br>\(\cdot\) <a href=http://dx.doi.org/10.1109/ICCV.2005.94 target=“blank”>link</a><a href=http://goo.gl/UHzjWW target=“blank”>pdf</a><a href=data/bibtex/Shen2005Fast.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Fast+global+kernel+density+mode+seeking+with+application+to+localisation+and+tracking+Shen,+Chunhua+and+Brooks,+Michael+J.+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Fast+global+kernel+density+mode+seeking+with+application+to+localisation+and+tracking" target=“blank”>semantic scholar</a> </p> <ol reversed> <li><p>Oral presentation, 45 out of 1200 submissions. </p> </li></ol> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1806.04895.pdf><img class="imgP  right" src="data/thumbnail/Cao2018ICML</u>arXiv.jpg"></a><b>Adversarial learning with local coordinate coding</b> <br>\(\cdot\) <i>J. Cao, Y. Guo, Q. Wu, C. Shen, J. Huang, M. Tan</i>. <br>\(\cdot\) <i>Proc. International Conference on Machine Learning (ICML&rsquo;18), 2018</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1806.04895 target=“blank”>arXiv</a><a href=data/bibtex/Cao2018ICML.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Adversarial+Learning+with+Local+Coordinate+Coding+Cao,+Jiezhang+and+Guo,+Yong+and+Wu,+Qingyao+and+Shen,+Chunhua+and+Huang,+Junzhou+and+Tan,+Mingkui" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Adversarial+Learning+with+Local+Coordinate+Coding" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/ICML13a_PDF.jpg><b>Learning hash functions using column generation</b> <br>\(\cdot\) <i>X. Li, G. Lin, C. Shen, A. van den Hengel, A. Dick</i>. <br>\(\cdot\) <i>Proc. International Conference on Machine Learning (ICML&rsquo;13), 2013</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1303.0339 target=“blank”>arXiv</a><a href=http://jmlr.csail.mit.edu/proceedings/papers/v28/li13a.pdf target=“blank”>pdf</a><a href=data/bibtex/ICML13a.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Learning+Hash+Functions+Using+Column+Generation+Li,+Xi+and+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Dick,+Anthony" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Learning+Hash+Functions+Using+Column+Generation" target=“blank”>semantic scholar</a><a href=https://bitbucket.org/guosheng/column-generation-hashing/ target=“blank”>project webpage</a> </p> <ol reversed> <li><p>Oral presentation. </p> </li></ol> </li> <li><p><img class="imgP  right" src=data/thumbnail/ICML12_arXiv.jpg><b>Is margin preserved after random projection?</b> <br>\(\cdot\) <i>Q. Shi, C. Shen, R. Hill, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. International Conference on Machine Learning (ICML&rsquo;12), 2012</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1206.4651 target=“blank”>arXiv</a><a href=http://hdl.handle.net/2440/71063 target=“blank”>link</a><a href=data/bibtex/ICML12.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Is+margin+preserved+after+random+projection?+Shi,+Qinfeng+and+Shen,+Chunhua+and+Hill,+Rhys+and+van+den+Hengel,+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Is+margin+preserved+after+random+projection?" target=“blank”>semantic scholar</a> </p> <ol reversed> <li><p>This work provides an analysis of margin distortion under random projections, the conditions under which margins are preserved, and presents bounds on the margin distortion. </p> </li></ol> </li> <li><p><b>Twins: revisiting the design of spatial attention in vision transformers</b> <br>\(\cdot\) <i>X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, C. Shen</i>. <br>\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2104.13840 target=“blank”>arXiv</a><a href=data/bibtex/Twins2021Chu.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Twins:+Revisiting+the+Design+of+Spatial+Attention+in+Vision+Transformers+Chu,+Xiangxiang+and+Tian,+Zhi+and+Wang,+Yuqing+and+Zhang,+Bo+and+Ren,+Haibing+and+Wei,+Xiaolin+and+Xia,+Huaxia+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Twins:+Revisiting+the+Design+of+Spatial+Attention+in+Vision+Transformers" target=“blank”>semantic scholar</a><a href=https://github.com/Meituan-AutoML/Twins target=“blank”>project webpage</a> </p> </li> <li><p><b>Dynamic neural representational decoders for high-resolution semantic segmentation</b> <br>\(\cdot\) <i>B. Zhang, Y. Liu, Z. Tian, C. Shen</i>. <br>\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;21), 2021</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2107.14428 target=“blank”>arXiv</a><a href=data/bibtex/DNRD2021Zhang.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Dynamic+Neural+Representational+Decoders+for+High-Resolution+Semantic+Segmentation+Zhang,+Bowen+and+Liu,+Yifan+and+Tian,+Zhi+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Dynamic+Neural+Representational+Decoders+for+High-Resolution+Semantic+Segmentation" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/SoloV22020_arXiv.jpg><b>SOLOv2: dynamic and fast instance segmentation</b> <br>\(\cdot\) <i>X. Wang, R. Zhang, T. Kong, L. Li, C. Shen</i>. <br>\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;20), 2020</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/2003.10152 target=“blank”>arXiv</a><a href=data/bibtex/SoloV22020.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={SOLOv2}:+Dynamic+and+Fast+Instance+Segmentation+Wang,+Xinlong+and+Zhang,+Rufeng+and+Kong,+Tao+and+Li,+Lei+and+Shen,+Chunhua" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={SOLOv2}:+Dynamic+and+Fast+Instance+Segmentation" target=“blank”>semantic scholar</a><a href=https://git.io/AdelaiDet target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Scale2019Bian_arXiv.jpg><b>Unsupervised scale-consistent depth and ego-motion learning from monocular video</b> <br>\(\cdot\) <i>J. Bian, Z. Li, N. Wang, H. Zhan, C. Shen, M. Cheng, I. Reid</i>. <br>\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1908.10553 target=“blank”>arXiv</a><a href=data/bibtex/Scale2019Bian.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Unsupervised+Scale-consistent+Depth+and+Ego-motion+Learning+from+Monocular+Video+Bian,+Jia-Wang+and+Li,+Zhichao+and+Wang,+Naiyan+and+Zhan,+Huangying+and+Shen,+Chunhua+and+Cheng,+Ming-Ming+and+Reid,+Ian" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Unsupervised+Scale-consistent+Depth+and+Ego-motion+Learning+from+Monocular+Video" target=“blank”>semantic scholar</a><a href=https://github.com/JiawangBian/SC-SfMLearner-Release target=“blank”>project webpage</a> </p> </li> <li><p><a class=imglink target="<u>blank" href=https://arxiv.org/pdf/1911.00888.pdf><img class="imgP  right" src="data/thumbnail/Cao2019GAN</u>arXiv.jpg"></a><b>Multi-marginal wasserstein GAN</b> <br>\(\cdot\) <i>J. Cao, L. Mo, Y. Zhang, K. Jia, C. Shen, M. Tan</i>. <br>\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;19), 2019</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1911.00888 target=“blank”>arXiv</a><a href=data/bibtex/Cao2019GAN.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Multi-marginal+Wasserstein+{GAN}+Cao,+Jiezhang+and+Mo,+Langyuan+and+Zhang,+Yifan+and+Jia,+Kui+and+Shen,+Chunhua+and+Tan,+Mingkui" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Multi-marginal+Wasserstein+{GAN}" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/NeurIPS2016_PDF.jpg><b>Image restoration using very deep fully convolutional encoder-decoder networks with symmetric skip connections</b> <br>\(\cdot\) <i>X. Mao, C. Shen, Y. Yang</i>. <br>\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;16), 2016</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1603.09056 target=“blank”>arXiv</a><a href=http://papers.NeurIPS.cc/paper/6172-image-restoration-using-very-deep-convolutional-encoder-decoder-networks-with-symmetric-skip-connections.pdf target=“blank”>link</a><a href=data/bibtex/NeurIPS2016.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Image+Restoration+Using+Very+Deep+Fully+Convolutional+Encoder-Decoder+Networks+with+Symmetric+Skip+Connections+Mao,+Xiao-Jiao+and+Shen,+Chunhua+and+Yang,+Yu-Bin" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Image+Restoration+Using+Very+Deep+Fully+Convolutional+Encoder-Decoder+Networks+with+Symmetric+Skip+Connections" target=“blank”>semantic scholar</a><a href=https://bitbucket.org/chhshen/image-denoising/ target=“blank”>project webpage</a> </p> <ol reversed> <li><p>Others have <a href=https://github.com/titu1994/Image-Super-Resolution target=“blank”>implemented our paper</a>. </p> </li></ol> </li> <li><p><img class="imgP  right" src=data/thumbnail/NeurIPS15Lin_PDF.jpg><b>Deeply learning the messages in message passing inference</b> <br>\(\cdot\) <i>G. Lin, C. Shen, I. Reid, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;15), 2015</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1506.02108 target=“blank”>arXiv</a><a href=http://papers.NeurIPS.cc/paper/5791-deeply-learning-the-messages-in-message-passing-inference.pdf target=“blank”>pdf</a><a href=data/bibtex/NeurIPS15Lin.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Deeply+Learning+the+Messages+in+Message+Passing+Inference+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Deeply+Learning+the+Messages+in+Message+Passing+Inference" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Liu2014Fisher_arXiv.jpg><b>Encoding high dimensional local features by sparse coding based Fisher vectors</b> <br>\(\cdot\) <i>L. Liu, C. Shen, L. Wang, A. van den Hengel, C. Wang</i>. <br>\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;14), 2014</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/1411.6406 target=“blank”>arXiv</a><a href=data/bibtex/Liu2014Fisher.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Encoding+High+Dimensional+Local+Features+by+Sparse+Coding+Based+{F}isher+Vectors+Liu,+Lingqiao+and+Shen,+Chunhua+and+Wang,+Lei+and+{van+den+Hengel},+Anton+and+Wang,+Chao" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Encoding+High+Dimensional+Local+Features+by+Sparse+Coding+Based+{F}isher+Vectors" target=“blank”>semantic scholar</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Shen2009PSD_PDF.jpg><b>Positive semidefinite metric learning with boosting</b> <br>\(\cdot\) <i>C. Shen, J. Kim, L. Wang, A. van den Hengel</i>. <br>\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;09), 2009</i>. <br>\(\cdot\) <a href=http://arxiv.org/abs/0910.2279 target=“blank”>arXiv</a><a href=http://papers.NeurIPS.cc/paper/3658-positive-semidefinite-metric-learning-with-boosting.pdf target=“blank”>pdf</a><a href=data/bibtex/Shen2009PSD.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=Positive+semidefinite+metric+learning+with+Boosting+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei+and+{van+den+Hengel},+Anton" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q=Positive+semidefinite+metric+learning+with+Boosting" target=“blank”>semantic scholar</a><a href=https://code.google.com/archive/p/boosting/downloads target=“blank”>project webpage</a> </p> </li> <li><p><img class="imgP  right" src=data/thumbnail/Shen2008PSD_PDF.jpg><b>PSDBoost: matrix-generation linear programming for positive semidefinite matrices learning</b> <br>\(\cdot\) <i>C. Shen, A. Welsh, L. Wang</i>. <br>\(\cdot\) <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;08), 2008</i>. <br>\(\cdot\) <a href=http://papers.NeurIPS.cc/paper/3611-psdboost-matrix-generation-linear-programming-for-positive-semidefinite-matrices-learning.pdf target=“blank”>pdf</a><a href=data/bibtex/Shen2008PSD.bib target=“blank”>bibtex</a><a href="https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q={PSDB}oost:+Matrix-generation+linear+programming+for+positive+semidefinite+matrices+learning+Shen,+Chunhua+and+Welsh,+Alan+and+Wang,+Lei" target=“blank”>google scholar</a><a href="https://www.semanticscholar.org/search?q={PSDB}oost:+Matrix-generation+linear+programming+for+positive+semidefinite+matrices+learning" target=“blank”>semantic scholar</a> </p> </li> </ol> <div id=footer> <div id=footer-text> &copy; <b>Chunhua Shen</b> &bull; last update: 2022-08-23 00:28:35 CST &bull; <a href=# onclick="changewidth(1);return false" title="Expand page width"><b>&larr;&rarr;</b></a> &bull; <a href=# onclick="changewidth(-1);return false" title="Reduce page width"><b>&rarr;&larr;</b></a> <!-- Javascript --> <script src=./script/jquery-1.6.2.min.js type=text/javascript>
</script> <script src=./script/jquery.flot.min.js type=text/javascript>
</script> <script src=./script/jquery-scroll.js type=text/javascript>
</script> <script src=./script/width_change.js type=text/javascript>
</script> <script src=./script/reverse_ol.js type=text/javascript>
</script> <script src=./script/jquery.highlight.js type=text/javascript>
</script> <!-- Required for the jQuery.LocalScroll Plug-in --> <script type=text/javascript>
    $(document).ready(function(){
    //
    $.localScroll();
    //
    // Round images
    //
	$(".rounded-img, .rounded-img2").load(function() {
	$(this).wrap(function(){
	return '<span class="' + $(this).attr('class')
                + '" style="background:url(' + $(this).attr('src')
                + ') no-repeat center center; width: '
                + $(this).width() + 'px; height: '
                + $(this).height() + 'px;" />';
		});
		$(this).css("opacity","0");
	});
      //
      //
      //  nav tab animation
        var navDuration = 150; //time in miliseconds
        $('#nav li a').hover(function() {
          $(this).animate({ paddingTop:"50px"  }, navDuration);
        }, function() {
             $(this).animate({ paddingTop:"31px"}, navDuration);
        });
        //
        // plot citation figure using jquery flot, 2012 July, CS
        //
        var flot_options = {
        legend: {
            show: false,
            margin: 10,
            backgroundOpacity: 0.5
                },
        bars:  {
            show: true,
            barWidth: 0.6,
            align: "center"
        },
        yaxis: {
            min: -20,
            tickFormatter: function(val, axis) {
                if (val < 50)
                    return " &nbsp; ";  // some string
                else
                    return val < axis.max ? val.toFixed(0) :   "  &nbsp;  ";
            }
        },
        grid: {
            borderWidth: 0
        }
    };  // end of flot_options
    $.getJSON("./data/cs_cite.json", function(json) {
       //succes - data loaded, now use plot:
       var plotarea = $("#citation_plot_holder");
       var data=[json.data];
       $.plot(plotarea , data, flot_options);
    });
//
// end of jquery flot
//
    changewidth( 0.9 );
//
//
//  highlight ``Shen''
    $("body p").highlight(['C. Shen', 'Chunhua Shen']);
//  highlight selected publication venues
    $("body p").highlight(['CVPR', 'ICCV', 'ECCV', 'ICML', 'NeurIPS',
    'TPAMI', 'IJCV', 'JMLR'],  { element: 'span', className: 'selected_venue' } );
//
    });
</script> <!-- News ticker --> <script type=text/javascript>
    function tick(){
        $('#ticker li:first').slideUp( function () { $(this).appendTo($('#ticker')).slideDown(); });
    }
    setInterval(function(){ tick () }, 5000);
</script> </div> </div> </div> </body> </html> 