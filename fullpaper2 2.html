<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<!--
<link rel="stylesheet" href="css/fonts_import.css"       type="text/css" />
-->
<link rel="stylesheet" href="css/cs.css"                 type="text/css" />
<link rel="stylesheet" href="css/content.css"            type="text/css" />
<!-- font family -->
<link href='//fonts.googleapis.com/css?family=Comfortaa'   rel='stylesheet' type='text/css' />
<link href='//fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css' />
<link href="//netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<link rel="stylesheet" href="css/full_publication.css" type="text/css" />
<title>Chunhua Shen | The University of Adelaide</title>
</head>
<body>
<div id="layout-content">
<div id="menu">
    <div id="menucontainer">
<ul id="nav">
   <li><a href="index.html" target="_self">Home</a></li>
   <li><a href="paper.html" target="_self">Publications</a></li>
   <li><a href="teaching.html" target="_self">Teaching</a></li>
</ul>
</div>
</div>
<div id="toptitle">
<h1>Publications (Full List)</h1>
<div id="subtitle">Categorised <a href="fullpaper2.html">by venue <i class='fa fa-location-arrow' aria-hidden='true'></i></a>,  <a href="fullpaper.html">by year <i class='fa fa-clock-o' aria-hidden='true'></i></a>. <b>320</b>  papers.</div>
</div>
<p><a href="http://scholar.google.com/citations?hl=en&amp;user=Ljk2BvIAAAAJ&amp;view_op=list_works&amp;pagesize=100">Google scholar (23147 citations)  <i class='ai ai-google-scholar'   aria-hidden='true'></i></a>,
<a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Chunhua">DBLP <i class='ai ai-dblp ai-1x'></i></a>,
<a href="https://tinyurl.com/ww4dlqm">arXiv <i class='ai ai-biorxiv ai-1x'></i></a>.</p>
<p><div id="citation_plot_holder"></div></p>
<h2>Journal</h2>
<ol reversed>
<li><p><b>Deep learning for anomaly detection: a review</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Pang, C. Shen, L. Cao, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>ACM Computing Surveys (ACMSurvey), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2007.02500">arXiv</a><a href="data/bibtex/Pan2020ACMSurvey.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Learning+for+Anomaly+Detection:+A+Review+Pang,+Guansong+and+Shen,+Chunhua+and+Cao,+Longbing+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Towards light-weight portrait matting via parameter sharing</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Dai, H. Lu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Computer Graphics Forum (CGF), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Daiyt2020.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Light-Weight+Portrait+Matting+via+Parameter+Sharing+Dai,+Yutong+and+Lu,+Hao+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Separating content from style using adversarial learning for recognizing text in the wild</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Luo, Q. Lin, Y. Liu, L. Jin, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2001.04189">arXiv</a><a href="data/bibtex/Luo2020IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Separating+Content+from+Style+Using+Adversarial+Learning+for+Recognizing+Text+in+the+Wild+Luo,+Canjie+and+Lin,+Qingxiang+and+Liu,+Yuliang+and+Jin,+Lianwen+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>TasselNetv2: in-field counting of wheat spikes with context-augmented local regression networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Xiong, Z. Cao, H. Lu, S. Madec, L. Liu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Plant Methods (PLME), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/TasselNet2020.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={TasselNetv2}:+in-field+counting+of+wheat+spikes+with+context-augmented+local+regression+networks+Xiong,+Haipeng+and+Cao,+Zhiguo+and+Lu,+Hao+and+Madec,+Simon+and+Liu,+Liang+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1908.03839.pdf"><img class="imgP  right"   src="data/thumbnail/MobileFAN2020_arXiv.jpg"></a><b>MobileFAN: transferring deep hidden representation for face alignment</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Zhao, Y. Liu, C. Shen, Y. Gao, S. Xiong</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.03839">arXiv</a><a href="data/bibtex/MobileFAN2020.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={MobileFAN}:+Transferring+Deep+Hidden+Representation+for+Face+Alignment+Zhao,+Yang+and+Liu,+Yifan+and+Shen,+Chunhua+and+Gao,+Yongsheng+and+Xiong,+Shengwu">search</a></p>
</li>
<li><p><b>Part-guided attention learning for vehicle instance retrieval</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Zhang, R. Zhang, J. Cao, D. Gong, M. You, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1909.06023">arXiv</a><a href="data/bibtex/Zhangx2020T-ITS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Part-Guided+Attention+Learning+for+Vehicle+Instance+Retrieval+Zhang,+Xinyu+and+Zhang,+Rufeng+and+Cao,+Jiewei+and+Gong,+Dong+and+You,+Mingyu+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>A robust attentional framework for license plate recognition in the wild</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhang, P. Wang, H. Li, Z. Li, C. Shen, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Li2020Carlicense.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+robust+attentional+framework+for+license+plate+recognition+in+the+wild+Zhang,+Linjiang+and+Wang,+Peng+and+Li,+Hui+and+Li,+Zhen+and+Shen,+Chunhua+and+Zhang,+Yanning">search</a></p>
</li>
<li><p><b>Real-time high-performance semantic image segmentation of urban street scenes</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Dong, Y. Yan, C. Shen, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Dong2020segmentation.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-time+high-performance+semantic+image+segmentation+of+urban+street+scenes+Dong,+Genshun+and+Yan,+Yan+and+Shen,+Chunhua+and+Wang,+Hanzi">search</a></p>
</li>
<li><p><b>Towards effective deep embedding for zero-shot learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhang, P. Wang, L. Liu, C. Shen, W. Wei, Y. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2020Zeroshot.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Effective+Deep+Embedding+for+Zero-Shot+Learning+Zhang,+Lei+and+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Wei,+Wei+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>NSSNet: scale-aware object counting with non-scale suppression</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Liu, Z. Cao, H. Lu, H. Xiong, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/LiuL2020CSVT.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={NSSNet}:+Scale-aware+object+counting+with+non-scale+suppression+Liu,+Liang+and+Cao,+Zhiguo+and+Lu,+Hao+and+Xiong,+Haipeng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Viral pneumonia screening on chest x-ray images using confidence-aware anomaly detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Zhang, Y. Xie, Z. Liao, G. Pang, J. Verjans, W. Li, Z. Sun, J. He, Y. Li, C. Shen, Y. Xia</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Medical Imaging (TMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.12338">arXiv</a><a href="data/bibtex/Zhang2020Covid.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Viral+Pneumonia+Screening+on+Chest+X-ray+Images+Using+Confidence-Aware+Anomaly+Detection+Zhang,+Jianpeng+and+Xie,+Yutong+and+Liao,+Zhibin+and+Pang,+Guansong+and+Verjans,+Johan+and+Li,+Wenxin+and+Sun,+Zongji+and+He,+Jian+and+Li,+Yi+and+Shen,+Chunhua+and+Xia,+Yong">search</a></p>
</li>
<li><p><b>A mutual bootstrapping model for automated skin lesion segmentation and classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Xie, J. Zhang, Y. Xia, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Medical Imaging (TMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.03313">arXiv</a><a href="data/bibtex/Xie2020TMIa.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Mutual+Bootstrapping+Model+for+Automated+Skin+Lesion+Segmentation+and+Classification+Xie,+Yutong+and+Zhang,+Jianpeng+and+Xia,+Yong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>SESV: accurate medical image segmentation by predicting and correcting errors</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Xie, J. Zhang, H. Lu, C. Shen, Y. Xia</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Medical Imaging (TMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Xie2020TMIb.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SESV}:+Accurate+Medical+Image+Segmentation+by+Predicting+and+Correcting+Errors+Xie,+Yutong+and+Zhang,+Jianpeng+and+Lu,+Hao+and+Shen,+Chunhua+and+Xia,+Yong">search</a></p>
</li>
<li><p><b>OPMP: an omni-directional pyramid mask proposal network for arbitrary-shape scene text detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Zhang, Y. Liu, L. Jin, Z. Wei, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/ShengZhang2020TMM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={OPMP}:+An+Omni-directional+Pyramid+Mask+Proposal+Network+for+Arbitrary-shape+Scene+Text+Detection+Zhang,+Sheng+and+Liu,+Yuliang+and+Jin,+Lianwen+and+Wei,+Zhongrong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Joint deep learning of facial expression synthesis and recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Yan, Y. Huang, S. Chen, C. Shen, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Yan2020TMM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Joint+deep+learning+of+facial+expression+synthesis+and+recognition+Yan,+Yan+and+Huang,+Ying+and+Chen,+Si+and+Shen,+Chunhua+and+Wang,+Hanzi">search</a></p>
</li>
<li><p><b>Accurate tensor completion via adaptive low-rank representation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Q. Shi, C. Shen, A. van den Hengel, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2020TNNLS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Accurate+Tensor+Completion+via+Adaptive+Low-Rank+Representation+Zhang,+Lei+and+Wei,+Wei+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Zhang,+Yanning">search</a></p>
</li>
<li><p><b>Deep clustering with sample-assignment invariance prior</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Peng, H. Zhu, J. Feng, C. Shen, H. Zhang, J. Zhou</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Peng2020TNNLS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Clustering+with+Sample-Assignment+Invariance+Prior+Peng,+Xi+and+Zhu,+Hongyuan+and+Feng,+Jiashi+and+Shen,+Chunhua+and+Zhang,+Haixian+and+Zhou,+Joey">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1804.03368.pdf"><img class="imgP  right"   src="data/thumbnail/Gong2020TNNLS_arXiv.jpg"></a><b>Learning deep gradient descent optimization for image deconvolution</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>D. Gong, Z. Zhang, Q. Shi, A. van den Hengel, C. Shen, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1804.03368">arXiv</a><a href="data/bibtex/Gong2020TNNLS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Deep+Gradient+Descent+Optimization+for+Image+Deconvolution+Gong,+Dong+and+Zhang,+Zhen+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Zhang,+Yanning">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.07504.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2020TOG_arXiv.jpg"></a><b>Real-time image smoothing via iterative least squares</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>W. Liu, P. Zhang, X. Huang, J. Yang, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>ACM Transactions on Graphics (TOG), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.07504">arXiv</a><a href="https://doi.org/10.1145/3388887">link</a><a href="data/bibtex/Liu2020TOG.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-time+image+smoothing+via+iterative+least+squares+Liu,+Wei+and+Zhang,+Pingping+and+Huang,+Xiaolin+and+Yang,+Jie+and+Shen,+Chunhua+and+Reid,+Ian">search</a><a href="https://github.com/wliusjtu/Real-time-Image-Smoothing-via-Iterative-Least-Squares">project webpage</a></p>
</li>
<li><p><b>Plenty is plague: fine-grained learning for visual question answering</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Zhou, R. Ji, J. Su, X. Sun, D. Meng, Y. Gao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="https://doi.org/10.1109/TPAMI.2019.2956699">link</a><a href="data/bibtex/Zhou2020TPAMIZhou.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Plenty+Is+Plague:+Fine-Grained+Learning+for+Visual+Question+Answering+Zhou,+Yiyi+and+Ji,+Rongrong+and+Su,+Jinsong+and+Sun,+Xiaoshuai+and+Meng,+Deyu+and+Gao,+Yue+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1912.11236.pdf"><img class="imgP  right"   src="data/thumbnail/Zhang2020OrderlessReID_arXiv.jpg"></a><b>Ordered or orderless: a revisit for video based person re-identification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhang, Z. Shi, J. Zhou, M. Cheng, Y. Liu, J. Bian, Z. Zeng, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1912.11236">arXiv</a><a href="https://doi.org/10.1109/TPAMI.2020.2976969">link</a><a href="data/bibtex/Zhang2020OrderlessReID.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Ordered+or+Orderless:+A+Revisit+for+Video+based+Person+Re-Identification+Zhang,+Le+and+Shi,+Zenglin+and+Zhou,+Joey+Tianyi+and+Cheng,+Ming-Ming+and+Liu,+Yun+and+Bian,+Jia-Wang+and+Zeng,+Zeng+and+Shen,+Chunhua">search</a><a href="https://github.com/ZhangLeUestc/VideoReid-TPAMI2020">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1908.09895.pdf"><img class="imgP  right"   src="data/thumbnail/Lu2020PAMIIndexNet_arXiv.jpg"></a><b>Index networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Lu, Y. Dai, C. Shen, S. Xu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.09895">arXiv</a><a href="https://doi.org/10.1109/TPAMI.2020.3004474">link</a><a href="data/bibtex/Lu2020PAMIIndexNet.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Index+Networks+Lu,+Hao+and+Dai,+Yutong+and+Shen,+Chunhua+and+Xu,+Songcen">search</a><a href="https://git.io/IndexNet">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.04197.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2020PAMI_arXiv.jpg"></a><b>Structured knowledge distillation for dense prediction</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Liu, C. Shun, J. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.04197">arXiv</a><a href="https://ieeexplore.ieee.org/document/9115859">link</a><a href="data/bibtex/Liu2020PAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Knowledge+Distillation+for+Dense+Prediction+Liu,+Yifan+and+Shun,+Changyong+and+Wang,+Jingdong+and+Shen,+Chunhua">search</a><a href="https://github.com/irfanICMLL/structure_knowledge_distillation">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1711.00253.pdf"><img class="imgP  right"   src="data/thumbnail/Chen2019PAMI_arXiv.jpg"></a><b>Adversarial learning of structure-aware fully convolutional networks for landmark localization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Chen, C. Shen, H. Chen, X. Wei, L. Liu, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.00253">arXiv</a><a href="https://doi.org/10.1109/TPAMI.2019.2901875">link</a><a href="data/bibtex/Chen2019PAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adversarial+Learning+of+Structure-Aware+Fully+Convolutional+Networks+for+Landmark+Localization+Chen,+Yu+and+Shen,+Chunhua+and+Chen,+Hao+and+Wei,+Xiu-Shen+and+Liu,+Lingqiao+and+Yang,+Jian">search</a></p>
</li>
<li><p><b>Improving generative adversarial networks with local coordinate coding</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Cao, Y. Guo, Q. Wu, C. Shen, J. Huang, M. Tan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2008.00942">arXiv</a><a href="data/bibtex/Cao2020GAN.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Improving+Generative+Adversarial+Networks+with+Local+Coordinate+Coding+Cao,+Jiezhang+and+Guo,+Yong+and+Wu,+Qingyao+and+Shen,+Chunhua+and+Huang,+Junzhou+and+Tan,+Mingkui">search</a><a href="https://github.com/SCUTjinchengli/LCCGAN-v2">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1806.01576.pdf"><img class="imgP  right"   src="data/thumbnail/Adaptive2019Zhang_arXiv.jpg"></a><b>Adaptive importance learning for improving lightweight image super-resolution network</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhang, P. Wang, C. Shen, L. Liu, W. Wei, Y. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1806.01576">arXiv</a><a href="data/bibtex/Adaptive2019Zhang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adaptive+Importance+Learning+for+Improving+Lightweight+Image+Super-resolution+Network+Zhang,+Lei+and+Wang,+Peng+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Wei,+Wei+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton">search</a><a href="https://tinyurl.com/Super-resolution-Network">project webpage</a></p>
</li>
<li><p><b>Accurate imagery recovery using a multi-observation patch model</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Q. Shen, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Information Sciences (IS), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2019Accurate.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Accurate+Imagery+Recovery+Using+a+Multi-Observation+Patch+Model+Zhang,+Lei+and+Wei,+Wei+and+Shen,+Qiang+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Heritage image annotation via collective knowledge</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Zhang, Q. Wu, J. Zhang, C. Shen, J. Lu, Q. Wu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2019PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Heritage+Image+Annotation+via+Collective+Knowledge+Zhang,+Junjie+and+Wu,+Qi+and+Zhang,+Jian+and+Shen,+Chunhua+and+Lu,+Jianfeng+and+Wu,+Qiang">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1611.10080.pdf"><img class="imgP  right"   src="data/thumbnail/Wu2019PR_arXiv.jpg"></a><b>Wider or deeper: revisiting the ResNet model for visual recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Z. Wu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1611.10080">arXiv</a><a href="data/bibtex/Wu2019PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Wider+or+Deeper:+Revisiting+the+{ResNet}+Model+for+Visual+Recognition+Wu,+Zifeng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Order-aware convolutional pooling for video based action recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, L. Liu, C. Shen, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Wang2019PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Order-aware+Convolutional+Pooling+for+Video+Based+Action+Recognition+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Shen,+Heng+Tao">search</a></p>
</li>
<li><p><b>Structural analysis of attributes for vehicle re-identification and retrieval</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Zhao, C. Shen, H. Wang, S. Chen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Zhao2019Structural.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structural+Analysis+of+Attributes+for+Vehicle+Re-identification+and+Retrieval+Zhao,+Yanzhu+and+Shen,+Chunhua+and+Wang,+Huibing+and+Chen,+Shengyong">search</a></p>
</li>
<li><p><b>Human detection aided by deeply learned semantic masks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wang, C. Shen, H. Li, S. Xu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Wangxy2019CSVT.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Human+Detection+Aided+by+Deeply+Learned+Semantic+Masks+Wang,+Xinyu+and+Shen,+Chunhua+and+Li,+Hanxi+and+Xu,+Shugong">search</a></p>
</li>
<li><p><b>Embedding bilateral filter in least squares for efficient edge-preserving image smoothing</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>W. Liu, P. Zhang, X. Chen, C. Shen, X. Huang, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Liu2019CSVT.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Embedding+Bilateral+Filter+in+Least+Squares+for+Efficient+Edge-preserving+Image+Smoothing+Liu,+Wei+and+Zhang,+Pingping+and+Chen,+Xiaogang+and+Shen,+Chunhua+and+Huang,+Xiaolin+and+Yang,+Jie">search</a></p>
</li>
<li><p><b>Counting objects by blockwise classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Liu, H. Lu, H. Xiong, K. Xian, Z. Cao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Counting2019CSVT.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Counting+Objects+by+Blockwise+Classification+Liu,+Liang+and+Lu,+Hao+and+Xiong,+Haipeng+and+Xian,+Ke+and+Cao,+Zhiguo+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Hyperspectral classification based on lightweight 3D-CNN with transfer learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Zhang, Y. Li, Y. Jiang, P. Wang, Q. Shen, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2019Lightweight.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Hyperspectral+Classification+Based+on+Lightweight+{3D-CNN}+With+Transfer+Learning+Zhang,+Haokui+and+Li,+Ying+and+Jiang,+Yenan+and+Wang,+Peng+and+Shen,+Qiang+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Salient object detection with lossless feature reflection and weighted structural loss</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Zhang, W. Liu, H. Lu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2019Salient.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Salient+Object+Detection+with+Lossless+Feature+Reflection+and+Weighted+Structural+Loss+Zhang,+Pingping+and+Liu,+Wei+and+Lu,+Huchuan+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1805.04288.pdf"><img class="imgP  right"   src="data/thumbnail/Wei2019TIP_arXiv.jpg"></a><b>Piecewise classifier mappings: learning fine-grained learners for novel categories with few examples</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wei, P. Wang, L. Liu, C. Shen, J. Wu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1805.04288">arXiv</a><a href="data/bibtex/Wei2019TIP.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Piecewise+classifier+mappings:+Learning+fine-grained+learners+for+novel+categories+with+few+examples+Wei,+Xiu-Shen+and+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Wu,+Jianxin">search</a></p>
</li>
<li><p><b>Multiple instance learning with emerging novel class</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wei, H. Ye, X. Mu, J. Wu, C. Shen, Z. Zhou</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Knowledge and Data Engineering (TKDE), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Wei2019TKDE.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multiple+Instance+Learning+with+Emerging+Novel+Class+Wei,+Xiu-Shen+and+Ye,+Han-Jia+and+Mu,+Xin+and+Wu,+Jianxin+and+Shen,+Chunhua+and+Zhou,+Zhi-Hua">search</a></p>
</li>
<li><p><b>Attention residual learning for skin lesion classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Zhang, Y. Xie, Y. Xia, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Medical Imaging (TMI), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2019Attn.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Attention+residual+learning+for+skin+lesion+classification+Zhang,+Jianpeng+and+Xie,+Yutong+and+Xia,+Yong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1803.02563.pdf"><img class="imgP  right"   src="data/thumbnail/TZhang2019TMM_arXiv.jpg"></a><b>Decoupled spatial neural attention for weakly supervised semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>T. Zhang, G. Lin, J. Cai, T. Shen, C. Shen, A. Kot</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1803.02563">arXiv</a><a href="data/bibtex/TZhang2019TMM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Decoupled+Spatial+Neural+Attention+for+Weakly+Supervised+Semantic+Segmentation+Zhang,+Tianyi+and+Lin,+Guosheng+and+Cai,+Jianfei+and+Shen,+Tong+and+Shen,+Chunhua+and+Kot,+Alex+C.">search</a></p>
</li>
<li><p><b>RefineNet: multi-path refinement networks for dense prediction</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Lin, F. Liu, A. Milan, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="https://doi.org/10.1109/TPAMI.2019.2893630">link</a><a href="data/bibtex/Fayao2019PAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={RefineNet}:+Multi-Path+Refinement+Networks+for+Dense+Prediction+Lin,+Guosheng+and+Liu,+Fayao+and+Milan,+Anton+and+Shen,+Chunhua+and+Reid,+Ian">search</a><a href="https://github.com/guosheng/refinenet">project webpage</a></p>
<ol reversed>
<li><p>Pytorch code is <a href="https://github.com/DrSleep/refinenet-pytorch">here</a>.</p>
</li></ol>
</li>
<li><p><b>Cluster sparsity field: an internal hyperspectral imagery prior for reconstruction</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Y. Zhang, C. Shen, A. van den Hengel, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="https://www.researchgate.net/publication/323914969_Cluster_Sparsity_Field_An_Internal_Hyperspectral_Imagery_Prior_for_Reconstruction">pdf</a><a href="data/bibtex/Zhang2018IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Cluster+Sparsity+Field:+An+Internal+Hyperspectral+Imagery+Prior+for+Reconstruction+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1601.05610.pdf"><img class="imgP  right"   src="data/thumbnail/Li2018IVC_arXiv.jpg"></a><b>Reading car license plates using deep neural networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, P. Wang, M. You, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Image and Vision Computing (IVC), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1601.05610">arXiv</a><a href="data/bibtex/Li2018IVC.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Reading+Car+License+Plates+Using+Deep+Neural+Networks+Li,+Hui+and+Wang,+Peng+and+You,+Mingyu+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1805.01282.pdf"><img class="imgP  right"   src="data/thumbnail/Zhuang2018PR_arXiv.jpg"></a><b>Multi-label learning based deep transfer neural network for facial attribute classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>N. Zhuang, Y. Yan, S. Chen, H. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1805.01282">arXiv</a><a href="data/bibtex/Zhuang2018PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-label+Learning+Based+Deep+Transfer+Neural+Network+for+Facial+Attribute+Classification+Zhuang,+Ni+and+Yan,+Yan+and+Chen,+Si+and+Wang,+Hanzi+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1707.06397.pdf"><img class="imgP  right"   src="data/thumbnail/Wei2018PR_arXiv.jpg"></a><b>Unsupervised object discovery and co-localization by deep descriptor transforming</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wei, C. Zhang, J. Wu, C. Shen, Z. Zhou</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1707.06397">arXiv</a><a href="data/bibtex/Wei2018PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Object+Discovery+and+Co-Localization+by+Deep+Descriptor+Transforming+Wei,+Xiu-Shen+and+Zhang,+Chen-Lin+and+Wu,+Jianxin+and+Shen,+Chunhua+and+Zhou,+Zhi-Hua">search</a></p>
</li>
<li><p><b>An extended filtered channel framework for pedestrian detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>M. You, Y. Zhang, C. Shen, X. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/You2018T-ITS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+extended+filtered+channel+framework+for+pedestrian+detection+You,+Minyu+and+Zhang,+Yubin+and+Shen,+Chunhua+and+Zhang,+Xinyu">search</a></p>
</li>
<li><p><b>Towards end-to-end car license plates detection and recognition with deep neural networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, P. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Li2018T-ITSa.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+End-to-End+Car+License+Plates+Detection+and+Recognition+with+Deep+Neural+Networks+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Unsupervised domain adaptation using robust class-wise matching</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhang, P. Wang, W. Wei, H. Lu, C. Shen, A. van den Hengel, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2018TCSVT.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Domain+Adaptation+Using+Robust+Class-Wise+Matching+Zhang,+Lei+and+Wang,+Peng+and+Wei,+Wei+and+Lu,+Hao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Zhang,+Yanning">search</a></p>
</li>
<li><p><b>Semantics-aware visual object tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>R. Yao, G. Lin, C. Shen, Y. Zhang, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Yao2018TCSVT.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Semantics-Aware+Visual+Object+Tracking+Yao,+Rui+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+Shi,+Qinfeng">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1603.04525.pdf"><img class="imgP  right"   src="data/thumbnail/TCSVT2017Hu_arXiv.jpg"></a><b>Pushing the limits of deep CNNs for pedestrian detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Hu, P. Wang, C. Shen, A. van den Hengel, F. Porikli</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.04525">arXiv</a><a href="data/bibtex/TCSVT2017Hu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Pushing+the+Limits+of+Deep+{CNNs}+for+Pedestrian+Detection+Hu,+Qichang+and+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Porikli,+Fatih">search</a></p>
</li>
<li><p><b>An embarrassingly simple approach to visual domain adaptation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Lu, C. Shen, Z. Cao, Y. Xiao, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Lu2018TIP.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+Embarrassingly+Simple+Approach+to+Visual+Domain+Adaptation+Lu,+Hao+and+Shen,+Chunhua+and+Cao,+Zhiguo+and+Xiao,+Yang+and+{van+den+Hengel},+Anton">search</a><a href="https://github.com/poppinace/ldada">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1612.01082.pdf"><img class="imgP  right"   src="data/thumbnail/Zhang2018TMM_arXiv.jpg"></a><b>Multi-label image classification with regional latent semantic dependencies</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Zhang, Q. Wu, C. Shen, J. Zhang, J. Lu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1612.01082">arXiv</a><a href="data/bibtex/Zhang2018TMM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-Label+Image+Classification+with+Regional+Latent+Semantic+Dependencies+Zhang,+Junjie+and+Wu,+Qi+and+Shen,+Chunhua+and+Zhang,+Jian+and+Lu,+Jianfeng">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1712.09048.pdf"><img class="imgP  right"   src="data/thumbnail/Guo2018TMM_arXiv.jpg"></a><b>Automatic image cropping for visual aesthetic enhancement using deep neural networks and cascaded regression</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Guo, H. Wang, C. Shen, Y. Yan, H. Liao</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1712.09048">arXiv</a><a href="data/bibtex/Guo2018TMM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Automatic+image+cropping+for+visual+aesthetic+enhancement+using+deep+neural+networks+and+cascaded+regression+Guo,+Guanjun+and+Wang,+Hanzi+and+Shen,+Chunhua+and+Yan,+Yan+and+Liao,+Hong-Yuan">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1606.05433.pdf"><img class="imgP  right"   src="data/thumbnail/Wang2017FVQA_arXiv.jpg"></a><b>FVQA: fact-based visual question answering</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, Q. Wu, C. Shen, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1606.05433">arXiv</a><a href="data/bibtex/Wang2017FVQA.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FVQA}:+Fact-based+Visual+Question+Answering+Wang,+Peng+and+Wu,+Qi+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Ordinal constraint binary coding for approximate nearest neighbor search</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Liu, R. Ji, J. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="https://www.researchgate.net/publication/324053386_Ordinal_Constraint_Binary_Coding_for_Approximate_Nearest_Neighbor_Search">pdf</a><a href="data/bibtex/HLiu2018TPAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Ordinal+Constraint+Binary+Coding+for+Approximate+Nearest+Neighbor+Search+Liu,+Hong+and+Ji,+Rongrong+and+Wang,+Jingdong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1602.06654.pdf"><img class="imgP  right"   src="data/thumbnail/IJCV2017Lin_arXiv.jpg"></a><b>Structured learning of binary codes with column generation for optimizing ranking measures</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Lin, F. Liu, C. Shen, J. Wu, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1602.06654">arXiv</a><a href="data/bibtex/IJCV2017Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Learning+of+Binary+Codes+with+Column+Generation+for+Optimizing+Ranking+Measures+Lin,+Guosheng+and+Liu,+Fayao+and+Shen,+Chunhua+and+Wu,+Jianxin+and+Shen,+Heng+Tao">search</a><a href="https://bitbucket.org/guosheng/structhash">project webpage</a></p>
</li>
<li><p><b>Removal of optically thick clouds from high-resolution satellite imagery using dictionary group learning and interdictionary nonlocal joint sparse coding</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Li, W. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (JSTAEORS), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Li2017Removal.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Removal+of+Optically+Thick+Clouds+From+High-resolution+Satellite+Imagery+Using+Dictionary+Group+Learning+and+Interdictionary+Nonlocal+Joint+Sparse+Coding+Li,+Ying+and+Li,+Wenbo+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1707.02290.pdf"><img class="imgP  right"   src="data/thumbnail/Lu2017Counting_arXiv.jpg"></a><b>TasselNet: counting maize tassels in the wild via local counts regression network</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Lu, Z. Cao, Y. Xiao, B. Zhuang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Plant Methods (PLME), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1707.02290">arXiv</a><a href="data/bibtex/Lu2017Counting.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={TasselNet}:+Counting+maize+tassels+in+the+wild+via+local+counts+regression+network+Lu,+Hao+and+Cao,+Zhiguo+and+Xiao,+Yang+and+Zhuang,+Bohan+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1606.01595.pdf"><img class="imgP  right"   src="data/thumbnail/Wu2017PR_arXiv.jpg"></a><b>Deep linear discriminant analysis on Fisher networks: a hybrid architecture for person re-identification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Wu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1606.01595">arXiv</a><a href="data/bibtex/Wu2017PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Linear+Discriminant+Analysis+on+{F}isher+Networks:+A+Hybrid+Architecture+for+Person+Re-identification+Wu,+Lin+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Mask-CNN: localizing parts and selecting descriptors for bird species categorization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wei, C. Xie, J. Wu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Wei2017PR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mask-{CNN}:+Localizing+parts+and+selecting+descriptors+for+bird+species+categorization+Wei,+Xiu-Shen+and+Xie,+Chen-Wei+and+Wu,+Jianxin+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1504.04923.pdf"><img class="imgP  right"   src="data/thumbnail/PR2017Qiao_arXiv.jpg"></a><b>Learning discriminative trajectorylet detector sets for accurate skeleton-based action recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>R. Qiao, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1504.04923">arXiv</a><a href="data/bibtex/PR2017Qiao.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+discriminative+trajectorylet+detector+sets+for+accurate+skeleton-based+action+recognition+Qiao,+Ruizhi+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Deep CNNs with spatially weighted pooling for fine-grained car recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Hu, H. Wang, T. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/SWP2017Hu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+{CNNs}+with+Spatially+Weighted+Pooling+for+Fine-grained+Car+Recognition+Hu,+Qichang+and+Wang,+Huibing+and+Li,+Teng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1604.08660.pdf"><img class="imgP  right"   src="data/thumbnail/TCSVT2017Sheng_arXiv.jpg"></a><b>Crowd counting via weighted VLAD on dense attribute feature maps</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>B. Sheng, C. Shen, G. Lin, J. Li, W. Yang, C. Sun</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1604.08660">arXiv</a><a href="data/bibtex/TCSVT2017Sheng.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Crowd+Counting+via+Weighted+{VLAD}+on+Dense+Attribute+Feature+Maps+Sheng,+Biyun+and+Shen,+Chunhua+and+Lin,+Guosheng+and+Li,+Jun+and+Yang,+Wankou+and+Sun,+Changyin">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1605.02305.pdf"><img class="imgP  right"   src="data/thumbnail/Cao2017_arXiv.jpg"></a><b>Estimating depth from monocular images as classification using deep fully convolutional residual networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Cao, Z. Wu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1605.02305">arXiv</a><a href="data/bibtex/Cao2017.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Estimating+Depth+from+Monocular+Images+as+Classification+Using+Deep+Fully+Convolutional+Residual+Networks+Cao,+Yuanzhouhan+and+Wu,+Zifeng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1601.07649.pdf"><img class="imgP  right"   src="data/thumbnail/TIP2017Liu_arXiv.jpg"></a><b>Discriminative training of deep fully-connected continuous CRF with task-specific loss</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Liu, G. Lin, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1601.07649">arXiv</a><a href="data/bibtex/TIP2017Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Discriminative+Training+of+Deep+Fully-connected+Continuous+{CRF}+with+Task-specific+Loss+Liu,+Fayao+and+Lin,+Guosheng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1610.01706.pdf"><img class="imgP  right"   src="data/thumbnail/TIP2016Cao_arXiv.jpg"></a><b>Exploiting depth from single monocular images for object detection and semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Cao, C. Shen, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1610.01706">arXiv</a><a href="data/bibtex/TIP2016Cao.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploiting+Depth+from+Single+Monocular+Images+for+Object+Detection+and+Semantic+Segmentation+Cao,+Yuanzhouhan+and+Shen,+Chunhua+and+Shen,+Heng+Tao">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1703.08764.pdf"><img class="imgP  right"   src="data/thumbnail/TNNLS2017Liu_arXiv.jpg"></a><b>Structured learning of tree potentials in CRF for image segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Liu, G. Lin, R. Qiao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1703.08764">arXiv</a><a href="data/bibtex/TNNLS2017Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Learning+of+Tree+Potentials+in+{CRF}+for+Image+Segmentation+Liu,+Fayao+and+Lin,+Guosheng+and+Qiao,+Ruizhi+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1603.02814.pdf"><img class="imgP  right"   src="data/thumbnail/Wu2017External_arXiv.jpg"></a><b>Image captioning and visual question answering based on attributes and external knowledge</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Wu, C. Shen, P. Wang, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.02814">arXiv</a><a href="data/bibtex/Wu2017External.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Image+Captioning+and+Visual+Question+Answering+Based+on+Attributes+and+External+Knowledge+Wu,+Qi+and+Shen,+Chunhua+and+Wang,+Peng+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1601.04143.pdf"><img class="imgP  right"   src="data/thumbnail/TPAMI2017Liu_arXiv.jpg"></a><b>Compositional model based Fisher vector coding for image classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Liu, P. Wang, C. Shen, L. Wang, A. van den Hengel, C. Wang, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1601.04143">arXiv</a><a href="data/bibtex/TPAMI2017Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Compositional+Model+based+{F}isher+Vector+Coding+for+Image+Classification+Liu,+Lingqiao+and+Wang,+Peng+and+Shen,+Chunhua+and+Wang,+Lei+and+{van+den+Hengel},+Anton+and+Wang,+Chao+and+Shen,+Heng+Tao">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1510.00921.pdf"><img class="imgP  right"   src="data/thumbnail/Cross2017Liu_arXiv.jpg"></a><b>Cross-convolutional-layer pooling for image recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1510.00921">arXiv</a><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7779086">link</a><a href="data/bibtex/Cross2017Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Cross-convolutional-layer+Pooling+for+Image+Recognition+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1603.03183.pdf"><img class="imgP  right"   src="data/thumbnail/Lin2017Semantic_arXiv.jpg"></a><b>Exploring context with deep structured models for semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Lin, C. Shen, A. van den Hengel, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.03183">arXiv</a><a href="data/bibtex/Lin2017Semantic.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploring+Context+with+Deep+Structured+models+for+Semantic+Segmentation+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Reid,+Ian">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1511.08531.pdf"><img class="imgP  right"   src="data/thumbnail/CVIU2016_arXiv.jpg"></a><b>Structured learning of metric ensembles with application to person re-identification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, L. Wu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Computer Vision and Image Understanding (CVIU), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1511.08531">arXiv</a><a href="data/bibtex/CVIU2016.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+learning+of+metric+ensembles+with+application+to+person+re-identification+Paisitkriangkrai,+Sakrapee+and+Wu,+Lin+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1501.00642.pdf"><img class="imgP  right"   src="data/thumbnail/Zhang2015IJCV_arXiv.jpg"></a><b>Unsupervised feature learning for dense correspondences across scenes</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Zhang, C. Shen, T. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1501.00642">arXiv</a><a href="data/bibtex/Zhang2015IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Feature+Learning+for+Dense+Correspondences+across+Scenes+Zhang,+Chao+and+Shen,+Chunhua+and+Shen,+Tingzhi">search</a><a href="https://bitbucket.org/chhshen/ufl">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1404.5009.pdf"><img class="imgP  right"   src="data/thumbnail/BnB2015Wang_arXiv.jpg"></a><b>Efficient semidefinite branch-and-cut for MAP-MRF inference</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, C. Shen, A. van den Hengel, P. Torr</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1404.5009">arXiv</a><a href="http://doi.org/10.1007/s11263-015-0865-2">link</a><a href="data/bibtex/BnB2015Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Semidefinite+Branch-and-Cut+for+{MAP-MRF}+Inference+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Torr,+Philip">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1506.06343.pdf"><img class="imgP  right"   src="data/thumbnail/Yao2016IJCV_arXiv.jpg"></a><b>Mining mid-level visual patterns with deep CNN activations</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1506.06343">arXiv</a><a href="http://rdcu.be/j1mA">link</a><a href="data/bibtex/Yao2016IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mining+Mid-level+Visual+Patterns+with+Deep+{CNN}+Activations+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a><a href="https://github.com/yaoliUoA/MDPM">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1310.1690.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2016Tracking_arXiv.jpg"></a><b>Online unsupervised feature learning for visual tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Liu, C. Shen, I. Reid, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Image and Vision Computing (IVC), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1310.1690">arXiv</a><a href="data/bibtex/Liu2016Tracking.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Online+Unsupervised+Feature+Learning+for+Visual+Tracking+Liu,+Fayao+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Canonical principal angles correlation analysis for two-view data</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Wang, J. Lu, X. Gu, C. Shen, R. Xia, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Journal of Visual Communication and Image Representation (JVCIR), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1016/j.jvcir.2015.12.001">link</a><a href="data/bibtex/Canonical2016Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Canonical+principal+angles+correlation+analysis+for+two-view+data+Wang,+Sheng+and+Lu,+Jianfeng+and+Gu,+Xingjian+and+Shen,+Chunhua+and+Xia,+Rui+and+Yang,+Jingyu">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1406.6811.pdf"><img class="imgP  right"   src="data/thumbnail/PRFace2016Shen_arXiv.jpg"></a><b>Face image classification by pooling raw features</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Shen, C. Shen, X. Zhou, Y. Yang, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1406.6811">arXiv</a><a href="data/bibtex/PRFace2016Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Face+Image+Classification+by+Pooling+Raw+Features+Shen,+Fumin+and+Shen,+Chunhua+and+Zhou,+Xiang+and+Yang,+Yang+and+Shen,+Heng+Tao">search</a><a href="https://github.com/bd622/FacePooling">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1110.0264.pdf"><img class="imgP  right"   src="data/thumbnail/Face2016Li_arXiv.jpg"></a><b>Face recognition using linear representation ensembles</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, F. Shen, C. Shen, Y. Yang, Y. Gao</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1110.0264">arXiv</a><a href="http://dx.doi.org/10.1016/j.patcog.2015.12.011">link</a><a href="data/bibtex/Face2016Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Face+Recognition+Using+Linear+Representation+Ensembles+Li,+Hanxi+and+Shen,+Fumin+and+Shen,+Chunhua+and+Yang,+Yang+and+Gao,+Yongsheng">search</a></p>
</li>
<li><p><b>Fast detection of multiple objects in traffic scenes with a common detection framework</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Hu, S. Paisitkriangkrai, C. Shen, A. van den Hengel, F. Porikli</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Intelligent Transportation Systems (T-ITS), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Hu2015T-ITS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Detection+of+Multiple+Objects+in+Traffic+Scenes+with+a+Common+Detection+Framework+Hu,+Qichang+and+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Porikli,+Fatih">search</a></p>
</li>
<li><p><b>Part-based robust tracking using online latent structured learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>R. Yao, Q. Shi, C. Shen, Y. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TCSVT.2016.2527358">link</a><a href="data/bibtex/Part2016Yao.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Part-based+robust+tracking+using+online+latent+structured+learning+Yao,+Rui+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1503.01224.pdf"><img class="imgP  right"   src="data/thumbnail/Pooling2016Wang_arXiv.jpg"></a><b>Temporal pyramid pooling based convolutional neural network for action recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, Y. Cao, C. Shen, L. Liu, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1503.01224">arXiv</a><a href="data/bibtex/Pooling2016Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Temporal+Pyramid+Pooling+Based+Convolutional+Neural+Network+for+Action+Recognition+Wang,+Peng+and+Cao,+Yuanzhouhan+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Shen,+Heng+Tao">search</a></p>
</li>
<li><p><b>Dictionary learning for promoting structured sparsity in hyerpsectral compressive sensing</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Y. Zhang, C. Shen, A. van den Hengel, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Zhang2016TGSE.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Dictionary+Learning+for+Promoting+Structured+Sparsity+in+Hyerpsectral+Compressive+Sensing+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng">search</a></p>
</li>
<li><p><b>Scalable linear visual feature learning via online parallel nonnegative matrix factorization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Zhao, X. Li, Z. Zhang, C. Shen, L. Gao, X. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TNNLS.2015.2499273">link</a><a href="data/bibtex/Zhao2015TNN.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Scalable+Linear+Visual+Feature+Learning+via+Online+Parallel+Nonnegative+Matrix+Factorization+Zhao,+Xueyi+and+Li,+Xi+and+Zhang,+Zhongfei+and+Shen,+Chunhua+and+Gao,+Lixin+and+Li,+Xuelong">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.7564.pdf"><img class="imgP  right"   src="data/thumbnail/BQP2015Wang_arXiv.jpg"></a><b>Large-scale binary quadratic optimization using semidefinite relaxation and applications</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, C. Shen, A. van den Hengel, P. Torr</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.7564">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2016.2541146">link</a><a href="data/bibtex/BQP2015Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Large-scale+Binary+Quadratic+Optimization+Using+Semidefinite+Relaxation+and+Applications+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Torr,+Philip+H.+S.">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1409.5209.pdf"><img class="imgP  right"   src="data/thumbnail/Paisitkriangkrai2015TPAMI_arXiv.jpg"></a><b>Pedestrian detection with spatially pooled features and structured ensemble learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1409.5209">arXiv</a><a href="http://doi.org/10.1109/TPAMI.2015.2474388">link</a><a href="data/bibtex/Paisitkriangkrai2015TPAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Pedestrian+Detection+with+Spatially+Pooled+Features+and+Structured+Ensemble+Learning+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a><a href="https://github.com/chhshen/pedestrian-detection">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1401.7713.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2015TPAMI_arXiv.jpg"></a><b>A generalized probabilistic framework for compact codebook creation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Liu, L. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1401.7713">arXiv</a><a href="http://doi.org/10.1109/TPAMI.2015.2441069">link</a><a href="data/bibtex/Liu2015TPAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Generalized+Probabilistic+Framework+for+Compact+Codebook+Creation+Liu,+Lingqiao+and+Wang,+Lei+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1502.07411.pdf"><img class="imgP  right"   src="data/thumbnail/Depth2015Liu_arXiv.jpg"></a><b>Learning depth from single monocular images using deep convolutional neural fields</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Liu, C. Shen, G. Lin, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1502.07411">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2015.2505283">link</a><a href="data/bibtex/Depth2015Liu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Depth+from+Single+Monocular+Images+Using+Deep+Convolutional+Neural+Fields+Liu,+Fayao+and+Shen,+Chunhua+and+Lin,+Guosheng+and+Reid,+Ian">search</a><a href="http://goo.gl/rAKWrS">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1507.05737.pdf"><img class="imgP  right"   src="data/thumbnail/Xi2015TPAMI_arXiv.jpg"></a><b>Online metric-weighted linear representations for robust visual tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Li, C. Shen, A. Dick, Z. Zhang, Y. Zhuang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1507.05737">arXiv</a><a href="data/bibtex/Xi2015TPAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Online+Metric-Weighted+Linear+Representations+for+Robust+Visual+Tracking+Li,+Xi+and+Shen,+Chunhua+and+Dick,+Anthony+and+Zhang,+Zhongfei+and+Zhuang,+Yueting">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1401.8126.pdf"><img class="imgP  right"   src="data/thumbnail/Harandi2015IJCV_arXiv.jpg"></a><b>Extrinsic methods for coding and dictionary learning on Grassmann manifolds</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>M. Harandi, R. Hartley, C. Shen, B. Lovell, C. Sanderson</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1401.8126">arXiv</a><a href="data/bibtex/Harandi2015IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Extrinsic+Methods+for+Coding+and+Dictionary+Learning+on+{G}rassmann+Manifolds+Harandi,+Mehrtash+and+Hartley,+Richard+and+Shen,+Chunhua+and+Lovell,+Brian+and+Sanderson,+Conrad">search</a><a href="https://github.com/chhshen/Grassmann/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1503.08263.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2015CRFPR_arXiv.jpg"></a><b>CRF learning with CNN features for image segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Liu, G. Lin, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1503.08263">arXiv</a><a href="data/bibtex/Liu2015CRFPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={CRF}+Learning+with+{CNN}+Features+for+Image+Segmentation+Liu,+Fayao+and+Lin,+Guosheng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1412.0826.pdf"><img class="imgP  right"   src="data/thumbnail/Hashing2015Shen_arXiv.jpg"></a><b>Hashing on nonlinear manifolds</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Shen, C. Shen, Q. Shi, A. van den Hengel, Z. Tang, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1412.0826">arXiv</a><a href="data/bibtex/Hashing2015Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Hashing+on+Nonlinear+Manifolds+Shen,+Fumin+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin+and+Shen,+Heng+Tao">search</a><a href="https://github.com/chhshen/Hashing-on-Nonlinear-Manifolds">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1409.2104.pdf"><img class="imgP  right"   src="data/thumbnail/TIP2014Shortcut_arXiv.jpg"></a><b>A computational model of the short-cut rule for 2D shape decomposition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Luo, C. Shen, X. Liu, C. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1409.2104">arXiv</a><a href="data/bibtex/TIP2014Shortcut.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Computational+Model+of+the+Short-Cut+Rule+for+{2D}+Shape+Decomposition+Luo,+Lei+and+Shen,+Chunhua+and+Liu,+Xinwang+and+Zhang,+Chunyuan">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.7450.pdf"><img class="imgP  right"   src="data/thumbnail/SDP2015Li_arXiv.jpg"></a><b>Worst-case linear discriminant analysis as scalable semidefinite feasibility problems</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, C. Shen, A. van den Hengel, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.7450">arXiv</a><a href="data/bibtex/SDP2015Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Worst-Case+Linear+Discriminant+Analysis+as+Scalable+Semidefinite+Feasibility+Problems+Li,+Hui+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng">search</a><a href="https://github.com/chhshen/SDP-WLDA">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1408.5574.pdf"><img class="imgP  right"   src="data/thumbnail/FastHash2015Lin_arXiv.jpg"></a><b>Supervised hashing using graph cuts and boosted decision trees</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Lin, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1408.5574">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2015.2404776">link</a><a href="data/bibtex/FastHash2015Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Supervised+Hashing+Using+Graph+Cuts+and+Boosted+Decision+Trees+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a><a href="https://bitbucket.org/chhshen/fasthash/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1304.1250.pdf"><img class="imgP  right"   src="data/thumbnail/Shen2014Outlier_arXiv.jpg"></a><b>Fast approximate <img class="eq" src="eqs/6240687686047976017-130.png" alt="l_infty" style="vertical-align: -4px" /> minimization: Speeding up robust regression</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Shen, C. Shen, R. Hill, A. van den Hengel, Z. Tang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Computational Statistics and Data Analysis (CSDA), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1304.1250">arXiv</a><a href="data/bibtex/Shen2014Outlier.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+approximate+L_\infty+minimization:+{S}peeding+up+robust+regression+Shen,+Fumin+and+Shen,+Chunhua+and+Hill,+Rhys+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1310.0890.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2014MKL_arXiv.jpg"></a><b>Multiple kernel learning in the primal for multi-modal Alzheimer's disease classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Liu, L. Zhou, C. Shen, J. Yin</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Journal of Biomedical and Health Informatics (JBHI), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1310.0890">arXiv</a><a href="http://dx.doi.org/10.1109/JBHI.2013.2285378">link</a><a href="data/bibtex/Liu2014MKL.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multiple+Kernel+Learning+in+the+Primal+for+Multi-modal+{A}lzheimer's+Disease+Classification+Liu,+Fayao+and+Zhou,+Luping+and+Shen,+Chunhua+and+Yin,+Jianping">search</a></p>
<ol reversed>
<li><p>Online published at IEEE: 10 October 2013.</p>
</li></ol>
</li>
<li><p><b>Multiple kernel clustering based on centered kernel alignment</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Lu, L. Wang, J. Lu, J. Yang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/MKL2014.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multiple+kernel+clustering+based+on+centered+kernel+alignment+Lu,+Yanting+and+Wang,+Liantao+and+Lu,+Jianfeng+and+Yang,+Jingyu+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1402.5497.pdf"><img class="imgP  right"   src="data/thumbnail/Yan2014TIPa_arXiv.jpg"></a><b>Efficient semidefinite spectral clustering via Lagrange duality</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Yan, C. Shen, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1402.5497">arXiv</a><a href="data/bibtex/Yan2014TIPa.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Semidefinite+Spectral+Clustering+via+{L}agrange+Duality+Yan,+Yan+and+Shen,+Chunhua+and+Wang,+Hanzi">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1402.6383.pdf"><img class="imgP  right"   src="data/thumbnail/Paul2014TIPb_arXiv.jpg"></a><b>Large-margin learning of compact binary image encodings</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1402.6383">arXiv</a><a href="data/bibtex/Paul2014TIPb.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Large-margin+Learning+of+Compact+Binary+Image+Encodings+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1309.6691.pdf"><img class="imgP  right"   src="data/thumbnail/Li2014TIP_arXiv.jpg"></a><b>Characterness: An indicator of text in the wild</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Li, W. Jia, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1309.6691">arXiv</a><a href="http://dx.doi.org/10.1109/TIP.2014.2302896">link</a><a href="data/bibtex/Li2014TIP.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Characterness:+{A}n+Indicator+of+Text+in+the+Wild+Li,+Yao+and+Jia,+Wenjing+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a><a href="https://github.com/yaoliUoA/characterness">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1401.0764.pdf"><img class="imgP  right"   src="data/thumbnail/Li2013Hyper_arXiv.jpg"></a><b>Context-aware hypergraph construction for robust spectral clustering</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Li, W. Hu, C. Shen, A. Dick, Z. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Knowledge and Data Engineering (TKDE), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1401.0764">arXiv</a><a href="http://doi.ieeecomputersociety.org/10.1109/TKDE.2013.126">link</a><a href="data/bibtex/Li2013Hyper.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Context-aware+hypergraph+construction+for+robust+spectral+clustering+Li,+Xi+and+Hu,+Weiming+and+Shen,+Chunhua+and+Dick,+Anthony+and+Zhang,+Zhongfei">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1303.6066.pdf"><img class="imgP  right"   src="data/thumbnail/Paul2013TMM_arXiv.jpg"></a><b>Asymmetric pruning for learning cascade detectors</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1303.6066">arXiv</a><a href="http://dx.doi.org/10.1109/TMM.2014.2308723">link</a><a href="data/bibtex/Paul2013TMM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Asymmetric+pruning+for+learning+cascade+detectors+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1302.3219.pdf"><img class="imgP  right"   src="data/thumbnail/Shen2014Metric_arXiv.jpg"></a><b>Efficient dual approach to distance metric learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, J. Kim, F. Liu, L. Wang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1302.3219">arXiv</a><a href="data/bibtex/Shen2014Metric.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Dual+Approach+to+Distance+Metric+Learning+Shen,+Chunhua+and+Kim,+Junae+and+Liu,+Fayao+and+Wang,+Lei+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Paul2013Fastboosting_PDF.jpg"><b>A scalable stage-wise approach to large-margin multi-class loss based boosting</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1307.5497">arXiv</a><a href="http://dx.doi.org/10.1109/TNNLS.2013.2282369">link</a><a href="https://bytebucket.org/chhshen/data/raw/7e2f958b104603e54e9d8376a8e1672363f742a3/papers/Paisitkriangkrai2014TNNLS.pdf">pdf</a><a href="data/bibtex/Paul2013Fastboosting.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+scalable+stage-wise+approach+to+large-margin+multi-class+loss+based+boosting+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1302.0963.pdf"><img class="imgP  right"   src="data/thumbnail/Paisitkriangkrai2013RandomBoost_arXiv.jpg"></a><b>RandomBoost: Simplified multi-class boosting through randomization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, Q. Shi, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1302.0963">arXiv</a><a href="http://dx.doi.org/10.1109/TNNLS.2013.2281214">link</a><a href="data/bibtex/Paisitkriangkrai2013RandomBoost.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={RandomBoost}:+{S}implified+Multi-class+Boosting+through+Randomization+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>A hierarchical word-merging algorithm with class separability measure</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Wang, L. Zhou, C. Shen, L. Liu, H. Liu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="https://bitbucket.org/chhshen/chhshen.bitbucket.org/src/be12d4ef8deb6207ec97f0fdac6efbe2df151b59/_download/TPAMI14Wang.pdf">pdf</a><a href="data/bibtex/Wang2014PAMI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Hierarchical+Word-merging+Algorithm+with+Class+Separability+Measure+Wang,+Lei+and+Zhou,+Luping+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Liu,+Huan">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2014SBoosting_PDF.jpg"><b>StructBoost: Boosting methods for predicting structured output variables</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, G. Lin, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1302.3283">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2014.2315792">link</a><a href="http://goo.gl/goCVLK">pdf</a><a href="data/bibtex/Shen2014SBoosting.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={StructBoost}:+{B}oosting+Methods+for+Predicting+Structured+Output+Variables+Shen,+Chunhua+and+Lin,+Guosheng+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1301.2032.pdf"><img class="imgP  right"   src="data/thumbnail/FisherBoost2013IJCV_arXiv.jpg"></a><b>Training effective node classifiers for cascade classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, P. Wang, S. Paisitkriangkrai, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>International Journal of Computer Vision (IJCV), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1301.2032">arXiv</a><a href="http://link.springer.com/article/10.1007%2Fs11263-013-0608-1">link</a><a href="data/bibtex/FisherBoost2013IJCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Training+Effective+Node+Classifiers+for+Cascade+Classification+Shen,+Chunhua+and+Wang,+Peng+and+Paisitkriangkrai,+Sakrapee+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Fully corrective boosting with arbitrary loss and regularization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, H. Li, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Neural Networks (NN), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/78929">pdf</a><a href="data/bibtex/Shen2013NN.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fully+Corrective+Boosting+with+Arbitrary+Loss+and+Regularization+Shen,+Chunhua+and+Li,+Hanxi+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Approximate least trimmed sum of squares fitting and applications in image analysis</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Shen, C. Shen, A. van den Hengel, Z. Tang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6408142">link</a><a href="http://hdl.handle.net/2440/79428">pdf</a><a href="data/bibtex/LMS2013TIP.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Approximate+Least+Trimmed+Sum+of+Squares+Fitting+and+Applications+in+Image+Analysis+Shen,+Fumin+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin">search</a></p>
</li>
<li><p><b>Visual tracking with spatio-temporal Dempster-Shafer information fusion</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Li, A. Dick, C. Shen, Z. Zhang, A. van den Hengel, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/77448">pdf</a><a href="data/bibtex/Xi2013TIP.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+Tracking+with+Spatio-Temporal+{Dempster-Shafer}+Information+Fusion+Li,+Xi+and+Dick,+Anthony+and+Shen,+Chunhua+and+Zhang,+Zhongfei+and+{van+den+Hengel},+Anton+and+Wang,+Hanzi">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1303.4803.pdf"><img class="imgP  right"   src="data/thumbnail/Xi2013Survey_arXiv.jpg"></a><b>A survey of appearance models in visual object tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Li, W. Hu, C. Shen, Z. Zhang, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>ACM Transactions on Intelligent Systems and Technology (TIST), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1303.4803">arXiv</a><a href="data/bibtex/Xi2013Survey.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Survey+of+Appearance+Models+in+Visual+Object+Tracking+Li,+Xi+and+Hu,+Weiming+and+Shen,+Chunhua+and+Zhang,+Zhongfei+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Shape similarity analysis by self-tuning locally constrained mixed-diffusion</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Luo, C. Shen, C. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Multimedia (TMM), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/73304">pdf</a><a href="data/bibtex/TMM2013Shape.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Shape+Similarity+Analysis+by+Self-Tuning+Locally+Constrained+Mixed-Diffusion+Luo,+Lei+and+Shen,+Chunhua+and+Zhang,+Chunyuan+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/TPAMI2013Xi_PDF.jpg"><b>Incremental learning of 3D-DCT compact representations for robust visual tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Li, A. Dick, C. Shen, A. van den Hengel, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1207.3389">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2012.166">link</a><a href="https://sites.google.com/site/chhshen/publication/tpami12xi.pdf?attredirects=1">pdf</a><a href="data/bibtex/TPAMI2013Xi.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Incremental+Learning+of+{3D-DCT}+Compact+Representations+for+Robust+Visual+Tracking+Li,+Xi+and+Dick,+Anthony+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Wang,+Hanzi">search</a><a href="https://github.com/chhshen/DCT-Tracking/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1104.4704.pdf"><img class="imgP  right"   src="data/thumbnail/JMLR2012Shen_arXiv.jpg"></a><b>Positive semidefinite metric learning using boosting-like algorithms</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, J. Kim, L. Wang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Journal of Machine Learning Research (JMLR), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1104.4704">arXiv</a><a href="http://jmlr.csail.mit.edu/papers/v13/shen12a.html">link</a><a href="data/bibtex/JMLR2012Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Positive+Semidefinite+Metric+Learning+Using+Boosting-like+Algorithms+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei+and+{van+den+Hengel},+Anton">search</a><a href="https://bitbucket.org/chhshen/data/raw/45d101372013763d18f0a7ed191c86569532ed96/code/BoostMetric-NeurIPS09-codes-V0.1.tar.bz2">project webpage</a></p>
</li>
<li><p><b>Fast and robust object detection using asymmetric totally-corrective boosting</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, C. Shen, N. Barnes, H. Zheng</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks and Learning Systems (TNN), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/66763">pdf</a><a href="data/bibtex/AsymBoost2011Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+and+Robust+Object+Detection+Using+Asymmetric+Totally-corrective+Boosting+Wang,+Peng+and+Shen,+Chunhua+and+Barnes,+Nick+and+Zheng,+Hong">search</a></p>
</li>
<li><p><b>UBoost: Boosting with the Universum</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, P. Wang, F. Shen, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/67027">pdf</a><a href="data/bibtex/UBoost2011Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={UBoost}:+{B}oosting+with+the+{U}niversum+Shen,+Chunhua+and+Wang,+Peng+and+Shen,+Fumin+and+Wang,+Hanzi">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/0903.3103.pdf"><img class="imgP  right"   src="data/thumbnail/GSLDA2010Shen_arXiv.jpg"></a><b>Efficiently learning a detection cascade with sparse eigenvectors</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, S. Paisitkriangkrai, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0903.3103">arXiv</a><a href="http://dx.doi.org/10.1109/TIP.2010.2055880">link</a><a href="data/bibtex/GSLDA2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficiently+Learning+a+Detection+Cascade+with+Sparse+Eigenvectors+Shen,+Chunhua+and+Paisitkriangkrai,+Sakrapee+and+Zhang,+Jian">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1005.4118.pdf"><img class="imgP  right"   src="data/thumbnail/Incremental2010Shen_arXiv.jpg"></a><b>Incremental training of a detector using online sparse eigen-decomposition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1005.4118">arXiv</a><a href="http://dx.doi.org/10.1109/TIP.2010.2053548">link</a><a href="data/bibtex/Incremental2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Incremental+Training+of+a+Detector+Using+Online+Sparse+Eigen-decomposition+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Li2010Interactive_PDF.jpg"><b>Interactive color image segmentation with linear programming</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Machine Vision and Applications (MVA), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://www.springerlink.com/content/b254775776114226">link</a><a href="http://sites.google.com/site/chhshen/publication/MVA2010LP.pdf">pdf</a><a href="data/bibtex/Li2010Interactive.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Interactive+Color+Image+Segmentation+with+Linear+Programming+Li,+Hongdong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Generalized2010Shen_PDF.jpg"><b>Generalized kernel-based visual tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, J. Kim, H. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0905.2463">arXiv</a><a href="http://dx.doi.org/10.1109/TCSVT.2009.2031393">link</a><a href="http://sites.google.com/site/chhshen/publication/TCSVT2010.pdf">pdf</a><a href="data/bibtex/Generalized2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Generalized+Kernel-based+Visual+Tracking+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Hanzi">search</a><a href="https://github.com/chhshen/KernelTracking">project webpage</a></p>
</li>
<li><p><b>Feature selection with redundancy-constrained class separability</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhou, L. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks (TNN), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TNN.2010.2044189">link</a><a href="data/bibtex/Zhou2010FS.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Feature+Selection+With+Redundancy-Constrained+Class+Separability+Zhou,+Luping+and+Wang,+Lei+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/0904.2037.pdf"><img class="imgP  right"   src="data/thumbnail/MDBoost2010Shen_arXiv.jpg"></a><b>Boosting through optimization of margin distributions</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, H. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks (TNN), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0904.2037">arXiv</a><a href="http://dx.doi.org/10.1109/TNN.2010.2040484">link</a><a href="data/bibtex/MDBoost2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Boosting+through+optimization+of+margin+distributions+Shen,+Chunhua+and+Li,+Hanxi">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1003.0487.pdf"><img class="imgP  right"   src="data/thumbnail/Scalable2010Shen_arXiv.jpg"></a><b>Scalable large-margin Mahalanobis distance metric learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, J. Kim, L. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Neural Networks (TNN), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1003.0487">arXiv</a><a href="http://dx.doi.org/10.1109/TNN.2010.2052630">link</a><a href="data/bibtex/Scalable2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Scalable+Large-Margin+{M}ahalanobis+Distance+Metric+Learning+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/0901.3590.pdf"><img class="imgP  right"   src="data/thumbnail/Dual2010Shen_arXiv.jpg"></a><b>On the dual formulation of boosting algorithms</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, H. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0901.3590">arXiv</a><a href="http://dx.doi.org/10.1109/TPAMI.2010.47">link</a><a href="data/bibtex/Dual2010Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=On+the+Dual+Formulation+of+Boosting+Algorithms+Shen,+Chunhua+and+Li,+Hanxi">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Performance2008Paul_PDF.jpg"><b>Performance evaluation of local features in human classification and detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IET Computer Vision (IETCV), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1049/iet-cvi:20080026">link</a><a href="http://sites.google.com/site/chhshen/publication/Huam2009IET.pdf">pdf</a><a href="data/bibtex/Performance2008Paul.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Performance+Evaluation+of+Local+Features+in+Human+Classification+and+Detection+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian">search</a></p>
<ol reversed>
<li><p>Invited submission, special issue of DICTA2007.</p>
</li></ol>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/SDP2008Shen_PDF.jpg"><b>Supervised dimensionality reduction via sequential semidefinite programming</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, H. Li, M. Brooks</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Pattern Recognition (PR), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1016/j.patcog.2008.06.015">link</a><a href="http://sites.google.com/site/chhshen/publication/PR1.pdf">pdf</a><a href="data/bibtex/SDP2008Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Supervised+Dimensionality+Reduction+via+Sequential+Semidefinite+Programming+Shen,+Chunhua+and+Li,+Hongdong+and+Brooks,+Michael+J.">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Human2008Paul_PDF.jpg"><b>Fast pedestrian detection using a cascade of boosted covariance features</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TCSVT.2008.928213">link</a><a href="http://goo.gl/lgpDJB">pdf</a><a href="data/bibtex/Human2008Paul.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Pedestrian+Detection+Using+a+Cascade+of+Boosted+Covariance+Features+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Fast2007Shen_PDF.jpg"><b>Fast global kernel density mode seeking: applications to localization and tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, M. Brooks, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Image Processing (TIP), 2007</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TIP.2007.894233">link</a><a href="http://sites.google.com/site/chhshen/publication/TIP2007Shen.pdf">pdf</a><a href="data/bibtex/Fast2007Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+global+kernel+density+mode+seeking:+applications+to+localization+and+tracking+Shen,+Chunhua+and+Brooks,+Michael+J.+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Adaptive object tracking based on an effective appearance filter</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Wang, D. Suter, K. Schindler, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2007</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/TPAMI.2007.1112">link</a><a href="http://goo.gl/6rQTA1">pdf</a><a href="data/bibtex/Adaptive2007Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adaptive+object+tracking+based+on+an+effective+appearance+filter+Wang,+Hanzi+and+Suter,+David+and+Schindler,+Konrad+and+Shen,+Chunhua">search</a></p>
<ol reversed>
<li><p>Featured article of September issue 2007.</p>
</li></ol>
</li>
<li><p><b>Active control of radiation from a piston set in a rigid sphere</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Z. Lin, J. Lu, C. Shen, X. Qiu, B. Xu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Journal of Acoustical Society of America (JASA), 2004</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1121/1.1736654">link</a><a href="http://goo.gl/nc4SjU">pdf</a><a href="data/bibtex/Lin2004Active.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Active+control+of+radiation+from+a+piston+set+in+a+rigid+sphere+Lin,+Zhibin+and+Lu,+Jing+and+Shen,+Chunhua+and+Qiu,+Xiaojun+and+Xu,+Boling">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Lu2003Lattice_PDF.jpg"><b>Lattice form adaptive infinite impulse response filtering algorithm for active noise control</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Lu, C. Shen, X. Qiu, B. Xu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Journal of Acoustical Society of America (JASA), 2003</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1121/1.1529665">link</a><a href="http://sites.google.com/site/chhshen/publication/Lattice2003JASA.pdf?attredirects=1">pdf</a><a href="data/bibtex/Lu2003Lattice.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Lattice+form+adaptive+infinite+impulse+response+filtering+algorithm+for+active+noise+control+Lu,+Jing+and+Shen,+Chunhua+and+Qiu,+Xiaojun+and+Xu,+Boling">search</a></p>
</li>
</ol>
<h2>Conference</h2>
<ol reversed>
<li><p><b>Diverse knowledge distillation for end-to-end person search</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Zhang, X. Wang, J. Bian, C. Shen, M. You</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/ZhangPerson2021AAAI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Diverse+Knowledge+Distillation+for+End-to-end+Person+Search+Zhang,+Xinyu+and+Wang,+Xinlong+and+Bian,+Jia-Wang+and+Shen,+Chunhua+and+You,+Minyu">search</a></p>
</li>
<li><p><b>SA-BNN: state-aware binary neural network</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Liu, P. Chen, B. Zhuang, C. Shen, B. Zhang, W. Ding</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;21), 2021</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/LiuBNN2021AAAI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SA-BNN}:+State-Aware+Binary+Neural+Network+Liu,+Chunlei+and+Chen,+Peng+and+Zhuang,+Bohan+and+Shen,+Chunhua+and+Zhang,+Baochang+and+Ding,+Wenrui">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1909.07701.pdf"><img class="imgP  right"   src="data/thumbnail/AAAI20Wang_arXiv.jpg"></a><b>Task-aware monocular depth estimation for 3D object detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wang, W. Yin, T. Kong, Y. Jiang, L. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1909.07701">arXiv</a><a href="data/bibtex/AAAI20Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Task-Aware+Monocular+Depth+Estimation+for+{3D}+Object+Detection+Wang,+Xinlong+and+Yin,+Wei+and+Kong,+Tao+and+Jiang,+Yuning+and+Li,+Lei+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1907.12271.pdf"><img class="imgP  right"   src="data/thumbnail/AAAI20Teney_arXiv.jpg"></a><b>V-PROM: a benchmark for visual reasoning using visual progressive matrices</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>D. Teney, P. Wang, J. Cao, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1907.12271">arXiv</a><a href="data/bibtex/AAAI20Teney.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={V-PROM}:+A+Benchmark+for+Visual+Reasoning+Using+Visual+Progressive+Matrices+Teney,+Damien+and+Wang,+Peng+and+Cao,+Jiewei+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.11236.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Zhuang5_arXiv.jpg"></a><b>Training quantized neural networks with a full-precision auxiliary module</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>B. Zhuang, L. Liu, M. Tan, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.11236">arXiv</a><a href="data/bibtex/CVPR2020Zhuang5.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Training+Quantized+Neural+Networks+with+a+Full-precision+Auxiliary+Module+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Tan,+Mingkui+and+Shen,+Chunhua+and+Reid,+Ian">search</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.11712.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Zhang3_arXiv.jpg"></a><b>Mask encoding for single shot instance segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>R. Zhang, Z. Tian, C. Shen, M. You, Y. Yan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.11712">arXiv</a><a href="data/bibtex/CVPR2020Zhang3.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mask+Encoding+for+Single+Shot+Instance+Segmentation+Zhang,+Rufeng+and+Tian,+Zhi+and+Shen,+Chunhua+and+You,+Mingyu+and+Yan,+Youliang">search</a><a href="https://github.com/aim-uofa/AdelaiDet/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1909.08228.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020NAS11_arXiv.jpg"></a><b>Memory-efficient hierarchical neural architecture search for image denoising</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Zhang, Y. Li, H. Chen, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1909.08228">arXiv</a><a href="data/bibtex/CVPR2020NAS11.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Memory-Efficient+Hierarchical+Neural+Architecture+Search+for+Image+Denoising+Zhang,+Haokui+and+Li,+Ying+and+Chen,+Hao+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.06777.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020EMD9_arXiv.jpg"></a><b>DeepEMD: few-shot image classification with differentiable earth mover's distance and structured classifiers</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Zhang, Y. Cai, G. Lin, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.06777">arXiv</a><a href="data/bibtex/CVPR2020EMD9.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={DeepEMD}:+Few-Shot+Image+Classification+with+Differentiable+Earth+Mover's+Distance+and+Structured+Classifiers+Zhang,+Chi+and+Cai,+Yujun+and+Lin,+Guosheng+and+Shen,+Chunhua">search</a><a href="https://github.com/icoz69/DeepEMD">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2004.01547.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Yu2_arXiv.jpg"></a><b>Context prior for scene segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Yu, J. Wang, C. Gao, G. Yu, C. Shen, N. Sang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2004.01547">arXiv</a><a href="data/bibtex/CVPR2020Yu2.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Context+Prior+for+Scene+Segmentation+Yu,+Changqian+and+Wang,+Jingbo+and+Gao,+Changxin+and+Yu,+Gang+and+Shen,+Chunhua+and+Sang,+Nong">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1909.13226.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Xie8_arXiv.jpg"></a><b>PolarMask: single shot instance segmentation with polar representation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>E. Xie, P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen, P. Luo</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1909.13226">arXiv</a><a href="data/bibtex/CVPR2020Xie8.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={PolarMask}:+Single+Shot+Instance+Segmentation+with+Polar+Representation+Xie,+Enze+and+Sun,+Peize+and+Song,+Xiaoge+and+Wang,+Wenhai+and+Liu,+Xuebo+and+Liang,+Ding+and+Shen,+Chunhua+and+Luo,+Ping">search</a><a href="https://github.com/xieenze/PolarMask">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2002.10215.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Wang4_arXiv.jpg"></a><b>On the general value of evidence, and bilingual scene-text visual question answering</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wang, Y. Liu, C. Shen, C. Ng, C. Luo, L. Jin, C. Chan, A. van den Hengel, L. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2002.10215">arXiv</a><a href="data/bibtex/CVPR2020Wang4.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=On+the+General+Value+of+Evidence,+and+Bilingual+Scene-Text+Visual+Question+Answering+Wang,+Xinyu+and+Liu,+Yuliang+and+Shen,+Chunhua+and+Ng,+Chun+Chet+and+Luo,+Canjie+and+Jin,+Lianwen+and+Chan,+Chee+Seng+and+{van+den+Hengel},+Anton+and+Wang,+Liangwei">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1906.04423.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020NASFCOS10_arXiv.jpg"></a><b>NAS-FCOS: fast neural architecture search for object detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1906.04423">arXiv</a><a href="data/bibtex/CVPR2020NASFCOS10.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={NAS-FCOS}:+Fast+Neural+Architecture+Search+for+Object+Detection+Wang,+Ning+and+Gao,+Yang+and+Chen,+Hao+and+Wang,+Peng+and+Tian,+Zhi+and+Shen,+Chunhua+and+Zhang,+Yanning">search</a><a href="https://github.com/Lausannen/NAS-FCOS">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1904.10151.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020REVERIE1_arXiv.jpg"></a><b>REVERIE: remote embodied visual referring expression in real indoor environments</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Wang, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1904.10151">arXiv</a><a href="data/bibtex/CVPR2020REVERIE1.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={REVERIE}:+Remote+Embodied+Visual+Referring+Expression+in+Real+Indoor+Environments+Qi,+Yuankai+and+Wu,+Qi+and+Anderson,+Peter+and+Wang,+Xin+and+Wang,+William+Yang+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.06780.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Self12_arXiv.jpg"></a><b>Self-trained deep ordinal regression for end-to-end video anomaly detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Pang, C. Yan, C. Shen, A. van den Hengel, X. Bai</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.06780">arXiv</a><a href="data/bibtex/CVPR2020Self12.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Self-trained+Deep+Ordinal+Regression+for+End-to-End+Video+Anomaly+Detection+Pang,+Guansong+and+Yan,+Cheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Bai,+Xiao">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2002.10200.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020Liu6_arXiv.jpg"></a><b>ABCNet: arbitrarily-shaped scene text spotting with adaptive Bezier-curve network in real time</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Liu, H. Chen, C. Shen, T. He, L. Jin, L. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2002.10200">arXiv</a><a href="data/bibtex/CVPR2020Liu6.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={ABCNet}:+Arbitrarily-Shaped+Scene+Text+Spotting+with+Adaptive+{B}ezier-Curve+Network+in+Real+Time+Liu,+Yuliang+and+Chen,+Hao+and+Shen,+Chunhua+and+He,+Tong+and+Jin,+Lianwen+and+Wang,+Liangwei">search</a><a href="https://github.com/aim-uofa/AdelaiDet/">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2001.00309.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2020BlendMask7_arXiv.jpg"></a><b>BlendMask: top-down meets bottom-up for instance segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Chen, K. Sun, Z. Tian, C. Shen, Y. Huang, Y. Yan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2001.00309">arXiv</a><a href="data/bibtex/CVPR2020BlendMask7.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={BlendMask}:+Top-Down+Meets+Bottom-Up+for+Instance+Segmentation+Chen,+Hao+and+Sun,+Kunyang+and+Tian,+Zhi+and+Shen,+Chunhua+and+Huang,+Yongming+and+Yan,+Youliang">search</a><a href="https://github.com/aim-uofa/AdelaiDet/">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><b>Representative graph neural network</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Yu, Y. Liu, C. Gao, C. Shen, N. Sang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2008.05202">arXiv</a><a href="data/bibtex/Yu2020RepGraphNet.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Representative+Graph+Neural+Network+Yu,+Changqian+and+Liu,+Yifan+and+Gao,+Changxin+and+Shen,+Chunhua+and+Sang,+Nong">search</a><a href="https://github.com/ycszen/RepGraph">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.13948.pdf"><img class="imgP  right"   src="data/thumbnail/Xie2020Segtransp_arXiv.jpg"></a><b>Segmenting transparent objects in the wild</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>E. Xie, W. Wang, W. Wang, M. Ding, C. Shen, P. Luo</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.13948">arXiv</a><a href="data/bibtex/Xie2020Segtransp.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Segmenting+Transparent+Objects+in+the+Wild+Xie,+Enze+and+Wang,+Wenjia+and+Wang,+Wenhai+and+Ding,+Mingyu+and+Shen,+Chunhua+and+Luo,+Ping">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1912.04488.pdf"><img class="imgP  right"   src="data/thumbnail/Wang2020SOLO_arXiv.jpg"></a><b>SOLO: segmenting objects by locations</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wang, T. Kong, C. Shen, Y. Jiang, L. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1912.04488">arXiv</a><a href="data/bibtex/Wang2020SOLO.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SOLO}:+Segmenting+Objects+by+Locations+Wang,+Xinlong+and+Kong,+Tao+and+Shen,+Chunhua+and+Jiang,+Yuning+and+Li,+Lei">search</a><a href="https://github.com/aim-uofa/adet">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2005.03341.pdf"><img class="imgP  right"   src="data/thumbnail/Wang2020SuperRes_arXiv.jpg"></a><b>Scene text image super-resolution in the wild</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>W. Wang, E. Xie, X. Liu, W. Wang, D. Liang, C. Shen, X. Bai</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2005.03341">arXiv</a><a href="data/bibtex/Wang2020SuperRes.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Scene+Text+Image+Super-Resolution+in+the+Wild+Wang,+Wenjia+and+Xie,+Enze+and+Liu,+Xuebo+and+Wang,+Wenhai+and+Liang,+Ding+and+Shen,+Chunhua+and+Bai,+Xiang">search</a></p>
</li>
<li><p><b>AE TextSpotter: learning visual and linguistic representation for ambiguous text spotting</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>W. Wang, X. Liu, X. Ji, E. Xie, D. Liang, Z. Yang, T. Lu, C. Shen, P. Luo</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Wang2020AET.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={AE+TextSpotter}:+Learning+Visual+and+Linguistic+Representation+for+Ambiguous+Text+Spotting+Wang,+Wenhai+and+Liu,+Xuebo+and+Ji,+Xiaozhong+and+Xie,+Enze+and+Liang,+Ding+and+Yang,+ZhiBo+and+Lu,+Tong+and+Shen,+Chunhua+and+Luo,+Ping">search</a></p>
</li>
<li><p><b>Soft expert reward learning for vision-and-language navigation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Wang, Q. Wu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Wang2020Soft.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Soft+Expert+Reward+Learning+for+Vision-and-Language+Navigation+Wang,+Hu+and+Wu,+Qi+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2003.05664.pdf"><img class="imgP  right"   src="data/thumbnail/Tian2020CondInst_arXiv.jpg"></a><b>Conditional convolutions for instance segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Z. Tian, C. Shen, H. Chen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.05664">arXiv</a><a href="data/bibtex/Tian2020CondInst.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Conditional+Convolutions+for+Instance+Segmentation+Tian,+Zhi+and+Shen,+Chunhua+and+Chen,+Hao">search</a><a href="https://github.com/aim-uofa/adet">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/2002.11433.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2020EfficientSemantic_arXiv.jpg"></a><b>Efficient semantic video segmentation with per-frame inference</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Liu, C. Shen, C. Yu, J. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2002.11433">arXiv</a><a href="data/bibtex/Liu2020EfficientSemantic.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+Semantic+Video+Segmentation+with+Per-frame+Inference+Liu,+Yifan+and+Shen,+Chunhua+and+Yu,+Changqian+and+Wang,+Jingdong">search</a><a href="https://tinyurl.com/segment-video">project webpage</a></p>
</li>
<li><p><b>Weighing counts: sequential crowd counting by reinforcement learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Liu, H. Lu, H. Zou, H. Xiong, Z. Cao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2007.08260">arXiv</a><a href="data/bibtex/Liu2020WeightingRL.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Weighing+Counts:+Sequential+Crowd+Counting+by+Reinforcement+Learning+Liu,+Liang+and+Lu,+Hao+and+Zou,+Hongwei+and+Xiong,+Haipeng+and+Cao,+Zhiguo+and+Shen,+Chunhua">search</a><a href="https://github.com/poppinace/libranet">project webpage</a></p>
</li>
<li><p><b>Instance-aware embedding for point cloud instance segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>T. He, Y. Liu, C. Shen, X. Wang, C. Sun</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/He2020InstanceAware.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Instance-Aware+Embedding+for+Point+Cloud+Instance+Segmentation+He,+Tong+and+Liu,+Yifan+and+Shen,+Chunhua+and+Wang,+Xinlong+and+Sun,+Changming">search</a></p>
</li>
<li><p><b>Learning and memorizing representative prototypes for 3D point cloud semantic and instance segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>T. He, D. Gong, Z. Tian, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/He2020PC1.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+and+Memorizing+Representative+Prototypes+for+{3D}+Point+Cloud+Semantic+and+Instance+Segmentation+He,+Tong+and+Gong,+Dong+and+Tian,+Zhi+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1912.12186.pdf"><img class="imgP  right"   src="data/thumbnail/Wang2020IJCAI_arXiv.jpg"></a><b>Unsupervised representation learning by predicting random distances</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Wang, G. Pang, C. Shen, C. Ma</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Joint Conferences on Artificial Intelligence (IJCAI&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1912.12186">arXiv</a><a href="data/bibtex/Wang2020IJCAI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Representation+Learning+by+Predicting+Random+Distances+Wang,+Hu+and+Pang,+Guansong+and+Shen,+Chunhua+and+Ma,+Congbo">search</a></p>
</li>
<li><p><b>Pairwise relation learning for semi-supervised gland segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Xie, J. Zhang, Z. Liao, C. Shen, J. Verjans, Y. Xia</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/YXie2020MICCAI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Pairwise+Relation+Learning+for+Semi-supervised+Gland+Segmentation+Xie,+Yutong+and+Zhang,+Jianpeng+and+Liao,+Zhibin+and+Shen,+Chunhua+and+Verjans,+Johan+and+Xia,+Yong">search</a></p>
</li>
<li><p><b>SOLOv2: dynamic and fast instance segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wang, R. Zhang, T. Kong, L. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;20), 2020</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/2003.10152">arXiv</a><a href="data/bibtex/SoloV22020.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={SOLOv2}:+Dynamic+and+Fast+Instance+Segmentation+Wang,+Xinlong+and+Zhang,+Rufeng+and+Kong,+Tao+and+Li,+Lei+and+Shen,+Chunhua">search</a><a href="https://git.io/AdelaiDet">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1811.00751.pdf"><img class="imgP  right"   src="data/thumbnail/AAAI19Li_arXiv.jpg"></a><b>Show, attend and read: a simple and strong baseline for irregular text recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, P. Wang, C. Shen, G. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1811.00751">arXiv</a><a href="data/bibtex/AAAI19Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Show,+attend+and+read:+a+simple+and+strong+baseline+for+irregular+text+recognition+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua+and+Zhang,+Guyu">search</a></p>
</li>
<li><p><b>Deep hashing by discriminating hard examples</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Yan, G. Pang, X. Bai, C. Shen, J. Zhou, E. Hancock</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. ACM International Conference on Multimedia (ACMMM&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/MM2019Yan.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Hashing+by+Discriminating+Hard+Examples+Yan,+Cheng+and+Pang,+Guansong+and+Bai,+Xiao+and+Shen,+Chunhua+and+Zhou,+Jun+and+Hancock,+Edwin">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1811.10413.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Zhuang3_arXiv.jpg"></a><b>Structured binary neural networks for accurate image classification and semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>B. Zhuang, C. Shen, M. Tan, L. Liu, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1811.10413">arXiv</a><a href="data/bibtex/CVPR19Zhuang3.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Structured+Binary+Neural+Networks+for+Accurate+Image+Classification+and+Semantic+Segmentation+Zhuang,+Bohan+and+Shen,+Chunhua+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian">search</a><a href="https://bitbucket.org/jingruixiaozhuang/group-net-image-classification/">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR19Zhang6_PDF.jpg"><b>Mind your neighbours: image annotation with metadata neighbourhood graph co-attention networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Zhang, Q. Wu, J. Zhang, C. Shen, J. Lu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Mind_Your_Neighbours_Image_Annotation_With_Metadata_Neighbourhood_Graph_Co-Attention_CVPR_2019_paper.pdf">link</a><a href="data/bibtex/CVPR19Zhang6.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mind+Your+Neighbours:+Image+Annotation+with+Metadata+Neighbourhood+Graph+Co-Attention+Networks+Zhang,+Junjie+and+Wu,+Qi+and+Zhang,+Jian+and+Shen,+Chunhua+and+Lu,+Jianfeng">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.02351.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Zhang5_arXiv.jpg"></a><b>CANet: class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Zhang, G. Lin, F. Liu, R. Yao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.02351">arXiv</a><a href="data/bibtex/CVPR19Zhang5.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={CANet}:+Class-Agnostic+Segmentation+Networks+with+Iterative+Refinement+and+Attentive+Few-Shot+Learning+Zhang,+Chi+and+Lin,+Guosheng+and+Liu,+Fayao+and+Yao,+Rui+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1904.10293.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Yan9_arXiv.jpg"></a><b>Attention-guided network for ghost-free high dynamic range imaging</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Yan, D. Gong, Q. Shi, A. van den Hengel, C. Shen, I. Reid, Y. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1904.10293">arXiv</a><a href="data/bibtex/CVPR19Yan9.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Attention-guided+Network+for+Ghost-free+High+Dynamic+Range+Imaging+Yan,+Qingsen+and+Gong,+Dong+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Reid,+Ian+and+Zhang,+Yanning">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1902.09852.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Wang4_arXiv.jpg"></a><b>Associatively segmenting instances and semantics in point clouds</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wang, S. Liu, X. Shen, C. Shen, J. Jia</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1902.09852">arXiv</a><a href="data/bibtex/CVPR19Wang4.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Associatively+Segmenting+Instances+and+Semantics+in+Point+Clouds+Wang,+Xinlong+and+Liu,+Shu+and+Shen,+Xiaoyong+and+Shen,+Chunhua+and+Jia,+Jiaya">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1812.04794.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19PengWang0_arXiv.jpg"></a><b>Neighbourhood watch: referring expression comprehension via language-guided graph attention networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, Q. Wu, J. Cao, C. Shen, L. Gao, A. vanden Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1812.04794">arXiv</a><a href="data/bibtex/CVPR19PengWang0.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Neighbourhood+Watch:+Referring+Expression+Comprehension+via+Language-guided+Graph+Attention+Networks+Wang,+Peng+and+Wu,+Qi+and+Cao,+Jiewei+and+Shen,+Chunhua+and+Gao,+Lianli+and+{vanden+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.02120.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Tian7_arXiv.jpg"></a><b>Decoders matter for semantic segmentation: data-dependent decoding enables flexible feature aggregation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Z. Tian, T. He, C. Shen, Y. Yan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.02120">arXiv</a><a href="data/bibtex/CVPR19Tian7.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Decoders+Matter+for+Semantic+Segmentation:+Data-Dependent+Decoding+Enables+Flexible+Feature+Aggregation+Tian,+Zhi+and+He,+Tong+and+Shen,+Chunhua+and+Yan,+Youliang">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1810.10804.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19Nekrasov1_arXiv.jpg"></a><b>Fast neural architecture search of compact semantic segmentation models via auxiliary cells</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>V. Nekrasov, H. Chen, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1810.10804">arXiv</a><a href="data/bibtex/CVPR19Nekrasov1.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Neural+Architecture+Search+of+Compact+Semantic+Segmentation+Models+via+Auxiliary+Cells+Nekrasov,+Vladimir+and+Chen,+Hao+and+Shen,+Chunhua+and+Reid,+Ian">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1811.11903.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19HuiLi2_arXiv.jpg"></a><b>Visual question answering as reading comprehension</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, P. Wang, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1811.11903">arXiv</a><a href="data/bibtex/CVPR19HuiLi2.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+Question+Answering+as+Reading+Comprehension+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1903.04688.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR19He8_arXiv.jpg"></a><b>Knowledge adaptation for efficient semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>T. He, C. Shen, Z. Tian, D. Gong, C. Sun, Y. Yan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1903.04688">arXiv</a><a href="data/bibtex/CVPR19He8.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Knowledge+Adaptation+for+Efficient+Semantic+Segmentation+He,+Tong+and+Shen,+Chunhua+and+Tian,+Zhi+and+Gong,+Dong+and+Sun,+Changming+and+Yan,+Youliang">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1907.13315.pdf"><img class="imgP  right"   src="data/thumbnail/PersonReID2019Zhang_arXiv.jpg"></a><b>Self-training with progressive augmentation for unsupervised cross-domain person re-identification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Zhang, J. Cao, C. Shen, M. You</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1907.13315">arXiv</a><a href="data/bibtex/PersonReID2019Zhang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Self-Training+with+Progressive+Augmentation+for+Unsupervised+Cross-Domain+Person+Re-Identification+Zhang,+Xinyu+and+Cao,+Jiewei+and+Shen,+Chunhua+and+You,+Mingyu">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1908.03706.pdf"><img class="imgP  right"   src="data/thumbnail/Temporal2019Zhang_arXiv.jpg"></a><b>Exploiting temporal consistency for real-time video depth estimation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Zhang, C. Shen, Y. Li, Y. Cao, Y. Liu, Y. Yan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.03706">arXiv</a><a href="data/bibtex/Temporal2019Zhang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Exploiting+temporal+consistency+for+real-time+video+depth+estimation+Zhang,+Haokui+and+Shen,+Chunhua+and+Li,+Ying+and+Cao,+Yuanzhouhan+and+Liu,+Yu+and+Yan,+Youliang">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1907.12209.pdf"><img class="imgP  right"   src="data/thumbnail/VNL2019Yin_arXiv.jpg"></a><b>Enforcing geometric constraints of virtual normal for depth prediction</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>W. Yin, Y. Liu, C. Shen, Y. Yan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1907.12209">arXiv</a><a href="data/bibtex/VNL2019Yin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Enforcing+geometric+constraints+of+virtual+normal+for+depth+prediction+Yin,+Wei+and+Liu,+Yifan+and+Shen,+Chunhua+and+Yan,+Youliang">search</a><a href="https://github.com/YvanYin/VNL_Monocular_Depth_Prediction">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1908.06473.pdf"><img class="imgP  right"   src="data/thumbnail/OpenSet2019Xiong_arXiv.jpg"></a><b>From open set to closed set: counting objects by spatial divide-and-conquer</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Xiong, H. Lu, C. Liu, L. Liu, Z. Cao, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.06473">arXiv</a><a href="data/bibtex/OpenSet2019Xiong.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=From+Open+Set+to+Closed+Set:+Counting+Objects+by+Spatial+Divide-and-Conquer+Xiong,+Haipeng+and+Lu,+Hao+and+Liu,+Chengxin+and+Liu,+Liang+and+Cao,+Zhiguo+and+Shen,+Chunhua">search</a><a href="https://github.com/xhp-hust-2018-2011/S-DCNet">project webpage</a></p>
</li>
<li><p><b>Efficient and accurate arbitrary-shaped text detection with pixel aggregation network</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>W. Wang, E. Xie, X. Song, Y. Zang, W. Wang, T. Lu, G. Yu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/TextDet2019Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+and+Accurate+Arbitrary-Shaped+Text+Detection+with+Pixel+Aggregation+Network+Wang,+Wenhai+and+Xie,+Enze+and+Song,+Xiaoge+and+Zang,+Yuhang+and+Wang,+Wenjia+and+Lu,+Tong+and+Yu,+Gang+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1904.01355.pdf"><img class="imgP  right"   src="data/thumbnail/FCOS2019Tian_arXiv.jpg"></a><b>FCOS: fully convolutional one-stage object detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Z. Tian, C. Shen, H. Chen, T. He</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1904.01355">arXiv</a><a href="data/bibtex/FCOS2019Tian.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FCOS}:+Fully+Convolutional+One-Stage+Object+Detection+Tian,+Zhi+and+Shen,+Chunhua+and+Chen,+Hao+and+He,+Tong">search</a><a href="https://tinyurl.com/FCOSv1">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1908.00672.pdf"><img class="imgP  right"   src="data/thumbnail/Matting2019Lu_arXiv.jpg"></a><b>Indices matter: learning to index for deep image matting</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Lu, Y. Dai, C. Shen, S. Xu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.00672">arXiv</a><a href="data/bibtex/Matting2019Lu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Indices+Matter:+Learning+to+Index+for+Deep+Image+Matting+Lu,+Hao+and+Dai,+Yutong+and+Shen,+Chunhua+and+Xu,+Songcen">search</a></p>
</li>
<li><p><b>Real-time joint semantic segmentation and depth estimation using asymmetric annotations</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>V. Nekrasov, T. Dharmasiri, A. Spek, T. Drummond, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Robotics and Automation (ICRA&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/ICRA19Nekrasov.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-Time+Joint+Semantic+Segmentation+and+Depth+Estimation+Using+Asymmetric+Annotations+Nekrasov,+Vladimir+and+Dharmasiri,+Thanuja+and+Spek,+Andrew+and+Drummond,+Tom+and+Shen,+Chunhua+and+Reid,+Ian">search</a></p>
</li>
<li><p><b>Light-weight hybrid convolutional network for liver tumor segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Zhang, Y. Xie, P. Zhang, H. Chen, Y. Xia, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/IJCAI19Zhang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Light-Weight+Hybrid+Convolutional+Network+for+Liver+Tumor+Segmentation+Zhang,+Jianpeng+and+Xie,+Yutong+and+Zhang,+Pingping+and+Chen,+Hao+and+Xia,+Yong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Deep anomaly detection with deviation networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Pang, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/KDD19Pang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Anomaly+Detection+with+Deviation+Networks+Pang,+Guansong+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Deep segmentation-emendation model for gland instance segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Xie, H. Lu, J. Zhang, C. Shen, Y. Xia</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/MICCAI2019Xie.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Segmentation-Emendation+Model+for+Gland+Instance+Segmentation+Xie,+Yutong+and+Lu,+Hao+and+Zhang,+Jianpeng+and+Shen,+Chunhua+and+Xia,+Yong">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1911.00888.pdf"><img class="imgP  right"   src="data/thumbnail/Cao2019GAN_arXiv.jpg"></a><b>Multi-marginal wasserstein GAN</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Cao, L. Mo, Y. Zhang, K. Jia, C. Shen, M. Tan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1911.00888">arXiv</a><a href="data/bibtex/Cao2019GAN.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-marginal+Wasserstein+{GAN}+Cao,+Jiezhang+and+Mo,+Langyuan+and+Zhang,+Yifan+and+Jia,+Kui+and+Shen,+Chunhua+and+Tan,+Mingkui">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1908.10553.pdf"><img class="imgP  right"   src="data/thumbnail/Scale2019Bian_arXiv.jpg"></a><b>Unsupervised scale-consistent depth and ego-motion learning from monocular video</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Bian, Z. Li, N. Wang, H. Zhan, C. Shen, M. Cheng, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;19), 2019</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1908.10553">arXiv</a><a href="data/bibtex/Scale2019Bian.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Unsupervised+Scale-consistent+Depth+and+Ego-motion+Learning+from+Monocular+Video+Bian,+Jia-Wang+and+Li,+Zhichao+and+Wang,+Naiyan+and+Zhan,+Huangying+and+Shen,+Chunhua+and+Cheng,+Ming-Ming+and+Reid,+Ian">search</a><a href="https://github.com/JiawangBian/SC-SfMLearner-Release">project webpage</a></p>
</li>
<li><p><b>HCVRD: a benchmark for large-scale human-centered visual relationship detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>B. Zhuang, Q. Wu, C. Shen, I. Reid, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/AAAI2018Zhuang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={HCVRD}:+a+benchmark+for+large-scale+Human-Centered+Visual+Relationship+Detection+Zhuang,+Bohan+and+Wu,+Qi+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Kill two birds with one stone: weakly-supervised neural network for image annotation and tag refinement</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Zhang, Q. Wu, J. Zhang, C. Shen, J. Lu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/AAAI2018Zhang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Kill+Two+Birds+with+One+Stone:+Weakly-Supervised+Neural+Network+for+Image+Annotation+and+Tag+Refinement+Zhang,+Junjie+and+Wu,+Qi+and+Zhang,+Jian+and+Shen,+Chunhua+and+Lu,+Jianfeng">search</a></p>
</li>
<li><p><b>Coarse-to-fine: a RNN-based hierarchical attention model for vehicle re-identification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wei, C. Zhang, L. Liu, C. Shen, J. Wu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/ACCV18Wei.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Coarse-to-fine:+A+{RNN}-based+hierarchical+attention+model+for+vehicle+re-identification+Wei,+Xiu-Shen+and+Zhang,+Chen-Lin+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Wu,+Jianxin">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1807.03959.pdf"><img class="imgP  right"   src="data/thumbnail/ACCV18Li_arXiv.jpg"></a><b>Deep attention-based classification network for robust depth prediction</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>R. Li, K. Xian, C. Shen, Z. Cao, H. Lu, L. Hang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1807.03959">arXiv</a><a href="data/bibtex/ACCV18Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+attention-based+classification+network+for+robust+depth+prediction+Li,+Ruibo+and+Xian,+Ke+and+Shen,+Chunhua+and+Cao,+Zhiguo+and+Lu,+Hao+and+Hang,+Lingxiao">search</a></p>
</li>
<li><p><b>Light-weight refinenet for real-time semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>V. Nekrasov, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. British Machine Vision Conference (BMVC&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/BMVC18Nekrasov.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Light-Weight+RefineNet+for+Real-Time+Semantic+Segmentation+Nekrasov,+Vladimir+and+Shen,+Chunhua+and+Reid,+Ian">search</a><a href="https://github.com/DrSleep/light-weight-refinenet">project webpage</a></p>
</li>
<li><p><b>A hybrid probabilistic model for camera relocalization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>M. Cai, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. British Machine Vision Conference (BMVC&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/BMVC18Cai.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Hybrid+Probabilistic+Model+for+Camera+Relocalization+Cai,+Ming+and+Shen,+Chunhua+and+Reid,+Ian">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1711.06370.pdf"><img class="imgP  right"   src="data/thumbnail/Zhuang2018CVPR_a_arXiv.jpg"></a><b>Parallel attention: a unified framework for visual object discovery through dialogs and queries</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>B. Zhuang, Q. Wu, C. Shen, I. Reid, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.06370">arXiv</a><a href="data/bibtex/Zhuang2018CVPR_a.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Parallel+Attention:+A+Unified+Framework+for+Visual+Object+Discovery+through+Dialogs+and+Queries+Zhuang,+Bohan+and+Wu,+Qi+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1711.00205.pdf"><img class="imgP  right"   src="data/thumbnail/Zhuang2018CVPR_b_arXiv.jpg"></a><b>Towards effective low-bitwidth convolutional neural networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>B. Zhuang, C. Shen, M. Tan, L. Liu, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.00205">arXiv</a><a href="data/bibtex/Zhuang2018CVPR_b.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Effective+Low-bitwidth+Convolutional+Neural+Networks+Zhuang,+Bohan+and+Shen,+Chunhua+and+Tan,+Mingkui+and+Liu,+Lingqiao+and+Reid,+Ian">search</a></p>
</li>
<li><p><b>Monocular relative depth perception with web stereo data supervision</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>K. Xian, C. Shen, Z. Cao, H. Lu, Y. Xiao, R. Li, Z. Luo</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Xian2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Monocular+Relative+Depth+Perception+with+Web+Stereo+Data+Supervision+Xian,+Ke+and+Shen,+Chunhua+and+Cao,+Zhiguo+and+Lu,+Hao+and+Xiao,+Yang+and+Li,+Ruibo+and+Luo,+Zhenbo">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1711.07613.pdf"><img class="imgP  right"   src="data/thumbnail/QWu2018CVPR_arXiv.jpg"></a><b>Are you talking to me? reasoned visual dialog generation through adversarial learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Wu, P. Wang, C. Shen, I. Reid, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.07613">arXiv</a><a href="data/bibtex/QWu2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Are+You+Talking+to+Me?+Reasoned+Visual+Dialog+Generation+through+Adversarial+Learning+Wu,+Qi+and+Wang,+Peng+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1711.07752.pdf"><img class="imgP  right"   src="data/thumbnail/Wang2018CVPR_arXiv.jpg"></a><b>Repulsion loss: detecting pedestrians in a crowd</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wang, T. Xiao, Y. Jiang, S. Shao, J. Sun, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.07752">arXiv</a><a href="data/bibtex/Wang2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Repulsion+Loss:+Detecting+Pedestrians+in+a+Crowd+Wang,+Xinlong+and+Xiao,+Tete+and+Jiang,+Yuning+and+Shao,+Shuai+and+Sun,+Jian+and+Shen,+Chunhua">search</a></p>
<ol reversed>
<li><p>Others have implemented our paper: <a href="https://github.com/bailvwangzi/repulsion_loss_ssd">Repulsion loss in SSD</a> and <a href="https://github.com/rainofmine/Repulsion_Loss">Repulsion loss in RetinaNet</a>.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1804.04273.pdf"><img class="imgP  right"   src="data/thumbnail/Song2018CVPR_arXiv.jpg"></a><b>VITAL: visual tracking via adversarial learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Song, C. Ma, X. Wu, L. Gong, L. Bao, W. Zuo, C. Shen, R. Lau, M. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1804.04273">arXiv</a><a href="data/bibtex/Song2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={VITAL}:+VIsual+Tracking+via+Adversarial+Learning+Song,+Yibing+and+Ma,+Chao+and+Wu,+Xiaohe+and+Gong,+Lijun+and+Bao,+Linchao+and+Zuo,+Wangmeng+and+Shen,+Chunhua+and+Lau,+Rynson+and+Yang,+Ming-Hsuan">search</a><a href="https://ybsong00.github.io/cvpr18_tracking/index">project webpage</a></p>
</li>
<li><p><b>Bootstrapping the performance of webly supervised semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>T. Shen, G. Lin, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/TongShen2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Bootstrapping+the+Performance+of+Webly+Supervised+Semantic+Segmentation+Shen,+Tong+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian">search</a><a href="https://github.com/ascust/BDWSS">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1707.04968.pdf"><img class="imgP  right"   src="data/thumbnail/Ma2018CVPR_a_arXiv.jpg"></a><b>Visual question answering with memory-augmented networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Ma, C. Shen, A. Dick, Q. Wu, P. Wang, A. van den Hengel, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1707.04968">arXiv</a><a href="data/bibtex/Ma2018CVPR_a.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+Question+Answering+with+Memory-Augmented+Networks+Ma,+Chao+and+Shen,+Chunhua+and+Dick,+Anthony+and+Wu,+Qi+and+Wang,+Peng+and+{van+den+Hengel},+Anton+and+Reid,+Ian">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1803.03474.pdf"><img class="imgP  right"   src="data/thumbnail/He2018CVPR_arXiv.jpg"></a><b>An end-to-end textspotter with explicit alignment and attention</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>T. He, Z. Tian, W. Huang, C. Shen, Y. Qiao, C. Sun</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1803.03474">arXiv</a><a href="data/bibtex/He2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+end-to-end+TextSpotter+with+Explicit+Alignment+and+Attention+He,+Tong+and+Tian,+Zhi+and+Huang,+Weilin+and+Shen,+Chunhua+and+Qiao,+Yu+and+Sun,+Changming">search</a><a href="https://github.com/tonghe90/textspotter">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1711.10703.pdf"><img class="imgP  right"   src="data/thumbnail/Chen2018CVPR_arXiv.jpg"></a><b>FSRNet: end-to-end learning face super-resolution with facial priors</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Chen, Y. Tai, X. Liu, C. Shen, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.10703">arXiv</a><a href="data/bibtex/Chen2018CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={FSRNet}:+End-to-End+Learning+Face+Super-Resolution+with+Facial+Priors+Chen,+Yu+and+Tai,+Ying+and+Liu,+Xiaoming+and+Shen,+Chunhua+and+Yang,+Jian">search</a><a href="https://github.com/tyshiwo/FSRNet">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1711.07614.pdf"><img class="imgP  right"   src="data/thumbnail/Zhang2018ECCV_arXiv.jpg"></a><b>Goal-oriented visual question generation via intermediate rewards</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Zhang, Q. Wu, C. Shen, J. Zhang, J. Lu, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1711.07614">arXiv</a><a href="data/bibtex/Zhang2018ECCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Goal-Oriented+Visual+Question+Generation+via+Intermediate+Rewards+Zhang,+Junjie+and+Wu,+Qi+and+Shen,+Chunhua+and+Zhang,+Jian+and+Lu,+Jianfeng+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1807.10097.pdf"><img class="imgP  right"   src="data/thumbnail/Deng2018ECCV_arXiv.jpg"></a><b>Learning to predict crisp boundaries</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>R. Deng, C. Shen, S. Liu, H. Wang, X. Liu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1807.10097">arXiv</a><a href="data/bibtex/Deng2018ECCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+to+Predict+Crisp+Boundaries+Deng,+Ruoxi+and+Shen,+Chunhua+and+Liu,+Shengjun+and+Wang,+Huibing+and+Liu,+Xinru">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1611.09119.pdf"><img class="imgP  right"   src="data/thumbnail/ICASSP2018Dong_arXiv.jpg"></a><b>Learning deep representations using convolutional auto-encoders with symmetric skip connections</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Dong, Y. Gan, X. Mao, Y. Yang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1611.09119">arXiv</a><a href="data/bibtex/ICASSP2018Dong.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Deep+Representations+Using+Convolutional+Auto-Encoders+with+Symmetric+Skip+Connections+Dong,+Lian-Feng+and+Gan,+Yuan-Zhu+and+Mao,+Xiao-Liao+and+Yang,+Yu-Bin+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1806.04895.pdf"><img class="imgP  right"   src="data/thumbnail/Cao2018ICML_arXiv.jpg"></a><b>Adversarial learning with local coordinate coding</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Cao, Y. Guo, Q. Wu, C. Shen, J. Huang, M. Tan</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Machine Learning (ICML&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1806.04895">arXiv</a><a href="data/bibtex/Cao2018ICML.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adversarial+Learning+with+Local+Coordinate+Coding+Cao,+Jiezhang+and+Guo,+Yong+and+Wu,+Qingyao+and+Shen,+Chunhua+and+Huang,+Junzhou+and+Tan,+Mingkui">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1802.06527.pdf"><img class="imgP  right"   src="data/thumbnail/Zhang2018IJCAI_arXiv.jpg"></a><b>Salient object detection by lossless feature reflection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Zhang, W. Liu, H. Lu, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI&rsquo;18), 2018</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1802.06527">arXiv</a><a href="data/bibtex/Zhang2018IJCAI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Salient+Object+Detection+by+Lossless+Feature+Reflection+Zhang,+Pingping+and+Liu,+Wei+and+Lu,+Huchuan+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Auxiliary tasks to improve trip hazard affordance detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. McMahon, T. Shen, N. Sunderhauf, I. Reid, C. Shen, M. Milford</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Australasian Conference on Robotics and Automation (ACRA&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/ACRA2017McMahon.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Auxiliary+Tasks+To+Improve+Trip+Hazard+Affordance+Detection+{McMahon},+Sean+and+Shen,+Tong+and+Sunderhauf,+Niko+and+Reid,+Ian+and+Shen,+Chunhua+and+Milford,+Michael">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1701.07122.pdf"><img class="imgP  right"   src="data/thumbnail/BMVC2017Tong_arXiv.jpg"></a><b>Weakly supervised semantic segmentation based on co-segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>T. Shen, G. Lin, L. Liu, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. British Machine Vision Conference (BMVC&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1701.07122">arXiv</a><a href="data/bibtex/BMVC2017Tong.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Weakly+supervised+semantic+segmentation+based+on+co-segmentation+Shen,+Tong+and+Lin,+Guosheng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Reid,+Ian">search</a></p>
</li>
<li><p><b>Visually aligned word embeddings for improving zero-shot learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>R. Qiao, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. British Machine Vision Conference (BMVC&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/BMVC17Zeroshot.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visually+Aligned+Word+Embeddings+for+Improving+Zero-shot+Learning+Qiao,+Ruizhi+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1611.09960.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017Zhuang_arXiv.jpg"></a><b>Attend in groups: a weakly-supervised deep learning framework for learning from web data</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>B. Zhuang, L. Liu, Y. Li, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1611.09960">arXiv</a><a href="data/bibtex/CVPR2017Zhuang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Attend+in+groups:+a+weakly-supervised+deep+learning+framework+for+learning+from+web+data+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Li,+Yao+and+Shen,+Chunhua+and+Reid,+Ian">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1612.05386.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017WangVQA_arXiv.jpg"></a><b>The VQA-machine: learning how to use existing vision algorithms to answer new questions</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, Q. Wu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1612.05386">arXiv</a><a href="data/bibtex/CVPR2017WangVQA.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=The+{VQA}-Machine:+Learning+How+to+Use+Existing+Vision+Algorithms+to+Answer+New+Questions+Wang,+Peng+and+Wu,+Qi+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR2017WangAttend_PDF.jpg"><b>Multi-attention network for one shot learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, L. Liu, C. Shen, Z. Huang, A. van den Hengel, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Multi-Attention_Network_for_CVPR_2017_paper.pdf">pdf</a><a href="data/bibtex/CVPR2017WangAttend.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-attention+Network+for+One+Shot+Learning+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Huang,+Zi+and+{van+den+Hengel},+Anton+and+Shen,+Heng+Tao">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1611.06612.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017Lin_arXiv.jpg"></a><b>RefineNet: multi-path refinement networks for high-resolution semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Lin, A. Milan, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1611.06612">arXiv</a><a href="data/bibtex/CVPR2017Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={RefineNet}:+Multi-Path+Refinement+Networks+for+High-Resolution+Semantic+Segmentation+Lin,+Guosheng+and+Milan,+Anton+and+Shen,+Chunhua+and+Reid,+Ian">search</a><a href="https://github.com/guosheng/refinenet">project webpage</a></p>
<ol reversed>
<li><p><a href="https://github.com/DrSleep/light-weight-refinenet">Light-weight RefineNet with Pytorch code</a>.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1611.09967.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017YaoLi_arXiv.jpg"></a><b>Sequential person recognition in photo albums with a recurrent network</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Li, G. Lin, B. Zhuang, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1611.09967">arXiv</a><a href="data/bibtex/CVPR2017YaoLi.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Sequential+Person+Recognition+in+Photo+Albums+with+a+Recurrent+Network+Li,+Yao+and+Lin,+Guosheng+and+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1612.02583.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR2017Gong_arXiv.jpg"></a><b>From motion blur to motion flow: a deep learning solution for removing heterogeneous motion blur</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>D. Gong, J. Yang, L. Liu, Y. Zhang, I. Reid, C. Shen, A. van den Hengel, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1612.02583">arXiv</a><a href="data/bibtex/CVPR2017Gong.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=From+Motion+Blur+to+Motion+Flow:+a+Deep+Learning+Solution+for+Removing+Heterogeneous+Motion+Blur+Gong,+Dong+and+Yang,+Jie+and+Liu,+Lingqiao+and+Zhang,+Yanning+and+Reid,+Ian+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1703.06246.pdf"><img class="imgP  right"   src="data/thumbnail/ICCV2017Zhuang_arXiv.jpg"></a><b>Towards context-aware interaction recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>B. Zhuang, L. Liu, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1703.06246">arXiv</a><a href="data/bibtex/ICCV2017Zhuang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+Context-aware+Interaction+Recognition+Zhuang,+Bohan+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Reid,+Ian">search</a></p>
</li>
<li><p><b>When unsupervised domain adaptation meets tensor representations</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Lu, L. Zhang, Z. Cao, W. Wei, K. Xian, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/ICCV2017Haolu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=When+Unsupervised+Domain+Adaptation+Meets+Tensor+Representations+Lu,+Hao+and+Zhang,+Lei+and+Cao,+Zhiguo+and+Wei,+Wei+and+Xian,+Ke+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1705.01674.pdf"><img class="imgP  right"   src="data/thumbnail/ICCV2017WeiLiu_arXiv.jpg"></a><b>Semi-global weighted least squares in image filtering</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>W. Liu, X. Chen, C. Shen, Z. Liu, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1705.01674">arXiv</a><a href="data/bibtex/ICCV2017WeiLiu.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Semi-Global+Weighted+Least+Squares+in+Image+Filtering+Liu,+Wei+and+Chen,+Xiaogang+and+Shen,+Chuanhua+and+Liu,+Zhi+and+Yang,+Jie">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1707.03985.pdf"><img class="imgP  right"   src="data/thumbnail/ICCV2017HuiLi_arXiv.jpg"></a><b>Towards end-to-end text spotting with convolutional recurrent neural networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, P. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1707.03985">arXiv</a><a href="data/bibtex/ICCV2017HuiLi.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Towards+End-to-end+Text+Spotting+with+Convolutional+Recurrent+Neural+Networks+Li,+Hui+and+Wang,+Peng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1705.00389.pdf"><img class="imgP  right"   src="data/thumbnail/ICCV2017Chen_arXiv.jpg"></a><b>Adversarial PoseNet: a structure-aware convolutional network for human pose estimation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Chen, C. Shen, X. Wei, L. Liu, J. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1705.00389">arXiv</a><a href="data/bibtex/ICCV2017Chen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adversarial+{PoseNet}:+A+Structure-aware+Convolutional+Network+for+Human+Pose+Estimation+Chen,+Yu+and+Shen,+Chunhua+and+Wei,+Xiu-Shen+and+Liu,+Lingqiao+and+Yang,+Jian">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1701.05105.pdf"><img class="imgP  right"   src="data/thumbnail/ICRA2017Chen_arXiv.jpg"></a><b>Deep learning features at scale for visual place recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Z. Chen, A. Jacobson, N. Sunderhauf, B. Upcroft, L. Liu, C. Shen, I. Reid, M. Milford</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Robotics and Automation (ICRA&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1701.05105">arXiv</a><a href="data/bibtex/ICRA2017Chen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Learning+Features+at+Scale+for+Visual+Place+Recognition+Chen,+Zetao+and+Jacobson,+Adam+and+Sunderhauf,+Niko+and+Upcroft,+Ben+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Reid,+Ian+and+Milford,+Michael">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1705.02758.pdf"><img class="imgP  right"   src="data/thumbnail/IJCAI2017Wei_arXiv.jpg"></a><b>Deep descriptor transforming for image co-localization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Wei, C. Zhang, Y. Li, C. Xie, J. Wu, C. Shen, Z. Zhou</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1705.02758">arXiv</a><a href="data/bibtex/IJCAI2017Wei.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Descriptor+Transforming+for+Image+Co-Localization+Wei,+Xiu-Shen+and+Zhang,+Chen-Lin+and+Li,+Yao+and+Xie,+Chen-Wei+and+Wu,+Jianxin+and+Shen,+Chunhua+and+Zhou,+Zhi-Hua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1511.02570.pdf"><img class="imgP  right"   src="data/thumbnail/IJCAI2017Wang_arXiv.jpg"></a><b>Explicit knowledge-based reasoning for visual question answering</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, Q. Wu, C. Shen, A. van den Hengel, A. Dick</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1511.02570">arXiv</a><a href="data/bibtex/IJCAI2017Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Explicit+Knowledge-based+Reasoning+for+Visual+Question+Answering+Wang,+Peng+and+Wu,+Qi+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Dick,+Anthony">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1701.07122.pdf"><img class="imgP  right"   src="data/thumbnail/IJCAI2017Tong_arXiv.jpg"></a><b>Learning multi-level region consistency with dense multi-label networks for semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>T. Shen, G. Lin, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Joint Conference on Artificial Intelligence (IJCAI&rsquo;17), 2017</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1701.07122">arXiv</a><a href="data/bibtex/IJCAI2017Tong.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Multi-level+Region+Consistency+with+Dense+Multi-label+Networks+for+Semantic+Segmentation+Shen,+Tong+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1603.02844.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16Binary_arXiv.jpg"></a><b>Fast training of triplet-based deep binary embedding networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>B. Zhuang, G. Lin, C. Shen, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.02844">arXiv</a><a href="data/bibtex/CVPR16Binary.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Training+of+Triplet-based+Deep+Binary+Embedding+Networks+Zhuang,+Bohan+and+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian">search</a><a href="https://bitbucket.org/jingruixiaozhuang/fast-training-of-triplet-based-deep-binary-embedding-networks">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1511.06973.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16AMA_arXiv.jpg"></a><b>Ask me anything: free-form visual question answering based on knowledge from external sources</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Wu, P. Wang, C. Shen, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1511.06973">arXiv</a><a href="data/bibtex/CVPR16AMA.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Ask+Me+Anything:+Free-form+Visual+Question+Answering+Based+on+Knowledge+from+External+Sources+Wu,+Qi+and+Wang,+Peng+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1506.01144.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16What_arXiv.jpg"></a><b>What value do explicit high level concepts have in vision to language problems</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Wu, C. Shen, L. Liu, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1506.01144">arXiv</a><a href="data/bibtex/CVPR16What.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=What+Value+Do+Explicit+High+Level+Concepts+Have+in+Vision+to+Language+Problems+Wu,+Qi+and+Shen,+Chunhua+and+Liu,+Lingqiao+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1602.04422.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16Irregular_arXiv.jpg"></a><b>What's wrong with that object? identifying irregular object from images by modelling the detection score distribution</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, L. Liu, C. Shen, Z. Huang, A. van den Hengel, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1602.04422">arXiv</a><a href="data/bibtex/CVPR16Irregular.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=What's+Wrong+with+that+Object?+Identifying+Irregular+Object+From+Images+by+Modelling+the+Detection+Score+Distribution+Wang,+Peng+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+Huang,+Zi+and+{van+den+Hengel},+Anton+and+Shen,+Heng+Tao">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1604.01146.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16Zeroshot_arXiv.jpg"></a><b>Less is more: zero-shot learning from online textual documents with noise suppression</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>R. Qiao, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1604.01146">arXiv</a><a href="data/bibtex/CVPR16Zeroshot.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Less+is+More:+Zero-shot+Learning+from+Online+Textual+Documents+with+Noise+Suppression+Qiao,+Ruizhi+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1504.01013.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR16labelling_arXiv.jpg"></a><b>Efficient piecewise training of deep structured models for semantic segmentation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Lin, C. Shen, A. van dan Hengel, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1504.01013">arXiv</a><a href="data/bibtex/CVPR16labelling.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+piecewise+training+of+deep+structured+models+for+semantic+segmentation+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+dan+Hengel},+Anton+and+Reid,+Ian">search</a></p>
</li>
<li><p><b>Cluster sparsity field for hyperspectral imagery denoising</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Y. Zhang, C. Shen, A. van den Hengel, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/ECCV16hyperspectral.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Cluster+Sparsity+Field+for+Hyperspectral+Imagery+Denoising+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shi,+Qinfeng">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1603.04619.pdf"><img class="imgP  right"   src="data/thumbnail/ECCV16Li_arXiv.jpg"></a><b>Image co-localization by mimicking a good detector's confidence score distribution</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.04619">arXiv</a><a href="data/bibtex/ECCV16Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Image+Co-localization+by+Mimicking+a+Good+Detector's+Confidence+Score+Distribution+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/NeurIPS2016_PDF.jpg"><b>Image restoration using very deep fully convolutional encoder-decoder networks with symmetric skip connections</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Mao, C. Shen, Y. Yang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;16), 2016</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1603.09056">arXiv</a><a href="http://papers.NeurIPS.cc/paper/6172-image-restoration-using-very-deep-convolutional-encoder-decoder-networks-with-symmetric-skip-connections.pdf">link</a><a href="data/bibtex/NeurIPS2016.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Image+Restoration+Using+Very+Deep+Fully+Convolutional+Encoder-Decoder+Networks+with+Symmetric+Skip+Connections+Mao,+Xiao-Jiao+and+Shen,+Chunhua+and+Yang,+Yu-Bin">search</a><a href="https://bitbucket.org/chhshen/image-denoising/">project webpage</a></p>
<ol reversed>
<li><p>Others have <a href="https://github.com/titu1994/Image-Super-Resolution">implemented our paper</a>.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1504.01492.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15e_arXiv.jpg"></a><b>Efficient SDP inference for fully-connected CRFs based on low-rank decomposition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1504.01492">arXiv</a><a href="data/bibtex/CVPR15e.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+{SDP}+Inference+for+Fully-connected+{CRFs}+Based+on+Low-rank+Decomposition+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR15g_PDF.jpg"><b>Learning graph structure for multi-label image classification via clique generation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>M. Tan, Q. Shi, A. van den Hengel, C. Shen, J. Gao, F. Hu, Z. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tan_Learning_Graph_Structure_2015_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR15g.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Graph+Structure+for+Multi-label+Image+Classification+via+Clique+Generation+Tan,+Mingkui+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Shen,+Chunhua+and+Gao,+Junbin+and+Hu,+Fuyuan+and+Zhang,+Zhen">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR15c_PDF.jpg"><b>Supervised discrete hashing</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Shen, C. Shen, W. Liu, H. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Shen_Supervised_Discrete_Hashing_2015_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR15c.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Supervised+Discrete+Hashing+Shen,+Fumin+and+Shen,+Chunhua+and+Liu,+Wei+and+Shen,+Heng+Tao">search</a><a href="https://github.com/bd622/DiscretHashing/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1503.01543.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15f_arXiv.jpg"></a><b>Learning to rank in person re-identification with metric ensembles</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1503.01543">arXiv</a><a href="data/bibtex/CVPR15f.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+to+rank+in+person+re-identification+with+metric+ensembles+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.7466.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15d_arXiv.jpg"></a><b>The treasure beneath convolutional layers: cross convolutional layer pooling for image classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.7466">arXiv</a><a href="data/bibtex/CVPR15d.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=The+Treasure+beneath+Convolutional+Layers:+Cross+convolutional+layer+Pooling+for+Image+Classification+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.6387.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15b_arXiv.jpg"></a><b>Deep convolutional neural fields for depth estimation from a single image</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Liu, C. Shen, G. Lin</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.6387">arXiv</a><a href="data/bibtex/CVPR15b.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deep+Convolutional+Neural+Fields+for+Depth+Estimation+from+a+Single+Image+Liu,+Fayao+and+Shen,+Chunhua+and+Lin,+Guosheng">search</a><a href="http://goo.gl/rAKWrS">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.6382.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR15a_arXiv.jpg"></a><b>Mid-level deep pattern mining</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Li, L. Liu, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.6382">arXiv</a><a href="data/bibtex/CVPR15a.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Mid-level+Deep+Pattern+Mining+Li,+Yao+and+Liu,+Lingqiao+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a><a href="https://github.com/yaoliUoA/MDPM">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR15h_PDF.jpg"><b>Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>B. Li, C. Shen, Y. Dai, A. van den Hengel, M. He</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Li_Depth_and_Surface_2015_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR15h.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Depth+and+Surface+Normal+Estimation+from+Monocular+Images+Using+Regression+on+Deep+Features+and+Hierarchical+{CRFs}+Li,+Bo+and+Shen,+Chunhua+and+Dai,+Yuchao+and+{van+den+Hengel},+Anton+and+He,+Mingyi">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV15Zhang_PDF.jpg"><b>Hyperspectral compressive sensing using manifold-structured sparsity prior</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhang, W. Wei, Y. Zhang, F. Li, C. Shen, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zhang_Hyperspectral_Compressive_Sensing_ICCV_2015_paper.pdf">pdf</a><a href="data/bibtex/ICCV15Zhang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Hyperspectral+Compressive+Sensing+Using+Manifold-Structured+Sparsity+Prior+Zhang,+Lei+and+Wei,+Wei+and+Zhang,+Yanning+and+Li,+Fei+and+Shen,+Chunhua+and+Shi,+Qinfeng">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/NeurIPS15Lin_PDF.jpg"><b>Deeply learning the messages in message passing inference</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Lin, C. Shen, I. Reid, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1506.02108">arXiv</a><a href="http://papers.NeurIPS.cc/paper/5791-deeply-learning-the-messages-in-message-passing-inference.pdf">pdf</a><a href="data/bibtex/NeurIPS15Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Deeply+Learning+the+Messages+in+Message+Passing+Inference+Lin,+Guosheng+and+Shen,+Chunhua+and+Reid,+Ian+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR15workshop_PDF.jpg"><b>Sequence searching with deep-learnt depth for condition- and viewpoint-invariant route-based place recognition</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>M. Milford, C. Shen, S. Lowry, N. Suenderhauf, S. Shirazi, G. Lin, F. Liu, E. Pepperell, C. Lerma, B. Upcroft, I. Reid</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. 6th International Workshop on Computer Vision in Vehicle Technology, in conjunction with IEEE Conference on Computer Vision and Pattern Recognition (CVVT&rsquo;15), 2015</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W11/papers/Milford_Sequence_Searching_With_2015_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR15workshop.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Sequence+searching+with+Deep-Learnt+Depth+for+Condition-+and+Viewpoint-Invariant+Route-Based+Place+Recognition+Milford,+Michael+and+Shen,+Chunhua+and+Lowry,+Stephanie+and+Suenderhauf,+Niko+and+Shirazi,+Sareh+and+Lin,+Guosheng+and+Liu,+Fayao+and+Pepperell,+Edward+and+Lerma,+Cesar+and+Upcroft,+Ben+and+Reid,+Ian">search</a></p>
<ol reversed>
<li><p>Best paper award (Sponsored by NVIDIA).</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1404.1561.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR14Lin_arXiv.jpg"></a><b>Fast supervised hashing with decision trees for high-dimensional data</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Lin, C. Shen, Q. Shi, A. van den Hengel, D. Suter</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;14), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1404.1561">arXiv</a><a href="https://bitbucket.org/chhshen/fasthash/src">link</a><a href="data/bibtex/CVPR14Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Supervised+Hashing+with+Decision+Trees+for+High-Dimensional+Data+Lin,+Guosheng+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Suter,+David">search</a><a href="https://bitbucket.org/chhshen/fasthash/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1407.0786.pdf"><img class="imgP  right"   src="data/thumbnail/ECCV14Paul_arXiv.jpg"></a><b>Strengthening the effectiveness of pedestrian detection with spatially pooled features</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;14), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1407.0786">arXiv</a><a href="data/bibtex/ECCV14Paul.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Strengthening+the+Effectiveness+of+Pedestrian+Detection+with+Spatially+Pooled+Features+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a><a href="https://github.com/chhshen/pedestrian-detection">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1407.1151.pdf"><img class="imgP  right"   src="data/thumbnail/ECCV14Lin_arXiv.jpg"></a><b>Optimizing ranking measures for compact binary code learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Lin, C. Shen, J. Wu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;14), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1407.1151">arXiv</a><a href="data/bibtex/ECCV14Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Optimizing+Ranking+Measures+for+Compact+Binary+Code+Learning+Lin,+Guosheng+and+Shen,+Chunhua+and+Wu,+Jianxin">search</a><a href="https://bitbucket.org/guosheng/structhash">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1411.6406.pdf"><img class="imgP  right"   src="data/thumbnail/Liu2014Fisher_arXiv.jpg"></a><b>Encoding high dimensional local features by sparse coding based Fisher vectors</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Liu, C. Shen, L. Wang, A. van den Hengel, C. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;14), 2014</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1411.6406">arXiv</a><a href="data/bibtex/Liu2014Fisher.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Encoding+High+Dimensional+Local+Features+by+Sparse+Coding+Based+{F}isher+Vectors+Liu,+Lingqiao+and+Shen,+Chunhua+and+Wang,+Lei+and+{van+den+Hengel},+Anton+and+Wang,+Chao">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR13eYao_PDF.jpg"><b>Part-based visual tracking with online latent structural learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>R. Yao, Q. Shi, C. Shen, Y. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/77413">link</a><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yao_Part-Based_Visual_Tracking_2013_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR13eYao.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Part-based+Visual+Tracking+with+Online+Latent+Structural+Learning+Yao,+Rui+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton">search</a><a href="https://github.com/chhshen/PartTracking">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR13cWang_PDF.jpg"><b>Bilinear programming for human activity recognition with unknown MRF graphs</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Z. Wang, Q. Shi, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/77411">link</a><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_Bilinear_Programming_for_2013_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR13cWang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Bilinear+Programming+for+Human+Activity+Recognition+with+unknown+{MRF}+graphs+Wang,+Zhenhua+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1304.0840.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR13dWang_arXiv.jpg"></a><b>A fast semidefinite approach to solving binary quadratic problems</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1304.0840">arXiv</a><a href="data/bibtex/CVPR13dWang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Fast+Semidefinite+Approach+to+Solving+Binary+Quadratic+Problems+Wang,+Peng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a><a href="./projects/BQP/">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation, 60 out of 1870 submissions.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1303.7043.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR13aShen_arXiv.jpg"></a><b>Inductive hashing on manifolds</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>F. Shen, C. Shen, Q. Shi, A. van den Hengel, Z. Tang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1303.7043">arXiv</a><a href="data/bibtex/CVPR13aShen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Inductive+Hashing+on+Manifolds+Shen,+Fumin+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+{van+den+Hengel},+Anton+and+Tang,+Zhenmin">search</a><a href="https://github.com/chhshen/Hashing-on-Nonlinear-Manifolds">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/CVPR13bLi_PDF.jpg"><b>Learning compact binary codes for visual tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Li, C. Shen, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/77412">link</a><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Learning_Compact_Binary_2013_CVPR_paper.pdf">pdf</a><a href="data/bibtex/CVPR13bLi.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Compact+Binary+Codes+for+Visual+Tracking+Li,+Xi+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1310.4891.pdf"><img class="imgP  right"   src="data/thumbnail/ICCV2013Harandi_arXiv.jpg"></a><b>Dictionary learning and sparse coding on Grassmann manifolds: an extrinsic solution</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>M. Harandi, C. Sanderson, C. Shen, B. Lovell</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1310.4891">arXiv</a><a href="data/bibtex/ICCV2013Harandi.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Dictionary+Learning+and+Sparse+Coding+on+{G}rassmann+Manifolds:+An+Extrinsic+Solution+{Harandi},+Mehrtash+and+{Sanderson},+Conrad+and+Shen,+Chunhua+and+Lovell,+Brian">search</a><a href="https://github.com/chhshen/Grassmann/">project webpage</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1310.0900.pdf"><img class="imgP  right"   src="data/thumbnail/ICCV13Pai_arXiv.jpg"></a><b>Efficient pedestrian detection by directly optimizing the partial area under the ROC curve</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1310.0900">arXiv</a><a href="http://hdl.handle.net/2440/83158">pdf</a><a href="data/bibtex/ICCV13Pai.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficient+pedestrian+detection+by+directly+optimizing+the+partial+area+under+the+{ROC}+curve+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1309.1853.pdf"><img class="imgP  right"   src="data/thumbnail/ICCV13Lin_arXiv.jpg"></a><b>A general two-step approach to learning-based hashing</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Lin, C. Shen, D. Suter, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1309.1853">arXiv</a><a href="data/bibtex/ICCV13Lin.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+General+Two-step+Approach+to+Learning-Based+Hashing+Lin,+Guosheng+and+Shen,+Chunhua+and+Suter,+David+and+{van+den+Hengel},+Anton">search</a><a href="https://bitbucket.org/guosheng/two-step-hashing/">project webpage</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV13Li_arXiv.jpg"><b>Contextual hypergraph modeling for salient object detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Li, Y. Li, C. Shen, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1310.5767">arXiv</a><a href="data/bibtex/ICCV13Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Contextual+Hypergraph+Modeling+for+Salient+Object+Detection+Li,+Xi+and+Li,+Yao+and+Shen,+Chunhua+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">search</a><a href="https://bitbucket.org/chhshen/saliency-detection">project webpage</a></p>
</li>
<li><p><b>Extended depth-of-field via focus stacking and graph cuts</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Zhang, J. Bastian, C. Shen, A. van den Hengel, T. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Image Processing (ICIP&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/ICIP13cShen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Extended+depth-of-field+via+focus+stacking+and+graph+cuts+Zhang,+Chao+and+Bastian,+John+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Shen,+Tingzhi">search</a></p>
</li>
<li><p><b>Approximate constraint generation for efficient structured boosting</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Lin, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Image Processing (ICIP&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/ICIP13aShen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Approximate+constraint+generation+for+efficient+structured+boosting+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Leveraging surrounding context for scene text detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Li, C. Shen, W. Jia, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Image Processing (ICIP&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/ICIP13bShen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Leveraging+surrounding+context+for+scene+text+detection+Li,+Yao+and+Shen,+Chunhua+and+Jia,+Wenjing+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICML13a_PDF.jpg"><b>Learning hash functions using column generation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Li, G. Lin, C. Shen, A. van den Hengel, A. Dick</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Machine Learning (ICML&rsquo;13), 2013</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1303.0339">arXiv</a><a href="http://jmlr.csail.mit.edu/proceedings/papers/v28/li13a.pdf">pdf</a><a href="data/bibtex/ICML13a.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Hash+Functions+Using+Column+Generation+Li,+Xi+and+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Dick,+Anthony">search</a><a href="https://bitbucket.org/guosheng/column-generation-hashing/">project webpage</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1311.5947.pdf"><img class="imgP  right"   src="data/thumbnail/ACCV12_arXiv.jpg"></a><b>Fast training of effective multi-class boosting using coordinate descent optimization</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>G. Lin, C. Shen, A. van den Hengel, D. Suter</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;12), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1311.5947">arXiv</a><a href="data/bibtex/ACCV12.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+Training+of+Effective+Multi-class+Boosting+Using+Coordinate+Descent+Optimization+Lin,+Guosheng+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Suter,+David">search</a></p>
</li>
<li><p><b>Sharing features in multi-class boosting via group sparsity</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;12), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/69851">pdf</a><a href="data/bibtex/CVPR12b.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Sharing+Features+in+Multi-class+Boosting+via+Group+Sparsity+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1204.2912.pdf"><img class="imgP  right"   src="data/thumbnail/CVPR12a_arXiv.jpg"></a><b>Non-sparse linear representations for visual tracking with online reservoir metric learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Li, C. Shen, Q. Shi, A. Dick, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;12), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1204.2912">arXiv</a><a href="http://hdl.handle.net/2440/70244">pdf</a><a href="data/bibtex/CVPR12a.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Non-sparse+Linear+Representations+for+Visual+Tracking+with+Online+Reservoir+Metric+Learning+Li,+Xi+and+Shen,+Chunhua+and+Shi,+Qinfeng+and+Dick,+Anthony+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ECCV12_PDF.jpg"><b>Robust tracking with weighted online structured learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>R. Yao, Q. Shi, C. Shen, Y. Zhang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;12), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="https://sites.google.com/site/chhshen/publication/weighted_tracking_eccv12.pdf?attredirects=1">pdf</a><a href="data/bibtex/ECCV12.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Robust+Tracking+with+Weighted+Online+Structured+Learning+Yao,+Rui+and+Shi,+Qinfeng+and+Shen,+Chunhua+and+Zhang,+Yanning+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1206.4651.pdf"><img class="imgP  right"   src="data/thumbnail/ICML12_arXiv.jpg"></a><b>Is margin preserved after random projection?</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Shi, C. Shen, R. Hill, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Machine Learning (ICML&rsquo;12), 2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1206.4651">arXiv</a><a href="http://hdl.handle.net/2440/71063">link</a><a href="data/bibtex/ICML12.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Is+margin+preserved+after+random+projection?+Shi,+Qinfeng+and+Shen,+Chunhua+and+Hill,+Rhys+and+van+den+Hengel,+Anton">search</a></p>
<ol reversed>
<li><p>This work provides an analysis of margin distortion under random projections, the conditions under which margins are preserved, and presents bounds on the margin distortion.</p>
</li></ol>
</li>
<li><p><b>Efficiently learning a distance metric for large margin nearest neighbor classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>K. Park, C. Shen, Z. Hao, J. Kim</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. AAAI Conference on Artificial Intelligence (AAAI&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/AAAI2011.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficiently+learning+a+distance+metric+for+large+margin+nearest+neighbor+classification+Park,+Kyoungup+and+Shen,+Chunhua+and+Hao,+Zhihui+and+Kim,+Junae">search</a></p>
</li>
<li><p><b>Is face recognition really a compressive sensing problem?</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Shi, A. Eriksson, A. van den Hengel, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/67036">pdf</a><a href="data/bibtex/Shi2011CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Is+face+recognition+really+a+Compressive+Sensing+problem?+Shi,+Qinfeng+and+Eriksson,+Anders+and+van+den+Hengel,+Anton+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2011CVPRb_PDF.jpg"><b>A scalable dual approach to semidefinite metric learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, J. Kim, L. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://goo.gl/UyVdEc">pdf</a><a href="data/bibtex/Shen2011CVPRb.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Scalable+Dual+Approach+to+Semidefinite+Metric+Learning+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei">search</a></p>
</li>
<li><p><b>A direct formulation for totally-corrective multi-class boosting</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, Z. Hao</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/62919">pdf</a><a href="data/bibtex/Shen2011CVPRa.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+direct+formulation+for+totally-corrective+multi-class+boosting+Shen,+Chunhua+and+Hao,+Zhihui">search</a></p>
</li>
<li><p><b>A generalized probabilistic framework for compact codebook creation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Liu, L. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://hdl.handle.net/2440/63014">pdf</a><a href="data/bibtex/Liu2011CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+generalized+probabilistic+framework+for+compact+codebook+creation+Liu,+Lingqiao+and+Wang,+Lei+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Real-time visual tracking using compressive sensing</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, C. Shen, Q. Shi</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://goo.gl/dsjsoM">pdf</a><a href="data/bibtex/Li2011CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-time+visual+tracking+Using+compressive+sensing+Li,+Hanxi+and+Shen,+Chunhua+and+Shi,+Qinfeng">search</a></p>
</li>
<li><p><b>Laplacian margin distribution boosting for learning from sparsely labeled data</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>T. Wang, X. He, C. Shen, N. Barnes</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Digital Image Computing: Techniques and Applications (DICTA&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/DICTA2011a.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Laplacian+margin+distribution+boosting+for+learning+from+sparsely+labeled+data+Wang,+Tao+and+He,+Xuming+and+Shen,+Chunhua+and+Barnes,+Nick">search</a></p>
</li>
<li><p><b>On the optimality of sequential forward feature selection using class separability measure</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Wang, C. Shen, R. Hartley</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Digital Image Computing: Techniques and Applications (DICTA&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/DICTA2011b.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=On+The+Optimality+of+Sequential+Forward+Feature+Selection+Using+Class+Separability+Measure+Wang,+Lei+and+Shen,+Chunhua+and+Hartley,+Richard">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/ICCV2011_PDF.jpg"><b>Graph mode-based contextual kernels for robust SVM tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>X. Li, A. Dick, H. Wang, C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;11), 2011</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://goo.gl/GzpBVb">pdf</a><a href="data/bibtex/ICCV2011.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Graph+mode-based+contextual+kernels+for+robust+{SVM}+tracking+Li,+Xi+and+Dick,+Anthony+and+Wang,+Hanzi+and+Shen,+Chunhua+and+van+den+Hengel,+Anton">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Zheng2010ACCV_PDF.jpg"><b>Pyramid center-symmetric local binary, trinary patterns for effective pedestrian detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Zheng, C. Shen, R. Hartley, X. Huang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;10), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://goo.gl/5Cthse">pdf</a><a href="data/bibtex/Zheng2010ACCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Pyramid+Center-symmetric+Local+Binary,+Trinary+Patterns+for+Effective+Pedestrian+Detection+Zheng,+Yongbin+and+Shen,+Chunhua+and+Hartley,+Richard+and+Huang,+Xinsheng">search</a></p>
</li>
<li><p><b>Asymmetric totally-corrective boosting for real-time object detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, C. Shen, N. Barnes, H. Zheng, Z. Ren</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;10), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Wang2010ACCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Asymmetric+Totally-corrective+Boosting+for+Real-time+Object+Detection+Wang,+Peng+and+Shen,+Chunhua+and+Barnes,+Nick+and+Zheng,+Hong+and+Ren,+Zhang">search</a></p>
<ol reversed>
<li><p>Oral presentation.</p>
</li></ol>
</li>
<li><p><b>Face detection with effective feature extraction</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;10), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Paul2010ACCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Face+Detection+with+Effective+Feature+Extraction+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian">search</a></p>
</li>
<li><p><b>Totally-corrective multi-class boosting</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Z. Hao, C. Shen, N. Barnes, B. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Asian Conference on Computer Vision (ACCV&rsquo;10), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Hao2010ACCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Totally-corrective+Multi-class+Boosting+Hao,+Zhihui+and+Shen,+Chunhua+and+Barnes,+Nick+and+Wang,+Bo">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shi2010CVPR_PDF.jpg"><b>Rapid face recognition using hashing</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Shi, H. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;10), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://sites.google.com/site/chhshen/publication/cvpr10.pdf?attredirects=1">pdf</a><a href="data/bibtex/Shi2010CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Rapid+face+recognition+using+hashing+Shi,+Qinfeng+and+Li,+Hanxi+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Robust face recognition via accurate face alignment and sparse representation</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, P. Wang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on on Digital Image Computing: Techniques and Applications (DICTA&rsquo;10), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Face2010Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Robust+Face+Recognition+via+Accurate+Face+Alignment+and+Sparse+Representation+Li,+Hanxi+and+Wang,+Peng+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><a class="imglink"  target="_blank" href="https://arxiv.org/pdf/1005.4103.pdf"><img class="imgP  right"   src="data/thumbnail/Shen2010ECCV_arXiv.jpg"></a><b>LACBoost and FisherBoost: optimally building cascade classifiers</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, P. Wang, H. Li</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;10), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/1005.4103">arXiv</a><a href="http://dx.doi.org/10.1007/978-3-642-15552-9_44">link</a><a href="data/bibtex/Shen2010ECCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={LACBoost}+and+{FisherBoost}:+Optimally+Building+Cascade+Classifiers+Shen,+Chunhua+and+Wang,+Peng+and+Li,+Hanxi">search</a></p>
</li>
<li><p><b>Improved human detection and classification in thermal images</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>W. Wang, J. Zhang, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Image Processing (ICIP&rsquo;10), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Human2010.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Improved+Human+Detection+and+Classification+in+Thermal+Images+Wang,+Weihong+and+Zhang,+Jian+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Training a multi-exit cascade with linear asymmetric classification for efficient object detection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, C. Shen, H. Zheng, Z. Ren</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Image Processing (ICIP&rsquo;10), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Multiexit2010Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Training+a+multi-exit+cascade+with+linear+asymmetric+classification+for+efficient+object+detection+Wang,+Peng+and+Shen,+Chunhua+and+Zheng,+Hong+and+Ren,+Zhang">search</a></p>
</li>
<li><p><b>Hippocampal shape classification using redundancy constrained feature selection</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Zhou, L. Wang, C. Shen, N. Barnes</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI&rsquo;10), 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Zhou2010MICCAI.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Hippocampal+shape+classification+using+redundancy+constrained+feature+selection+Zhou,+Luping+and+Wang,+Lei+and+Shen,+Chunhua+and+Barnes,+Nick">search</a></p>
</li>
<li><p><b>A variant of the trace quotient formulation for dimensionality reduction</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>P. Wang, C. Shen, H. Zheng, Z. Ren</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. 9th Asian Conference on Computer Vision (ACCV&rsquo;09), 2009</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Wang2009ACCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Variant+of+the+Trace+Quotient+Formulation+for+Dimensionality+Reduction+Wang,+Peng+and+Shen,+Chunhua+and+Zheng,+Hong+and+Ren,+Zhang">search</a></p>
</li>
<li><p><b>A scalable algorithm for learning a Mahalanobis distance metric</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Kim, C. Shen, L. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. 9th Asian Conference on Computer Vision (ACCV&rsquo;09), 2009</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Kim2009ACCV.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Scalable+Algorithm+for+Learning+a+{M}ahalanobis+Distance+Metric+Kim,+Junae+and+Shen,+Chunhua+and+Wang,+Lei">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Paisitkriangkrai2009CVPR_PDF.jpg"><b>Efficiently training a better visual detector with sparse eigenvectors</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;09), 2009</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0903.3103">arXiv</a><a href="http://sites.google.com/site/chhshen/publication/CVPR2009GSLDA.pdf?attredirects=1">link</a><a href="data/bibtex/Paisitkriangkrai2009CVPR.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Efficiently+Training+a+Better+Visual+Detector+with+Sparse+Eigenvectors+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian">search</a></p>
</li>
<li><p><b>A two-layer night-time vehicle detector</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>W. Wang, C. Shen, J. Zhang, S. Paisitkriangkrai</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;09), 2009</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Wang2009DICTA.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Two-Layer+Night-time+Vehicle+Detector+Wang,+Weihong+and+Shen,+Chunhua+and+Zhang,+Jian+and+Paisitkriangkrai,+Sakrapee">search</a></p>
</li>
<li><p><b>Smooth approximation of <img class="eq" src="eqs/6240687686047976017-130.png" alt="l_infty" style="vertical-align: -4px" />-norm for multi-view geometry</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Dai, H. Li, M. He, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;09), 2009</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Dai2009DICTA.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Smooth+Approximation+of+L_\infty-Norm+for+Multi-view+Geometry+Dai,+Yuchao+and+Li,+Hongdong+and+He,+Mingyi+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2009PSD_PDF.jpg"><b>Positive semidefinite metric learning with boosting</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, J. Kim, L. Wang, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;09), 2009</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://arxiv.org/abs/0910.2279">arXiv</a><a href="http://papers.NeurIPS.cc/paper/3658-positive-semidefinite-metric-learning-with-boosting.pdf">pdf</a><a href="data/bibtex/Shen2009PSD.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Positive+semidefinite+metric+learning+with+Boosting+Shen,+Chunhua+and+Kim,+Junae+and+Wang,+Lei+and+{van+den+Hengel},+Anton">search</a><a href="https://bitbucket.org/chhshen/data/raw/45d101372013763d18f0a7ed191c86569532ed96/code/BoostMetric-NIPS09-codes-V0.1.tar.bz2">project webpage</a></p>
</li>
<li><p><b>Self-calibrating cameras using semidefinite programming</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, H. Li, M. Brooks</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;08), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/DICTA.2008.46">link</a><a href="data/bibtex/Shen2008Self.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Self-Calibrating+Cameras+Using+Semidefinite+Programming+Shen,+Chunhua+and+Li,+Hongdong+and+Brooks,+Michael+J.">search</a></p>
</li>
<li><p><b>Multi-view human motion capture with an improved deformation skin model</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Y. Lu, L. Wang, R. Hartley, H. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;08), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Lu2008Human.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Multi-view+Human+Motion+Capture+with+An+Improved+Deformation+Skin+Model+Lu,+Yifan+and+Wang,+Lei+and+Hartley,+Richard+and+Li,+Hongdong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Boosting the minimum margin: LPBoost vs. AdaBoost</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;08), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Li2008Boosting.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Boosting+the+minimum+margin:+{LPBoost}+vs.+{AdaBoost}+Li,+Hanxi+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Learning cascaded reduced-set SVMs using linear programming</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Kim, C. Shen, L. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;08), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Junae2008SVM.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Learning+Cascaded+Reduced-set+{SVM}s+Using+Linear+Programming+Kim,+Junae+and+Shen,+Chunhua+and+Wang,+Lei">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Fast2008Wang_PDF.jpg"><b>A fast algorithm for creating a compact and discriminative visual codebook</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>L. Wang, L. Zhou, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. European Conference on Computer Vision (ECCV&rsquo;08), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1007/978-3-540-88693-8_53">link</a><a href="http://sites.google.com/site/chhshen/publication/ECCV2008Wang.pdf?attredirects=1">pdf</a><a href="data/bibtex/Fast2008Wang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+Fast+Algorithm+for+Creating+a+Compact+and+Discriminative+Visual+Codebook+Wang,+Lei+and+Zhou,+Luping+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Face detection from few training examples</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, S. Paisitkriangkrai, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Image Processing (ICIP&rsquo;08), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/ICIP.2008.4712367">link</a><a href="data/bibtex/Face2008Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Face+Detection+From+Few+Training+Examples+Shen,+Chunhua+and+Paisitkriangkrai,+Sakrapee+and+Zhang,+Jian">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2008PSD_PDF.jpg"><b>PSDBoost: matrix-generation linear programming for positive semidefinite matrices learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, A. Welsh, L. Wang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Advances in Neural Information Processing Systems (NeurIPS&rsquo;08), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://papers.NeurIPS.cc/paper/3611-psdboost-matrix-generation-linear-programming-for-positive-semidefinite-matrices-learning.pdf">pdf</a><a href="data/bibtex/Shen2008PSD.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={PSDB}oost:+Matrix-generation+linear+programming+for+positive+semidefinite+matrices+learning+Shen,+Chunhua+and+Welsh,+Alan+and+Wang,+Lei">search</a></p>
</li>
<li><p><b>Real-time pedestrian detection using a boosted multi-layer classifier</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. 8th IEEE International Workshop on Visual Surveillance, in conjunction with European Conference on Computer Vision (ECCVW&rsquo;08), 2008</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/Realtime2008Paisitkriangkrai.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Real-time+Pedestrian+Detection+Using+a+Boosted+Multi-layer+Classifier+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian">search</a></p>
</li>
<li><p><b>A convex programming approach to the trace quotient problem</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, H. Li, M. Brooks</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. 8th Asian Conference on Computer Vision (ACCV&rsquo;07), 2007</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1007/978-3-540-76390-1_23">link</a><a href="data/bibtex/Convex2007Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=A+convex+programming+approach+to+the+trace+quotient+problem+Shen,+Chunhua+and+Li,+Hongdong+and+Brooks,+Michael+J.">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Kernel2007Quang_PDF.jpg"><b>Kernel-based tracking from a probabilistic viewpoint</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Nguyen, A. Robles-Kelly, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&rsquo;07), 2007</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/CVPR.2007.383240">link</a><a href="http://goo.gl/1QNmaq">pdf</a><a href="data/bibtex/Kernel2007Quang.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Kernel-based+tracking+from+a+probabilistic+viewpoint+Nguyen,+Quang+and+Robles-Kelly,+Antonio+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Feature extraction using sequential semidefinite programming</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, H. Li, M. Brooks</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;07), 2007</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/DICTA.2007.4426829">link</a><a href="data/bibtex/Feature2007Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Feature+extraction+using+sequential+semidefinite+programming+Shen,+Chunhua+and+Li,+Hongdong+and+Brooks,+Michael+J.">search</a></p>
</li>
<li><p><b>An experimental evaluation of local features for pedestrian classification</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>S. Paisitkriangkrai, C. Shen, J. Zhang</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;07), 2007</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/DICTA.2007.4426775">link</a><a href="data/bibtex/Experimental2007Paul.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+Experimental+Evaluation+of+Local+Features+for+Pedestrian+Classification+Paisitkriangkrai,+Sakrapee+and+Shen,+Chunhua+and+Zhang,+Jian">search</a></p>
<ol reversed>
<li><p>Best Paper Award.</p>
</li></ol>
</li>
<li><p><b>Color image labelling using linear programming</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, C. Shen, Z. Wen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;07), 2007</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/DICTA.2007.4426802">link</a><a href="data/bibtex/Color2007Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Color+image+labelling+using+linear+programming+Li,+Hongdong+and+Shen,+Chunhua+and+Wen,+Zhiying">search</a></p>
</li>
<li><p><b>Object-respecting colour image segmentation: an LP approach</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Image Processing (ICIP&rsquo;07), 2007</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/ICIP.2007.4379141">link</a><a href="data/bibtex/Object2007Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Object-respecting+colour+image+segmentation:+An+{LP}+approach+Li,+Hongdong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>Classification-based likelihood functions for Bayesian tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, H. Li, M. Brooks</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Advanced Video and Signal based Surveillance (AVSS&rsquo;06), 2006</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/AVSS.2006.33">link</a><a href="data/bibtex/Classification2006Shen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Classification-based+likelihood+functions+for+{B}ayesian+tracking+Shen,+Chunhua+and+Li,+Hongdong+and+Brooks,+Michael+J.">search</a></p>
</li>
<li><p><b>Enhanced kernel-based tracking for monochromatic and thermographic video</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Q. Nguyen, A. Robles-Kelly, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Advanced Video and Signal based Surveillance (AVSS&rsquo;06), 2006</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/AVSS.2006.47">link</a><a href="data/bibtex/Enhanced2006Nguyen.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Enhanced+kernel-based+tracking+for+monochromatic+and+thermographic+video+Nguyen,+Quang+and+Robles-Kelly,+Antonio+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><b>An LMI approach for reliable PTZ camera self-calibration</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>H. Li, C. Shen</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Advanced Video and Signal based Surveillance (AVSS&rsquo;06), 2006</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/AVSS.2006.21">link</a><a href="data/bibtex/LMI2006Li.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=An+{LMI}+approach+for+reliable+{PTZ}+camera+self-calibration+Li,+Hongdong+and+Shen,+Chunhua">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2005Fast_PDF.jpg"><b>Fast global kernel density mode seeking with application to localisation and tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, M. Brooks, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Computer Vision (ICCV&rsquo;05), 2005</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/ICCV.2005.94">link</a><a href="http://goo.gl/UHzjWW">pdf</a><a href="data/bibtex/Shen2005Fast.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Fast+global+kernel+density+mode+seeking+with+application+to+localisation+and+tracking+Shen,+Chunhua+and+Brooks,+Michael+J.+and+{van+den+Hengel},+Anton">search</a></p>
<ol reversed>
<li><p>Oral presentation, 45 out of 1200 submissions.</p>
</li></ol>
</li>
<li><p><b>Visual tracking via efficient kernel discriminant subspace learning</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, A. van den Hengel, M. Brooks</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Image Processing (ICIP&rsquo;05), 2005</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/ICIP.2005.1530124">link</a><a href="data/bibtex/Shen2005Visual.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Visual+tracking+via+efficient+kernel+discriminant+subspace+learning+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Brooks,+Michael+J.">search</a></p>
</li>
<li><p><b>Augmented particle filtering for efficient visual tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, M. Brooks, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. IEEE International Conference on Image Processing (ICIP&rsquo;05), 2005</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/ICIP.2005.1530527">link</a><a href="data/bibtex/Shen2005Augmented.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Augmented+particle+filtering+for+efficient+visual+tracking+Shen,+Chunhua+and+Brooks,+Michael+J.+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Adaptive over-relaxed mean shift</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, M. Brooks</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. 8th International Symposium on Signal Processing and Its Applications (ISSPA&rsquo;05), 2005</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1581003">link</a><a href="data/bibtex/Shen2005Adaptive.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Adaptive+over-relaxed+mean+shift+Shen,+Chunhua+and+Brooks,+Michael+J.">search</a></p>
<ol reversed>
<li><p>Errata: in figure 3 square marker and circle marker should be swapped.</p>
</li></ol>
</li>
<li><p><b>Enhanced importance sampling: unscented auxiliary particle filtering for visual tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, A. van den Hengel, A. Dick, M. Brooks</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. Australian Joint Conference on Artificial Intelligence (AI&rsquo;04), 2004</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://digital.library.adelaide.edu.au/dspace/handle/2440/29538">link</a><a href="data/bibtex/Shen2004Enhanced.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Enhanced+importance+sampling:+unscented+auxiliary+particle+filtering+for+visual+tracking+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Dick,+Anthony+and+Brooks,+Michael+J.">search</a></p>
</li>
<li><p><b>2D articulated tracking with dynamic Bayesian networks</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, A. van den Hengel, A. Dick, M. Brooks</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Computer and Information Technology (CIT&rsquo;04), 2004</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://dx.doi.org/10.1109/CIT.2004.1357185">link</a><a href="data/bibtex/Shen2004Articulated.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q={2D}+articulated+tracking+with+dynamic+{B}ayesian+networks+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Dick,+Anthony+and+Brooks,+Michael+J.">search</a></p>
</li>
<li><p><img class="imgP  right"   src="data/thumbnail/Shen2003Probabilistic_PDF.jpg"><b>Probabilistic multiple cue integration for particle filter based tracking</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, A. van den Hengel, A. Dick</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Proc. International Conference on Digital Image Computing - Techniques and Applications (DICTA&rsquo;03), 2003</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="http://sites.google.com/site/chhshen/publication/DICTA2003.pdf?attredirects=1">pdf</a><a href="data/bibtex/Shen2003Probabilistic.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Probabilistic+multiple+cue+integration+for+particle+filter+based+tracking+Shen,+Chunhua+and+{van+den+Hengel},+Anton+and+Dick,+Anthony">search</a></p>
<ol reversed>
<li><p>Nominated for Best Student Paper Award.</p>
</li>
</ol>

</li>
</ol>
<h2>Other</h2>
<ol reversed>
<li><p><b>Semidefinite programming (book chapter in: encyclopedia of computer vision, springer)</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>C. Shen, A. van den Hengel</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>2012</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/SDP2012.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Semidefinite+programming+(Book+chapter+in:+Encyclopedia+of+Computer+Vision,+Springer)+Shen,+Chunhua+and+{van+den+Hengel},+Anton">search</a></p>
</li>
<li><p><b>Proceedings of international conference on digital image computing: techniques and applications</b>   
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>J. Zhang, C. Shen, G. Geers, Q. Wu</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <i>Editors, IEEE, 2010</i>.
<br /><img class="eq" src="eqs/3225992146121917387-130.png" alt="cdot" style="vertical-align: 3px" /> <a href="data/bibtex/DICTA2010.bib">bibtex</a><a href="https://scholar.google.com/scholar?lr&amp;ie=UTF-8&amp;oe=UTF-8&amp;q=Proceedings+of+International+Conference+on+Digital+Image+Computing:+Techniques+and+Applications+Zhang,+Jian+and+Shen,+Chunhua+and+Geers,+Glen+and+Wu,+Qiang">search</a></p>
</li>
</ol>
<div id="footer">
<div id="footer-text">
&copy; <b>Chunhua Shen</b>
&bull;
last update: 2020-12-13 14:43:36 ACDT
&bull;
<a href="#" onClick="changewidth(1);return false" title="Expand page width"><b>&larr;&rarr;</b></a>
&bull;
<a href="#" onClick="changewidth(-1);return false" title="Reduce page width"><b>&rarr;&larr;</b></a>
<!-- Javascript -->
<script
    src="./script/jquery-1.6.2.min.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery.flot.min.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery-scroll.js"
    type="text/javascript">
</script>
<script
    src="./script/width_change.js"
    type="text/javascript">
</script>
<script
    src="./script/reverse_ol.js"
    type="text/javascript">
</script>
<script
    src="./script/jquery.highlight.js"
    type="text/javascript">
</script>
<!-- Required for the jQuery.LocalScroll Plug-in -->
<script type="text/javascript">
    $(document).ready(function(){
    //
    $.localScroll();
    //
    // Round images
    //
	$(".rounded-img, .rounded-img2").load(function() {
	$(this).wrap(function(){
	return '<span class="' + $(this).attr('class')
                + '" style="background:url(' + $(this).attr('src')
                + ') no-repeat center center; width: '
                + $(this).width() + 'px; height: '
                + $(this).height() + 'px;" />';
		});
		$(this).css("opacity","0");
	});
      //
      //
      //  nav tab animation
        var navDuration = 150; //time in miliseconds
        $('#nav li a').hover(function() {
          $(this).animate({ paddingTop:"50px"  }, navDuration);
        }, function() {
             $(this).animate({ paddingTop:"31px"}, navDuration);
        });
        //
        // plot citation figure using jquery flot, 2012 July, CS
        //
        var flot_options = {
        legend: {
            show: false,
            margin: 10,
            backgroundOpacity: 0.5
                },
        bars:  {
            show: true,
            barWidth: 0.6,
            align: "center"
        },
        yaxis: {
            min: -20,
            tickFormatter: function(val, axis) {
                if (val < 50)
                    return " &nbsp; ";  // some string
                else
                    return val < axis.max ? val.toFixed(0) :   "  &nbsp;  ";
            }
        },
        grid: {
            borderWidth: 0
        }
    };  // end of flot_options
    $.getJSON("./data/cs_cite.json", function(json) {
       //succes - data loaded, now use plot:
       var plotarea = $("#citation_plot_holder");
       var data=[json.data];
       $.plot(plotarea , data, flot_options);
    });
//
// end of jquery flot
//
    changewidth( 0.8 );
//
//
//  highlight ``Shen''
    $("body p").highlight(['C. Shen', 'Chunhua Shen']);
//  highlight selected publication venues
    $("body p").highlight(['CVPR', 'ICCV', 'ECCV', 'ICML', 'NeurIPS',
    'TPAMI', 'IJCV', 'JMLR'],  { element: 'span', className: 'selected_venue' } );
//
    });
</script>
<!-- News ticker -->
<script type="text/javascript">
    function tick(){
        $('#ticker li:first').slideUp( function () { $(this).appendTo($('#ticker')).slideDown(); });
    }
    setInterval(function(){ tick () }, 5000);
</script>
</div>
</div>
</div>
</body>
</html>
